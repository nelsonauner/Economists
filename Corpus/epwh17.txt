 ['Policy analysis had long been a main interest of Clive Granger\xe2\x80\x99s. Here, we present a framework for economic policy analysis that provides a novel integration of several fundamental concepts at the heart of Granger\xe2\x80\x99s contributions to time-series analysis. We work with a dynamic structural system analyzed by White and Lu (2010) with well defined causal meaning; under suitable conditional exogeneity restrictions, Granger causality coincides with this structural notion. The system contains target and control subsystems, with possibly integrated or cointegrated behavior. We ensure the invariance of the target subsystem to policy interventions using an explicitly causal partial equilibrium recursivity condition. Policy effectiveness is ensured by another explicit causality condition. These properties only involve the data generating process; models play a subsidiary role. Our framework thus complements that of Ericsson et al. (1998) (EHM) by providing conditions for policy analysis alternative to weak, strong, and superexogeneity. This makes possible policy analysis for systems that may fail EHM\xe2\x80\x99s conditions. It also facilitates analysis of the cointegrating properties of systems subject to policymaker control. We discuss a variety of practical procedures useful for analyzing such systems and illustrate with an application to a simple model of the US macroeconomy.'] ['A common exercise in empirical studies is a \xe2\x80\x9crobustness check\xe2\x80\x9d, where the researcher examines how certain \xe2\x80\x9ccore\xe2\x80\x9d regression coefficient estimates behave when the regression specification is modified by adding or removing regressors. If the coefficients are plausible and robust, this is commonly interpreted as evidence of structural validity. Here, we study when and how one can infer structural validity from coefficient robustness and plausibility. As we show, there are numerous pitfalls, as commonly implemented robustness checks give neither necessary nor sufficient evidence for structural validity. Indeed, if not conducted properly, robustness checks can be completely uninformative or entirely misleading. We discuss how critical and non-critical core variables can be properly specified and how non-core variables for the comparison regression can be chosen to ensure that robustness checks are indeed structurally informative. We provide a straightforward new Hausman (1978) type test of robustness for the critical core coefficients, additional diagnostics that can help explain why robustness test rejection occurs, and a new estimator, the Feasible Optimally combined GLS (FOGLeSs) estimator, that makes relatively efficient use of the robustness check regressions. A new procedure for Matlab, testrob, embodies these methods.'] ['We construct two classes of smoothed empirical likelihood ratio tests for the conditional independence hypothesis by writing the null hypothesis as an infinite collection of conditional moment restrictions indexed by a nuisance parameter. One class is based on the CDF; another is based on smoother functions. We show that the test statistics are asymptotically normal under the null hypothesis and a sequence of Pitman local alternatives. We also show that the tests possess an asymptotic optimality property in terms of average power. Simulations suggest that the tests are well behaved in finite samples. Applications to some economic and financial time series indicate that our tests reveal some interesting nonlinear causal relations which the traditional linear Granger causality test fails to detect.'] ['This paper studies a two-stage procedure for estimating partially identified models, based on Chernozhukov, Hong, and Tamer\xe2\x80\x99s (2007) theory of set estimation and inference. We consider the case where a sub-vector of parameters or their identified set can be estimated separately from the rest, possibly subject to a priori restrictions. Our procedure constructs the second-stage set estimator and confidence set by taking appropriate level sets of a criterion function, using a first-stage estimator to impose restrictions on the parameter of interest. We give conditions under which the two-stage set estimator is a set-valued random element that is measurable in an appropriate sense. We also establish the consistency of the two-stage set estimator.'] ['Separability is an important feature of structural equations, as it implies the absence of unobservable heterogeneity of effects and has significant implications for identification and efficiency of estimation. This paper provides a nonparametric test for separability in structural equations. The test is based on a conditional independence test recently developed by Huang et al. (2013), building on consistent procedures of Bierens (1982, 1990) and Stinchcombe and White (1998). The test is easy to implement and achieves n local power. We apply our test to study interest rate elasticities of loan demand in microfinance and the impact of education on wages.'] ['Notions of cause and effect are fundamental to economic explanation. Although concepts such as price effects are intuitive, rigorous foundations justifying causal discourse in the wide range of economic settings remain lacking. We illustrate this deficiency using an N-bidder private-value auction, posing causal questions that cannot be addressed within existing frameworks. We extend the frameworks of Pearl (2000) and White and Chalak (2009) to introduce topological settable systems (TSS), a causal framework capable of delivering the missing answers. Particularly, TSS accommodate choices belonging to general function spaces. Our analysis suggests how TSS enable causal discourse in various areas of economics.'] ['We analyze fast procedures for conducting Monte Carlo experiments involving bootstrap estimators, providing formal results establishing the properties of these methods under general conditions.'] [' We provide necessary and sufficient conditions for effect identification, thereby characterizing the limits to identification. Our results link the nonstructural potential outcome framework for identifying and estimating treatment effects to structural approaches in economics. This permits economic theory to be built into treatment effect methods. We elucidate the sources and consequences of identification failure by examining the biases arising when the necessary conditions fail, and we clarify the relations between unconfoundedness, conditional exogeneity, and the necessary and sufficient identification conditions. A new quantity, the exogeneity score, plays a central role in this analysis, permitting an omitted variable representation for effect biases. This analysis also provides practical guidance for selecting covariates and insight into the price paid for making various identifying assumptions and the benefits gained.'] ['We study the scope of local indirect least squares (LILS) methods for nonparametrically estimating average marginal effects of an endogenous cause X on a response Y in triangular structural systems that need not exhibit linearity, separability, or monotonicity in scalar unobservables. One main finding is negative: in the fully nonseparable case, LILS methods cannot recover the average marginal effect. LILS methods can nevertheless test the hypothesis of no effect in the general nonseparable case. We provide new nonparametric asymptotic theory, treating both the traditional case of observed exogenous instruments Z and the case where one observes only error-laden proxies for Z.'] ['This paper extends the familiar notion of fixed effects to nonlinear structures with infinite-dimensional unobservables, like preferences. The main result is that a generalized version of differencing identifies local average responses (LARs) in nonseparable structures. In contrast to existing results, this does not require either substantial restrictions on functional form or independence between the persistent unobservables and the explanatory variables of interest, and it requires only two time periods. On the other hand, the results are confined to the subpopulation of \xe2\x80\x9cstayers\xe2\x80\x9d (Chamberlain, 1982), i.e., the population for which the explanatory variables do not change over time. We extend the basic framework to include time trends and dynamics in the explanatory variables, and we show how distributional effects as well as average partial effects are identified. Our approach also allows endogeneity in the transitory unobservables. Furthermore, we show that this new identification principle can be applied to well-known objects like the slope coefficient in the semiparametric panel data binary choice model with fixed effects. Finally, we suggest estimators for the local average response and average partial effect, and we analyze their large- and finite-sample behavior.'] ['We provide an altrnative proof that the Ordinary Least Squares estimator is the (conditionally) best linear unbiased estimator.'] ['This note demonstrates that the conditions of Kotlarski\xe2\x80\x99s (1967, Pacific Journal of Mathematics 20(1), 69\xe2\x80\x9376) lemma can be substantially relaxed. In particular, the condition that the characteristic functions of M , U 1 , and U 2 are nonvanishing can be replaced with much weaker conditions: The characteristic function of U 1 can be allowed to have real zeros, as long as the derivative of its characteristic function at those points is not also zero; that of U 2 can have an isolated number of zeros; and that of M need satisfy no restrictions on its zeros. We also show that Kotlarski\xe2\x80\x99s lemma holds when the tails of U 1 are no thicker than exponential, regardless of the zeros of the characteristic functions of U 1 , U 2 , or M .'] ['Even though the trend components of economic time series were among the first to be distinguished, even today the trend remains relatively little understood. As Phillips (2005) notes, no one understands trends, but everyone sees them in the data. Economists and econometricians can give plenty of examples of trends, such as straight lines, exponentials or polynomials in time, and also forms of random walks, but these are merely examples. Individuals or groups do have their own personal definitions, but these diverse approaches illustrate the lack of a generally accepted definition of a trend. They also suggest a richness of alternatives to consider, both individually and jointly. Here, we make a variety of observations about trends, and based on these, we offer working definitions of various kinds of trends. We emphasize that these are working definitions, as our purpose here is to invite discussion, not to settle matters once and for all. Our hope is that our discussion here may facilitate development of increasingly better methods for prediction, estimation and hypothesis testing for non-stationary time-series data, and ultimately may enable decision makers to make more informed decisions.'] ['We provide a family of tests for the IID hypothesis based on generalized runs, powerful against unspecified alternatives, providing a useful complement to tests designed for specific alternatives, such as serial correlation, GARCH, or structural breaks. Our tests have appealing computational simplicity in that they do not require kernel density estimation, with the associated challenge of bandwidth selection. Simulations show levels close to nominal asymptotic levels. Our tests have power against both dependent and heterogeneous alternatives, as both theory and simulations demonstrate.'] [' Careful examination of the structure determining treatment choice and outcomes, as advocated by Heckman (2008), is central to the design of treatment effect estimators and, in particular, proper choice of covariates. Here, we demonstrate how causal diagrams developed in the machine learning literature by Judea Pearl and his colleagues, but not so well known to economists, can play a key role in this examination by using these methods to give a detailed analysis of the choice of efficient covariates identified by Hahn (2004). \xc2\xa9 2011 The President and Fellows of Harvard College and the Massachusetts Institute of Technology.'] ['We examine how structural systems can yield observed variables instrumental in identifying and estimating causal effects. We provide an exhaustive characterization of potentially identifying conditional exogeneity relationships and demonstrate how structural relations determine exogeneity and exclusion restrictions that yield moment conditions supporting identification. This provides a comprehensive framework for constructing instruments and covariates. We introduce notions of conditioning and conditional extended instrumental variables (XIVs). These permit identification but need not be traditional instruments, as they may be endogenous. We distinguish between observed XIVs and proxies for unobserved XIVs. A main message is the importance of sufficiently specifying causal relations governing the unobservables.'] ['We consider two tests of structural change for partially linear time-series models. The first tests for structural change in the parametric component, based on the cumulative sums of gradients from a single semiparametric regression. The second tests for structural change in the parametric and nonparametric components simultaneously, based on the cumulative sums of weighted residuals from the same semiparametric regression. We derive the limiting distributions of both tests under the null hypothesis of no structural change and for sequences of local alternatives. We show that the tests are generally not asymptotically pivotal under the null but may be free of nuisance parameters asymptotically under further asymptotic stationarity conditions. Our tests thus complement the conventional instability tests for parametric models. To improve the finite-sample performance of our tests, we also propose a wild bootstrap version of our tests and justify its validity. Finally, we conduct a small set of Monte Carlo simulations to investigate the finite-sample properties of the tests.'] ['We examine the econometric implications of the decision problem faced by a profit/utility-maximizing lender operating in a simple "double-binary" environment, where the two actions available are "approve" or "reject", and the two states of the world are "pay back" or "default". In practice, such decisions are often made by applying a fixed cutoff to the maximum likelihood estimate of a parametric model of the default probability. Following (Elliott and Lieli, 2007), we argue that this practice might contradict the lender\'s economic objective and, using German loan data, we illustrate the use of "context-specific" cutoffs and an estimation method derived directly from the lender\'s problem. We also provide a brief discussion of how to incorporate legal constraints, such as the prohibition of disparate treatment of potential borrowers, into the lender\'s problem.'] ['We give two new approaches to testing conditional exogeneity. This condition ensures unconfoundedness and identification of structural effects. Our approaches permit the presence of treatment effects under the null, thereby complementing methods of Rosenbaum (1987) and Heckman and Hotz (1989).'] [' Using a generally applicable dynamic structural system of equations, we give natural definitions of direct and total structural causality applicable to both structural vector autoregressions (VARs) and recursive structures representing time-series natural experiments. These concepts enable us to forge a previously missing link between Granger (G-)causality and structural causality by showing that, given a corresponding conditional form of exogeneity, G-causality holds if and only if a corresponding form of structural causality holds. We introduce a variety of structurally informative extensions of G-causality and provide their structural characterizations. Of importance for applications is the structural characterization of finite-order G-causality, which forms the basis for most empirical work. We show that conditional exogeneity is necessary for valid structural inference and prove that, in the absence of structural causality, conditional exogeneity is equivalent to G noncausality. These characterizations hold for both structural VARs and natural experiments. We propose practical new G-causality and conditional exogeneity tests and describe their use in testing for structural causality. We illustrate with studies of oil and gasoline prices, monetary policy and industrial production, and stock returns and macroeconomic announcements. Copyright The Author 2010. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oxfordjournals.org, Oxford University Press.'] ['We examine the use of the likelihood ratio (LR) statistic to test for unobserved heterogeneity in duration models, based on mixtures of exponential or Weibull distributions. We consider both the uncensored and censored duration cases. The asymptotic null distribution of the LR test statistic is not the standard chi-square, as the standard regularity conditions do not hold. Instead, there is a nuisance parameter identified only under the alternative, and a null parameter value on the boundary of the parameter space, as in Cho and White (2007a). We accommodate these and provide methods delivering consistent asymptotic critical values. We conduct a number of Monte Carlo simulations, comparing the level and power of the LR test statistic to an information matrix (IM) test due to Chesher (1984) and Lagrange multiplier (LM) tests of Kiefer (1985) and Sharma (1987). Our simulations show that the LR test statistic generally outperforms the IM and LM tests. We also revisit the work of van den Berg and Ridder (1998) on unemployment durations and of Ghysels et\xc3\x82 al. (2004) on interarrival times between stock trades, and, as it turns out, affirm their original informal inferences.'] ['No abstract is available for this item.'] [" This paper proposes an econometric framework to estimate market risk prices associated with risk-neutral measures Q under incomplete markets. We show that, under incomplete markets, the market price of risk is not point-identified but is instead identified as a bounded subset of an affine subspace. On the other hand, a structural assumption fully identifies diffusion coefficients for the data-generating probability measure P. We apply Kaido and White's (2008, Discussion Paper, University of California, San Diego) two-stage extension of Chernozhukov, Hong, and Tamer's (2007, Econometrica, 75(5), 1243--1284) partial identification framework to construct a set estimator and confidence regions for the identified set of market risk prices and to test hypotheses. We apply our results to study international risk sharing and risk premiums for market cap range indexes. Copyright The Author 2009. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oxfordjournals.org., Oxford University Press."] [" A correction on the optimal block size algorithms of Politis and White (2004) is given following a correction of Lahiri's (Lahiri 1999) theoretical results by Nordman (2008)."] ['We explore convenient analytic properties of distributions constructed as mixtures of scaled and shifted t-distributions. Particularly desirable for econometric applications are closed-form expressions for antiderivatives (e.g., the cumulative density function). We illustrate the usefulness of these distributions in two applications. In the first application, we produce density forecasts of U.S. inflation and show that these forecasts are more accurate, out-of-sample, than density forecasts obtained using normal or standard t-distributions. In the second application, we replicate the option-pricing exercise of Abadir and Rockinger [Density functionals, with an option-pricing application. Econometric Theory 19, 778-811.] and obtain comparably good results, while gaining analytical tractability.'] ['We propose a nonparametric test of conditional independence based on the weighted Hellinger distance between the two conditional densities, f (y |x ,z ) and f (y |x ), which is identically zero under the null. We use the functional delta method to expand the test statistic around the population value and establish asymptotic normality under \xce\xb2-mixing conditions. We show that the test is consistent and has power against alternatives at distance n \xe2\x88\x921/2 h\xe2\x88\x92 . The cases for which not all random variables of interest are continuously valued or observable are also discussed. Monte Carlo simulation results indicate that the test behaves reasonably well in finite samples and significantly outperforms some earlier tests for a variety of data generating processes. We apply our procedure to test for Granger noncausality in exchange rates.'] [" We analyze use of a quasi-likelihood ratio statistic for a mixture model to test the null hypothesis of one regime versus the alternative of two regimes in a Markov regime-switching context. This test exploits mixture properties implied by the regime-switching process, but ignores certain implied serial correlation properties. When formulated in the natural way, the setting is nonstandard, involving nuisance parameters on the boundary of the parameter space, nuisance parameters identified only under the alternative, or approximations using derivatives higher than second order. We exploit recent advances by Andrews (2001) and contribute to the literature by extending the scope of mixture models, obtaining asymptotic null distributions different from those in the literature. We further provide critical values for popular models or bounds for tail probabilities that are useful in constructing conservative critical values for regime-switching tests. We compare the size and power of our statistics to other useful tests for regime switching via Monte Carlo methods and find relatively good performance. We apply our methods to reexamine the classic cartel study of Porter (1983) and reaffirm Porter's findings. Copyright The Econometric Society 2007."] ['This paper proposes a nonparametric test of conditional independence based on the notion that two conditional distributions are equal if and only if the corresponding conditional characteristic functions are equal. We use the functional delta method to expand the test statistic around the population truth and establish asymptotic normality under $\\beta -$mixing conditions. We show that the test is consistent and has power against local alternatives at distance $n^{-1/2}h_{1}^{-(d_{1}+d_{3})/4}.$ The cases for which not all random variables of interest are\\ continuously valued or observable are also treated, and we show that the test is nuisance-parameter free. Simulation results suggest that the test has better finite sample performance than the Hellinger metric test of Su and White (2002) in detecting nonlinear Granger causality in the mean. Applications to exchange rates and to stock prices and trading volumes indicate that our test can reveal some interesting nonlinear causal relations that the traditional linear Granger causality test fails to detect.<p>(This abstract was borrowed from another version of this item.)'] [' We apply a new bootstrap statistical technique to examine the performance of the U.S. open-end, domestic equity mutual fund industry over the 1975 to 2002 period. A bootstrap approach is necessary because the cross section of mutual fund alphas has a complex nonnormal distribution due to heterogeneous risk-taking by funds as well as nonnormalities in individual fund alpha distributions. Our bootstrap approach uncovers findings that differ from many past studies. Specifically, we find that a sizable minority of managers pick stocks well enough to more than cover their costs. Moreover, the superior alphas of these managers "persist." Copyright 2006 by The American Finance Association.'] ['No abstract is available for this item.'] [' We propose a framework for out-of-sample predictive ability testing and forecast selection designed for use in the realistic situation in which the forecasting model is possibly misspecified, due to unmodeled dynamics, unmodeled heterogeneity, incorrect functional form, or any combination of these. Relative to the existing literature (Diebold and Mariano (1995) and West (1996)), we introduce two main innovations: (i) We derive our tests in an environment where the finite sample properties of the estimators on which the forecasts may depend are preserved asymptotically. (ii) We accommodate conditional evaluation objectives (can we predict which forecast will be more accurate at a future date?), which nest unconditional objectives (which forecast was more accurate on average?), that have been the sole focus of previous literature. As a result of (i), our tests have several advantages: they capture the effect of estimation uncertainty on relative forecast performance, they can handle forecasts based on both nested and nonnested models, they allow the forecasts to be produced by general estimation methods, and they are easy to compute. Although both unconditional and conditional approaches are informative, conditioning can help fine-tune the forecast selection to current economic conditions. To this end, we propose a two-step decision rule that uses current information to select the best forecast for the future date of interest. We illustrate the usefulness of our approach by comparing forecasts from leading parameter-reduction methods for macroeconomic forecasting using a large number of predictors. Copyright The Econometric Society 2006.'] ['In Perez-Amaral, Gallo, and White (2003), the authors proposed an automatic predictive modelling tool called Relevant Transformation of the Inputs Network Approach (RETINA). It is designed to embody flexibility (using nonlinear transformations of the predictors of interest), selective search within the range of possible models, control of collinearity, out-of-sample forecasting ability, and computational simplicity. In this paper we compare the characteristics of RETINA with PcGets, a well-known automatic modeling method proposed by David Hendry. We point out similarities, differences, and complementarities of the two methods. In an example using US telecommunications demand data we find that RETINA can improve both in- and out-of-sample over the usual linear regression model, and over some models suggested by PcGets. Thus, both methods are useful components of the modern applied econometrician\xe2\x80\x99s automated modelling tool chest.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] [" Entropy is a classical statistical concept with appealing properties. Establishing asymptotic distribution theory for smoothed nonparametric entropy measures of dependence has so far proved challenging. In this paper, we develop an asymptotic theory for a class of kernel-based smoothed nonparametric entropy measures of serial dependence in a time-series context. We use this theory to derive the limiting distribution of Granger and Lin's (1994) normalized entropy measure of serial dependence, which was previously not available in the literature. We also apply our theory to construct a new entropy-based test for serial dependence, providing an alternative to Robinson's (1991) approach. To obtain accurate inferences, we propose and justify a consistent smoothed bootstrap procedure. The naive bootstrap is not consistent for our test. Our test is useful in, for example, testing the random walk hypothesis, evaluating density forecasts, and identifying important lags of a time series. It is asymptotically locally more powerful than Robinson's (1991) test, as is confirmed in our simulation. An application to the daily S&amp;P; 500 stock price index illustrates our approach. Copyright The Econometric Society 2005."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['The bootstrap is an increasingly popular method for performing statistical inference. This paper provides the theoretical foundation for using the bootstrap as a valid tool of inference for quasi-maximum likelihood estimators (QMLE). We provide a unified framework for analyzing bootstrapped extremum estimators of nonlinear dynamic models for heterogeneous dependent stochastic processes. We apply our results to two block bootstrap methods, the moving blocks bootstrap of K\xc3\x83\xc2\xbcnsch (1989) and Liu and Singh (1992) and the stationary bootstrap of Politis and Romano (1994), and prove the first order asymptotic validity of the bootstrap approximation to the true distribution of QML estimators. Further, these block bootstrap methods are shown to provide heteroskedastic and autocorrelation consistent standard errors for the QMLE, thus extending the already large literature on robust inference and covariance matrix estimation. We also consider bootstrap testing. In particular, we prove the first order asymptotic validity of the bootstrap distribution of a suitable bootstrap analog of a Wald test statistic for testing hypotheses.<p>(This abstract was borrowed from another version of this item.)'] [' We review the different block bootstrap methods for time series, and present them in a unified framework. We then revisit a recent result of Lahiri [Lahiri, S. N. (1999b). Theoretical comparisons of block bootstrap methods, Ann. Statist. 27:386-404] comparing the different methods and give a corrected bound on their asymptotic relative efficiency; we also introduce a new notion of finite-sample \xe2\x80\x9cattainable\xe2\x80\x9d relative efficiency. Finally, based on the notion of spectral estimation via the flat-top lag-windows of Politis and Romano [Politis, D. N., Romano, J. P. (1995). Bias-corrected nonparametric spectral estimation. J. Time Series Anal. 16:67-103], we propose practically useful estimators of the optimal block size for the aforementioned block bootstrap methods. Our estimators are characterized by the fastest possible rate of convergence which is adaptive on the strength of the correlation of the time series as measured by the correlogram.'] [' A new method, called Relevant Transformation of the Inputs Network Approach is proposed as a tool for model building. It is designed around flexibility (with nonlinear transformations of the predictors of interest), selective search within the range of possible models, out-of-sample forecasting ability and computational simplicity. In tests on simulated data, it shows both a high rate of successful retrieval of the data generating process, which increases with the sample size and a good performance relative to other alternative procedures. A telephone service demand model is built to show how the procedure applies on real data. Copyright 2003 Blackwell Publishing Ltd.'] ['Data sharing is common practice in forecasting experiments in situations where fresh data samples are difficult or expensive to generate. This means that forecasters often analyze the same data set using a host of different models and sets of explanatory variables. This practice introduces statistical dependencies across forecasting studies that can severely distort statistical inference. Here we examine a new and inexpensive recursive bootstrap procedure that allows forecasters to account explicitly for these dependencies. The procedure allows forecasters to merge empirical evidence and draw inference in the light of previously accumulated results. In an empirical example, we merge results from predictions of daily stock prices based on (1) technical trading rules and (2) calendar rules, demonstrating both the significance of problems arising from data sharing and the simplicity of accounting for data sharing using these new methods.<p>(This abstract was borrowed from another version of this item.)'] ['Let H be an infinite-dimensional real separable Hilbert space. Given an unknown mapping M:H (r)H that can only be observed with noise, we consider two modified Robbins-Monro procedures to estimate the zero point ?o ( H of M. These procedures work in appropriate finite dimensional sub-spaces of growing dimension. Almost-sure convergence, functional central limit theorem (hence asymptotic normality), law of iterated logarithm (hence almost-sure loglog rate of convergence), and mean rate of convergence are obtained for Hilbert space-valued mixingale, (-dependent error processes.'] ["Presently, conditions ensuring the validity of bootstrap methods for the sample mean of (possibly heterogeneous) near epoch dependent (NED) functions of mixing processes are unknown. A0501n purpose of this paper is thus to establish the validity of the bootstrap in this context, extending the applicability of bootstrap methods to a class of processes broadly relevant for applications in economics and finance. The results apply to the moving blocks bootstrap of K\xc3\xbcnsch (1989) and Liu and Singh (1992) as well as to the stationary bootstrap of Politis and Romano (1994). In particular, the consistency of the bootstrap variance estimator for the sample mean is shown to be robust against heteroskedasticity and dependence of unknown form. The first order asymptotic validity of the bootstrap approximation to the actual distribution of the sample mean is also established in this heterogeneous NED context. Actuellement, les conditions assurant la validit\xc3\xa9 des m\xc3\xa9thodes de bootstrap pour la moyenne d'\xc3\xa9chantillon des (possiblement h\xc3\xa9t\xc3\xa9rog\xc3\xa8nes) fonctions de d\xc3\xa9pendance d'\xc3\xa9poque proche (DEP) des processus de mixage sont inconnues. Ainsi, un des objectifs principaux de cet article est d'\xc3\xa9tablir la validit\xc3\xa9 du bootstrap dans ce contexte, en \xc3\xa9largissant l'applicabilit\xc3\xa9 des m\xc3\xa9thodes de bootstrap \xc3\xa0 une classe de processus largement ad\xc3\xa9quats pour les applications en \xc3\xa9conomie et en finance. Les r\xc3\xa9sultats se rapportent au bootstrap de blocs mouvants de K\xc3\xbcnsch (1989) et Liu et Singh (1992), de m\xc3\xaame qu'au bootstrap stationnaire de Politis et Romano (1994). Plus particuli\xc3\xa8rement, nous d\xc3\xa9montrons que la consistance de l'estimateur de variance du bootstrap pour la moyenne d'\xc3\xa9chantillon r\xc3\xa9siste \xc3\xa0 l'h\xc3\xa9t\xc3\xa9rosc\xc3\xa9dasticit\xc3\xa9 et \xc3\xa0 la d\xc3\xa9pendance de forme inconnue. La validit\xc3\xa9 asymptotique de premier ordre de l'approximation du bootstrap \xc3\xa0 la distribution actuelle de la moyenne d'\xc3\xa9chantillon est \xc3\xa9galement d\xc3\xa9montr\xc3\xa9e dans ce contexte DEP h\xc3\xa9t\xc3\xa9rog\xc3\xa8ne.<p>(This abstract was borrowed from another version of this item.)"] ['We explore the extension of James-Stein type estimators in a direction that enables them to preserve their superiority when the sample size goes to infinity. Instead of shrinking a base estimator towards a fixed point, we shrink it towards a data-dependent point. We provide an analytic expression for the asymptotic risk and bias of James-Stein type estimators shrunk towards a data-dependent point and prove that they have smaller asymptotic risk than the base estimator. Shrinking an estimator toward a data-dependent point turns out to be equivalent to combining two random variables using the James-Stein rule. We propose a general combination scheme which includes random combination (the James-Stein combination) and the usual nonrandom combination as special cases. As an example, we apply our method to combine the Least Absolute Deviations estimator and the Least Squares estimator. Our simulation study indicates that the resulting combination estimators have desirable finite sample properties when errors are drawn from symmetric distributions. Finally, using stock return data we present some empirical evidence that the combination estimators have the potential to improve out-of-sample prediction in terms of both mean square error and mean absolute error.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['In this paper we introduce a class of nonlinear data generating processes (DGPs) that ara first order Markov and can be represented as the sum of a linear plus a bounded nonlinear component.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] [" In this paper we utilize White's Reality Check bootstrap methodology (White (1999)) to evaluate simple technical trading rules while quantifying the data-snooping bias and fully adjusting for its effect in the context of the full universe from which the trading rules were drawn. Hence, for the first time, the paper presents a comprehensive test of performance across all technical trading rules examined. We consider the study of Brock, Lakonishok, and LeBaron (1992), expand their universe of 26 trading rules, apply the rules to 100 years of daily data on the Dow Jones Industrial Average, and determine the effects of data-snooping. Copyright The American Finance Association 1999."] [' We describe an algorithm to efficiently compute maximum entropy densities, i.e. densities maximizing the Shannon entropy - [image omitted]\xef\xbf\xbd under a set of constraints [image omitted]\xef\xbf\xbd. Our method is based on an algorithm by Zellner and Highfield, which has been found not to converge under a variety of circumstances. To demonstrate that our method overcomes these difficulties, we conduct numerous experiments for the special case gi(x) = xi, n = 4. An extensive table of results for this case and computer code are available on the World Wide Web'] ['No abstract is available for this item.'] ["The authors show that quasimaximum likelihood (QML) estimators for conditional dispersion models can be severely affected by a small number of outliers such as market crashes and rallies, and they propose new estimation strategies (the two-stage Hampel estimators and two-stage S-estimators) resistant to the effects of outliers and study the properties of these estimators. They apply their methods to estimate models of the conditional volatility of the daily returns of the S&amp;P; 500 Cash Index series. In contrast to QML estimators, the authors' proposed method resists outliers, revealing an informative new picture of volatility dynamics during 'typical' daily market activity."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' We take a model selection approach to the question of whether a class of adaptive prediction models (artificial neural networks) is useful for predicting future values of nine macroeconomic variables. We use a variety of out-of-sample forecast-based model selection criteria, including forecast error measures and forecast direction accuracy. Ex ante or real-time forecasting results based on rolling window prediction methods indicate that multivariate adaptive linear vector autoregression models often outperform a variety of (1) adaptive and nonadaptive univariate models, (2) nonadaptive multivariate models, (3) adaptive nonlinear models, and (4) professionally available survey predictions. Further, model selection based on the in-sample Schwarz information criterion apparently fails to offer a convenient shortcut to true out-of-sample performance measures. \xc2\xa9 1997 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology'] ['To obtain consistency results for nonparametric estimators based on stochastic processes relevant in econometrics, we introduce the notions of Hilbert space-valued L mixingales and near-epoch dependent arrays, and we prove weak and strong laws of large numbers by using a new exponential inequality for Hilbert ( H) space-valued martingale difference arrays. We follow Andrews (1988, Econometric Theory 4, 458\xe2\x80\x93467), Hansen (1991, Econometric Theory 7, 213\xe2\x80\x93221; 1992, Econometric Theory 8, 421\xe2\x80\x93422), Davidson (1993, Statistics and Probability Letters 16,301\xe2\x80\x93304), and de Jong (1995, Econometric Theory 11, 347\xe2\x80\x93358), extending results for H = R and improving memory conditions in certain instances. We give as examples consistency results for series and kernel estimators.'] ['No abstract is available for this item.'] [' Contemporary tests for structural change are designed to detect a structural break within a given historical data set of fixed size. Due to the law of the iterated logarithm, these one-shot tests cannot be applied to monitor out-of-sample stability each time new data arrive. The authors propose and analyze two real-time monitoring procedures with controlled size asymptotically: the fluctuation and CUSUM monitoring procedures. Simulation results show that the proposed monitoring procedures indeed have controlled asymptotic size and that detection timing depends on the magnitude of parameter change, the signal to noise ratio, and the location of the out-of-sample break point. Copyright 1996 by The Econometric Society.'] [" We take a model selection approach to the question of whether forward interest rates are useful in predicting future spot rates, using a variety of out-of-sample forecast-based model selection criteria: forecast mean squared error, forecast direction accuracy, and forecast-based trading system profitability. We also examine the usefulness of a class of novel prediction models called 'artificial neural networks,' and investigate the issue of appropriate window sizes for rolling-window-based prediction methods. Results indicate that the premium of the forward rate over the spot rate helps to predict the sign of future changes in the interest rate. Further, model selection based on an in-sample Schwarz Information Criterion (SIC) does not appear to be a reliable guide to out-of-sample performance, in the case of short-term interest rates. Thus, the in-sample SIC apparently fails to offer a convenient shortcut to true out-of-sample performance measures."] ['No abstract is available for this item.'] [' This paper proposes two consistent one-sided specification tests for parametric regression models, one based on the sample covariance between the residual from the parametric model and the discrepancy between the parametric and nonparametric fitted values; the other based on the difference in sums of squared residuals between the parametric and nonparametric models. The authors estimate the nonparametric model by series regression. The new test statistics converge in distribution to a unit normal under correct specification and grow to infinity faster than the parametric rate [square root of] n under misspecification, while avoiding weighting, sample splitting, and nonnested testing procedures used elsewhere. Copyright 1995 by The Econometric Society.'] [' The authors provide a convergence theory for adaptive learning algorithms useful for the study of learning by economic agents. Their results extend the framework of L. Ljung previously utilized by A. Marcet-T. J. Sargent and M. Woodford by permitting nonlinear laws of motion driven by stochastic processes that may exhibit moderate dependence, such as mixing and mixingale processes. The authors draw on previous work by H. J. Kushner and D. S. Clark to provide readily verifiable and/or interpretable conditions ensuring algorithm convergence, chosen for their suitability in the context of adaptive learning. Copyright 1994 by The Econometric Society.'] ['We give a straightforward condition sufficient for determining the minimum asymptotic variance estimator in certain classes of estimators relevant to econometrics. These classes are relatively broad, as they include extremum estimation with smooth or nonsmooth objective functions; also, the rate of convergence to the asymptotic distribution is not required to be n \xe2\x88\x92\xc2\xbd . We present examples illustrating the content of our result. In particular, we apply our result to a class of weighted Huber estimators, and obtain, among other things, analogs of the generalized least-squares estimator for least L null -estimation, 1 \xe2\x89\xa4 p '] [' RegRESET is a regression post-processor which performs Ramsey\'s RESET test. Use this after running a linear regression to test the specification of that regression. Ramsey(1969): "Tests for specification errors in classical Least-Squares Regression Analysis," JRSS-B, vol 32, 350-371. Lee, White and Granger(1992), "Testing for Neglected Non-linearities in Time Series Models," J. of Econometrics, vol 56, 269-290.<p>(This abstract was borrowed from another version of this item.)'] [' The authors consider tests for changing trend that do not require prior knowledge about the location of the changepoint. The limiting distribution is derived from the functional central limit theorem and the critical value from the hitting probability of a Brownian bridge. Applying a test sensitive to the alternative of trend stationarity with structural breaks, they find that for real gross national product, real per capita gross national product, and real wages before World War II the hypothesis of trend stationarity is rejected.'] [' The authors consider the question, "Under what conditions is the extremum of a random function over a random set itself a random object?" The answer is relevant to problems in both game theory and econometrics, as they illustrate with examples. The authors\' purpose here is to bring the powerful tools of the theory of analytic sets as developed by C. Dellacherie and P.-A. Meyer (1978) to the wider attention of the economics profession and to distill Dellacherie and Meyer\'s work in such a way as to provide some readily accessible theoretical results that will permit relatively easy treatment of economically or econometrically relevant applications. Copyright 1992 by The Review of Economic Studies Limited.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['This paper estimates models of electricity and gas consumption for individual households using the Miracle 4 to 6 data sets collected by San Diego Gas and Electricity Company. Two types of model were constructed: the first involves typical end-use models with consumption explained by appliance ownership, household demographic characteristics, house dimensions and household income; the second class uses these variables plus consumption data for the previous year. The latter models consistently fitted better, while the end-use variables afforded little explanatory power. The results thus suggest that simple end-use models are of little value, at least for short-run forecasting. Their use for long-run forecasting has yet to be evaluated.'] ['Building on work of McLeish, we present a number of invariance principles for doubly indexed arrays of stochastic processes which may exhibit considerable dependence, heterogeneity, and/or trending moments. In particular, we consider possibly time-varying functions of infinite histories of heterogeneous mixing processes and obtain general invariance results, with central limit theorems following as corollaries. These results are formulated so as to apply to economic time series, which may exhibit some or all of the features allowed in our theorems. Results are given for the case of both scalar and vector stochastic processes. Using an approach recently pioneered by Phillips, and Phillips and Durlauf, we apply our results to least squares estimation of unit root models.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['We present a general theory of consistent estimation for possibly misspecified parametric models based on recent results of Domowitz and White. This theory extends the unification of Burguete, Gallant, and Souza by allowing for heterogeneous, time-dependent data and dynamic models. The theory is applied to yield consistency results for quasi-maximum-likelihood and method of moments estimators. Of particular interest is a new generalized rank condition for identifiability.'] ['No abstract is available for this item.'] ['We examine several modified versions of the heteroskedasticity-consistent covariance matrix estimator of Hinkley and White. On the basis of sampling experiments which compare the performance of quasi t statistics, we find that one estimator, based on the jackknife, performs better in small samples than the rest. We also examine finite-sample properties using modified critical values based on Edgeworth approximations, as proposed by Rothenberg. In addition, we compare the power of several tests for heteroskedasticity and find that it may be wise to employ the jackknife heteroskedasticity-consistent covariance matrix even in the absence of detected heteroskedasticity.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.']