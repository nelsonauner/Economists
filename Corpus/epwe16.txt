 [' We construct factors from a cross-section of exchange rates and use the idiosyncratic deviations from the factors to forecast. In a stylized data generating process, we show that such forecasts can be effective even if there is essentially no serial correlation in the univariate exchange rate processes. We apply the technique to a panel of bilateral U.S. dollar rates against 17 Organisation for Economic Co-operation and Development countries. We forecast using factors, and using factors combined with any of fundamentals suggested by Taylor rule, monetary and purchasing power parity models. For long horizon (8 and 12 quarter) forecasts, we tend to improve on the forecast of a "no change" benchmark in the late (1999-2007) but not early (1987-1998) parts of our sample.'] ['No abstract is available for this item.'] ["We fit a factor model to two monthly panels of deflated prices of energy, metals and agricultural commodities. Prices consistently display a tendency to revert towards the factor, though the speed of reversion to the factor is slow. Using both in- and out-of-sample metrics, we compare the factor model to that of a \xe2\x80\x9cno change\xe2\x80\x9d model and to two simple models that tie changes in commodity prices to percentage change in either global industrial production or the U.S. dollar. The factor model does relatively well at long (12 month) horizons. In terms of commodities, the factor model's performance is best for energy prices, worst for metals, with agricultural prices falling in between."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['This paper develops asymptotic econometric theory to help understand data generated by a present value model with a discount factor near one. A leading application is to exchange rate models. A key assumption of the asymptotic theory is that the discount factor approaches one as the sample size grows. The finite sample approximation implied by the asymptotic theory is quantitatively congruent with the modest departures from random walk behavior that are typically found and with imprecise estimation of a well-studied regression relating spot and forward exchange rates.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [" We propose two new procedures for comparing the mean squared prediction error (MSPE) of a benchmark model to the MSPEs of a small set of alternative models that nest the benchmark. Our procedures compare the benchmark to all the alternative models simultaneously rather than sequentially, and do not require re-estimation of models as part of a bootstrap procedure. Both procedures adjust MSPE differences in accordance with Clark and West (2007); one procedure then examines the maximum t-statistic, while the other computes a chi-squared statistic. Our simulations examine the proposed procedures and two existing procedures that do not adjust the MSPE differences: a chi-squared statistic and White's (2000) reality check. In these simulations, the two statistics that adjust MSPE differences have the most accurate size, and the procedure that looks at the maximum t-statistic has the best power. We illustrate our procedures by comparing forecasts of different models for US inflation. Copyright \xc2\xa9 2010 John Wiley &amp; Sons, Ltd."] [" We propose and evaluate a technique for instrumental variables estimation of linear models with conditional heteroskedasticity. The technique uses approximating parametric models for the projection of right-hand side variables onto the instrument space, and for conditional heteroskedasticity and serial correlation of the disturbance. Use of parametric models allows one to exploit information in all lags of instruments, unconstrained by degrees of freedom limitations. Analytical calculations and simulations indicate that sometimes there are large asymptotic and finite sample efficiency gains relative to conventional estimators (Hansen, 1982), and modest gains or losses depending on data generating process and sample size relative to quasi-maximum likelihood. These results are robust to minor misspecification of the parametric models used by our estimator. [Supplemental materials are available for this article. Go to the publisher's online edition of Econometric Reviews for the following free supplemental resources: two appendices containing additional results from this article.]"] ['Forecast evaluation often compares a parsimonious null model to a larger model that nests the null model. Under the null that the parsimonious model generates the data, the larger model introduces noise into its forecasts by estimating parameters whose population values are zero. We observe that the mean squared prediction error (MSPE) from the parsimonious model is therefore expected to be smaller than that of the larger model. We describe how to adjust MSPEs to account for this noise. We propose applying standard methods (West (1996)) to test whether the adjusted mean squared error difference is zero. We refer to nonstandard limiting distributions derived in Clark and McCracken (2001, 2005a) to argue that use of standard normal critical values will yield actual sizes close to, but a little less than, nominal size. Simulation evidence supports our recommended procedure.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['We consider using out of sample mean squared prediction errors (MSPEs) to evaluate the null that a given series follows a zero mean martingale difference against the alternative that it is linearly predictable. Under the null of zero predictability, the population MSPE of the null \xe2\x80\x9cno change\xe2\x80\x9d model equals that of the linear alternative. We show analytically and via simulations that despite this equality, the alternative model\xe2\x80\x99s sample MSPE is expected to be greater than the null\xe2\x80\x99s. We propose and evaluate an asymptotically normal test that properly accounts for the upward shift of the sample MSPE of the alternative model. Our simulations indicate that our proposed procedure works well.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['We explore the link between an interest rate rule for monetary policy and the behavior of the real exchange rate. The interest rate rule, in conjunction with some standard assumptions, implies that the deviation of the real exchange rate from its steady state depends on the present value of a weighted sum of inflation and output gap differentials. The weights are functions of the parameters of the interest rate rule. An initial look at German data yields some support for the model.'] ['This paper explores ways to integrate model uncertainty into policy evaluation. We first describe a general framework for the incorporation of model uncertainty into standard econometric calculations. This framework employs Bayesian model averaging methods that have begun to appear in a range of economic studies. Second, we illustrate these general ideas in the context of assessment of simple monetary policy rules for some standard New Keynesian specifications. The specifications vary in their treatment of expectations as well as in the dynamics of output and inflation. We conclude that the Taylor rule has good robustness properties, but may reasonably be challenged in overall quality with respect to stabilization by alternative simple rules that also condition on lagged interest rates, even though these rules employ parameters that are set without accounting for model uncertainty.'] ['No abstract is available for this item.'] [' The relationship between interest rates and exchange rates is puzzling and poorly understood. But under some standard assumptions, interest rates can be adjusted to smooth real exchange rate movements at the possible price of increased volatility in other variables. In New Zealand, estimates made under some generous suppositions about what monetary policy is able to accomplish suggest that decreasing real exchange rate volatility by about 25% would require increasing output volatility by about 10-15%, inflation volatility by about 0-15% and interest rate volatility by about 15-40%.'] ['This paper develops a general framework for economic policy evaluation. Using ideas from statistical decision theory, it argues that conventional approaches fail to appropriately integrate econometric analysis into evaluation problems. Further, it is argued that evaluation of alternative policies should explicitly account for uncertainty about the appropriate model of the economy. The paper shows how to develop an explicitly decision-theoretic approach to policy evaluation and how to incorporate model uncertainty into such an analysis. The theoretical implications of model uncertainty are explored in a set of examples, with a specific focus on how to design policies that are robust against such uncertainty. Finally, the framework is applied to the evaluation of monetary policy rules and to the analysis of tariff reductions as a way to increase aggregate economic growth.'] ['Standard economic models hold that exchange rates are influenced by fundamental variables such as relative money supplies, outputs, inflation rates and interest rates. Nonetheless, it has been well documented that such variables little help predict changes in floating exchange rates \xc5\xa0 that is, exchange rates follow a random walk. We show that the data do exhibit a related link suggested by standard models - that the exchange rate helps predict fundamentals. We also show analytically that in a rational expectations present value model, an asset price manifests near random walk behavior if fundamentals are I(1) and the factor for discounting future fundamentals is near one. We suggest that this may apply to exchange rates.'] [' We consider the contribution to the analysis of economic time series of the generalized method-of-moments estimator introduced by Hansen. We outline the theoretical contribution, conduct a small-scale literature survey, and discuss some ongoing theoretical research.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' In many time series models, an infinite number of moments can be used for estimation in a large sample. I supply a technically undemanding proof of a condition for optimal instrumental variables use of such moments in a parametric model. I also illustrate application of the condition in estimation of a linear model with a disturbance that is serially uncorrelated and conditionally heteroskedastic.'] ['No abstract is available for this item.'] [' This article presents analytical and simulation results on the properties of two tests for forecast encompassing allowing throughout for dependence of the forecasts on estimated regression parameters. One test, which was intended for forecasts that do not depend on regression parameters, was developed by Harvey, Leybourne, and Newbold. This test works relatively well when the size of the sample of forecast errors is very small. A second test, which explicitly accounts for uncertainty about the regression parameters, otherwise is comparable or preferable.'] ['This paper considers regression-based tests for encompassing, when none of the models under consideration encompasses all the other models. For both in- and out-of-sample applications, I derive asymptotic distributions and propose feasible procedures to construct confidence intervals and test statistics. Procedures that are asymptotically valid under the null of encompassing (e.g., Davidson and MacKinnon (1981)) can have large asymptotic and finite sample distortions. Simulations indicate that the proposed procedures can work well in samples of size typically available, though the divergence between actual and nominal confidence interval coverage sometimes is large.<p>(This abstract was borrowed from another version of this item.)'] [' The authors develop regression-based tests of hypotheses about out of sample prediction errors. Representative tests include ones for zero mean and zero correlation between a prediction error and a vector of predictors. The relevant environments are ones in which predictions depend on estimated parameters. The authors show that standard regression statistics generally fail to account for errors introduced by estimation of these parameters. They propose computationally convenient test statistics that properly account for such errors. Simulations indicate that the procedures can work well in samples of size typically available, although there sometimes are substantial size distortions. Copyright 1998 by Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association.'] ['No abstract is available for this item.'] ["A \xc3\xbeT consistent estimator of a heteroskedasticity and autocorrelation consistent covariance matrix estimator is proposed and evaluated. The relevant applications are ones in which the regression disturbance follows a moving average process of known order. In a system of \xc3\xbe equations, this `MA-\xc3\xbe' estimator entails estimation of the moving average coefficients of an \xc3\xbe-dimensional vector. Simulations indicate that the MA-\xc3\xbe estimator's finite sample performance is better than that of the estimators of Andrews and Monahan (1992) and Newey and West (1994) when cross-products of instruments and disturbances are sharply negatively autocorrelated, comparable or slightly worse otherwise.<p>(This abstract was borrowed from another version of this item.)"] [' This paper develops procedures for inference about the moments of smooth functions of out-of-sample predictions and prediction errors when there is a long time series of predictions and realizations. The aim is to provide tools for analysis of predictive accuracy and efficiency and, more generally, of predictive ability. The paper allows for nonnested and nonlinear models as well as for possible dependence of predictions and prediction errors on estimated regression parameters. Simulations indicate that the procedures can work well in samples of size typically available. Copyright 1996 by The Econometric Society.'] [' Using a dynamic linear equation that has a conditionally homoskedastic moving average disturbance, the authors compare two parameterizations of a commonly used instrumental variables estimator (Hansen (1982)) to one that is asymptotically optimal in a class of estimators that includes the conventional one (Hansen (1985)). They find that for some plausible data generating processes, the optimal one is distinctly more efficient asymptotically. Simulations indicate that, in samples of size typically available, asymptotic theory describes the distribution of the parameter estimates reasonably well but that test statistics sometimes are poorly sized.'] ['No abstract is available for this item.'] ['We compare the out-of-sample forecasting performance of univariate homoskedastic, GARCH, autoregressive and nonparametric models for conditional variances, using five bilateral weekly exchange rates for the dollar, 1973-1989. For a one week horizon, GARCH models tend to make slightly more accurate forecasts. For longer horizons, it is difficult to find grounds for choosing between the various models. None of the models perform well in a conventional test of forecast efficiency.<p>(This abstract was borrowed from another version of this item.)'] [" The authors propose a nonparametric method for automatically selecting the number of autocovariances to use in computing a heteroskedasticity and autocorrelation consistent covariance matrix. For a given kernel for weighting the autocovariances, they prove that their procedure is asymptotically equivalent to one that is optimal under a mean-squared error loss function. Monte Carlo simulations suggest that the authors' procedure performs tolerably well, although it does result in size distortions. Copyright 1994 by The Review of Economic Studies Limited."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['When estimates of variances are used to make asset allocation decisions, underestimates of population variances lead to lower expected utility than equivalent overestimates: a utility based criterion is asymmetric, unlike standard criteria such as mean squared error. To illustrate how to estimate a utility based criterion, we use five bilateral weekly dollar exchange rates, 1973-1989, and the corresponding pair of Eurodeposit rates. Of homoskedastic, GARCH, autoregressive and nonparametric models for the conditional variance of each exchange rate, GARCH models tend to produce the highest utility, on average. A mean squared error criterion also favors GARCH, but not as sharply.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ["A simple real model is used to decompose movements of aggregate inventories and output in Japan during 1975 to 1987 to three components, one due to cost shocks, one due to demand shocks, and one due to' shocks from abroad. Cost shocks are estimated to account for about one tenth of the movement in GNP, one half of the movement in inventories. Most of the remaining movement in GNP is due to demand shocks, in inventories to shocks from abroad. Confidence intervals around these point estimates are, however, very large.<p>(This abstract was borrowed from another version of this item.)"] ['This paper compares the cyclical and secular behavior of Japanese and U.S. inventories at the aggregate and sectoral level, 1967-1987. While, as is well known, U.S. inventories are sharply procyclical, Japanese inventories are only mildly procyclical. In neither country do inventory and sales move together in the long run, in the sense that the two series do not seem to be cointegrated. In Japan, but not in the U.S., there is a secular decline in the inventory-sales ratio.<p>(This abstract was borrowed from another version of this item.)'] [' A simple real linear-quadratic inventory model is used to determine how cost and demand shocks interacted to cause fluctuations in aggregate inventories and GNP in the United States, 1947-86. Cost shocks appear to be the predominant source of fluctuations in inventories and are largely, though not exclusively, responsible for the fact that GNP is more variable than final sales. Cost and demand shocks are of roughly equal importance for GNP. These estimates, however, are imprecise. With different, but plausible, values for a certain target inventory-sales ratio, cost shocks are less important than demand shocks for GNP fluctuations. Copyright 1990, the President and Fellows of Harvard College and the Massachusetts Institute of Technology.'] ['Casual examination of annual postwar data on inventories and aggregate output for seven developed countries -- Canada, France, West Germany, Italy, Japan, United Kingdom, United States -- suggests that in these countries the primary function of aggregate inventories is not to smooth aggregate output in the face of aggregate demand shocks. Japan is a possible exception to this generalization.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] [" A standard efficient markets model states that a stock price equals the expected present discounted valu e of its dividends, with a constant discount rate. This is shown to i mply that the variance of the innovation in the stock price is smalle r than that of a stock-price forecast made from a subset of the marke t's information set. The implication follows even if prices and divid ends require differencing to induce stationarity. The relation betwee n the variances appears not to hold for some annual U.S. stock-market data. The rejection of the model is both quantitatively and statisti cally significant. Copyright 1988 by The Econometric Society."] ['This is a summary and interpretation of some of the literature on stock price volatility that was stimulated by Leroy and Porter (1981) and Shiller (1981a). It appears that neither small sample bias, rational bubbles nor some standard models for expected returns adequately explain stock price volatility. This suggests a role for some nonstandard models for expected returns. One possibility is "fads" models in which noise trading by naive investors is important. At present, however, there is little direct evidence that such fads play a significant role in stock price determination.<p>(This abstract was borrowed from another version of this item.)'] ["It is shown that GNP will have an autoregressive root very close to unity in a variant of Taylor's (1980a,b) overlapping wage contracts model, for stylized versions of simple money supply rules and plausible values for the model's parameters. In this variant, monetary policy is the only reason for serial correlation in GNP. It is premature, therefore, to conclude, as some authors, have, that the presence of such a root in U.S. GNP is inconsistent with either a stationary natural rate or with nominal shocks playing a major role in the business cycle.<p>(This abstract was borrowed from another version of this item.)"] ["We use recent research on estimation and testing in the presence of unit roots to argue that Hall's (1978) t and F tests of whether consumption is predicted by lagged income, or by lags of consumption beyond the first, are asymptotically valid. A Monte Carlo experiment suggests that the asymptotic t and F distributions provide a good approximation to the actual finite sample distribution.<p>(This abstract was borrowed from another version of this item.)"] ['This paper uses a variance bounds test to see whether consumption is too sensitive to news about income to be consistent with a standard permanent income model, under the maintained hypothesis that income has a unit root. It is found that, if anything, consumption is less sensitive than the model would predict. This implication is robust to the representative consumer having private information about his future income that the econometrician does not have, to wealth shocks, and to transitory consumption. This suggests the importance in future research on the model of allowing for factors that tend to make consumption smooth.<p>(This abstract was borrowed from another version of this item.)'] [' Under fairly general conditions, ordinary least squares and linear instrumental variables estimators are asymptotically normal when a regression equation has nonstationary right hand side variables. Standard formulas may be used to calculate a consistent estimate of the asymptotic variance-covariance matrix of the estimated parameter vector, even if the disturbances are conditionally heteroskedastic and autocorrelated. So inference may proceed in the usual way. The key requirements are that the nonstationary variables share a common unit root and that the unconditional mean of their first differences is nonzero. Copyright 1988 by The Econometric Society.'] ['The set of parameters needed to calculate the expected present discounted value of a stream of dividends can be estimated in two ways. One may test for speculative bubbles, or fads, by testing whether the two estimates are the same. When the test is applied to some annual U.S. stock market data, the data usually reject the null hypothesis of no bubbles. The test is of general interest since it may be applied to a wide class of linear rational expectations models.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['This paper describes a simple method of calculating a heteroskedasticity and autocorrelation consistent covariance matrix that is positive semi-definite by construction. It also establishes consistency of the estimated covariance matrix under fairly general conditions.<p>(This abstract was borrowed from another version of this item.)'] [' Efficient method of moments estimation techniques include many commonly used techniques, including ordinary least squares, two- and three-stage least squares, quasi maximum likelihood, and versions of these for nonlinear environments. For models estimated by any efficient method of moments technique, the authors define analogues to the maximum likeliho od based Wald, likelihood ratio, Lagrange multiplier, and minimum chi-squared statistics. They prove the mutual asymptotic equivalence of the four in an environment that allows for disturbances that are auto correlated and heteroskedastic. They also describe a very convenient way to test a linear hypothesis in a linear model. Copyright 1987 by Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association.'] ['This paper uses a novel teat to see whether the Herse (1985) and Woo (1985) models are consistent with the variability of the deutschemark - dollar exchange rate 1974-1984. The answer, perhaps surprisingly, is yes. Both models, however, explain the month to month variability as resulting in a critical way from unobservable shocks to money demand and purchasing power parity. It would therefore be of interest in future work to model one or both of these shocks as explicit functions of economic variables.<p>(This abstract was borrowed from another version of this item.)'] ['This paper compares nominal income and monetary targets in a standard aggregate demand - aggregate supply framework. If the desirability of policies is measured by their effect on the unconditional variance of output, nominal income targeting is preferable if and only if the aggregate elasticity of demand for real balances is greater than one. This is precisely the opposite of the condition that in Bean (1984) is sufficient to make nominal income targeting preferable.This points out the importance of specification of supply and of objective function in work on nominal income targeting.<p>(This abstract was borrowed from another version of this item.)'] ['This paper compares numerically the asymptotic distributions of parameter estimates and test statistics associated with two estimation techniques: (a)a limited information one, which uses instrumental variables to estimate a single equation (Hansen and Singleton (1982)), and (b)a full information one, which uses a procedure asymptotically equivalent to maximum likelihood to simultaneously estimate multiple equations (Hansen and Sargent (1980)). The paper compares the two with respect to both (1)asymptotic efficiency under the null hypothesis of no misspecification, and (2)asymptotic bias and power in the presence of certain local alternatives. It is found that: (l)Full information standard errors are only moderately smaller than limited information standard errors. (2)When the model is misspecified, full information tests tend to be more powerful, and its parameter estimates tend to be more biased. This suggests that at least in the model considered here, the gains from the use of the less robust and computationally more complex full information technique are not particularly large.<p>(This abstract was borrowed from another version of this item.)'] ['This paper develops and applies a novel test of the Holt, et al.(1961) linear quadratic inventory model. It is shown that a central property of the model is that a certain weighted sum of variances and covariances of production, sales and inventories must be nonnegative. The weights are the basic structural parameters of the model. The model may be tested by seeing whether this sum in fact is nonnegative. When the test is applied to some non-durables data aggregated to the two-digit SIC code level, it almost always rejects the model, even though the model does well by traditional criteria.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.']