 [' This paper presents econometric methods for measuring the average output effect of reallocating an indivisible input across production units. A distinctive feature of reallocations is that, by definition, they involve no augmentation of resources and, as such, leave the marginal distribution of the reallocated input unchanged. Nevertheless, if the production technology is nonseparable, they may alter average output. An example is the reallocation of teachers across classrooms composed of students of varying mean ability. We focus on the effects of reallocating one input, while holding the assignment of another, potentially complementary, input fixed. We introduce a class of such reallocations\xe2\x80\x94correlated matching rules\xe2\x80\x94that includes the status quo allocation, a random allocation, and both the perfect positive and negative assortative matching allocations as special cases. We also characterize the effects of small changes in the status quo allocation. Our analysis leaves the production technology nonparametric. Identification therefore requires conditional exogeneity of the input to be reallocated given the potentially complementary (and possibly other) input(s). We relate this exogeneity assumption to the pairwise stability concept used in the game theoretic literature on two\xe2\x80\x90sided matching models with transfers. For estimation, we use a two\xe2\x80\x90step approach. In the first step, we nonparametrically estimate the production function. In the second step, we average the estimated production function over the distribution of inputs induced by the new assignment rule. Our methods build upon the partial mean literature, but require extensions involving boundary issues and the fact that the weight function used in averaging is itself estimated. We derive the large\xe2\x80\x90sample properties of our proposed estimators and assess their small\xe2\x80\x90sample properties via a limited set of Monte Carlo experiments. Our characterization of the large\xe2\x80\x90sample properties of estimated correlated matching rules uses a new result on kernel estimated \xe2\x80\x9cdouble averages,\xe2\x80\x9d which may be of independent interest.'] ['No abstract is available for this item.'] [' There is a large and growing literature on peer effects in economics. In the current article, we focus on a Manski-type linear-in-means model that has proved to be popular in empirical work. We critically examine some aspects of the statistical model that may be restrictive in empirical analyses. Specifically, we focus on three aspects. First, we examine the endogeneity of the network or peer groups. Second, we investigate simultaneously alternative definitions of links and the possibility of peer effects arising through multiple networks. Third, we highlight the representation of the traditional linear-in-means model as an autoregressive model, and contrast it with an alternative moving-average model, where the correlation between unconnected individuals who are indirectly connected is limited. Using data on friendship networks from the Add Health dataset, we illustrate the empirical relevance of these ideas.'] ['No abstract is available for this item.'] [' Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the \xe2\x80\x9chot deck,\xe2\x80\x9d a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials.'] [' We investigate the choice of the bandwidth for the regression discontinuity estimator. We focus on estimation by local linear regression, which was shown to have attractive properties (Porter, J. 2003, "Estimation in the Regression Discontinuity Model" (unpublished, Department of Economics, University of Wisconsin, Madison)). We derive the asymptotically optimal bandwidth under squared error loss. This optimal bandwidth depends on unknown functionals of the distribution of the data and we propose simple and consistent estimators for these functionals to obtain a fully data-driven bandwidth algorithm. We show that this bandwidth estimator is optimal according to the criterion of Li (1987, "Asymptotic Optimality for C p, C L, Cross-validation and Generalized Cross-validation: Discrete Index Set", Annals of Statistics, 15, 958--975), although it is not unique in the sense that alternative consistent estimators for the unknown functionals would lead to bandwidth estimators with the same optimality properties. We illustrate the proposed bandwidth, and the sensitivity to the choices made in our algorithm, by applying the methods to a data set previously analysed by Lee (2008, "Randomized Experiments from Non-random Selection in U.S. House Elections", Journal of Econometrics, 142, 675--697) as well as by conducting a small simulation study. Copyright , Oxford University Press.'] [' It is a standard practice in regression analyses to allow for clustering in the error covariance matrix if the explanatory variable of interest varies at a more aggregate level (e.g., the state level) than the units of observation (e.g., individuals). Often, however, the structure of the error covariance matrix is more complex, with correlations not vanishing for units in different clusters. Here, we explore the implications of such correlations for the actual and estimated precision of least squares estimators. Our main theoretical result is that with equal-sized clusters, if the covariate of interest is randomly assigned at the cluster level, only accounting for nonzero covariances at the cluster level, and ignoring correlations between clusters as well as differences in within-cluster correlations, leads to valid confidence intervals. However, in the absence of random assignment of the covariates, ignoring general correlation structures may lead to biases in standard errors. We illustrate our findings using the 5% public-use census data. Based on these results, we recommend that researchers, as a matter of routine, explore the extent of spatial correlations in explanatory variables beyond state-level clustering.'] [" In Abadie and Imbens (2006), it was shown that simple nearest-neighbor matching estimators include a conditional bias term that converges to zero at a rate that may be slower than N -super-1/2. As a result, matching estimators are not N -super-1/2-consistent in general. In this article, we propose a bias correction that renders matching estimators N -super-1/2-consistent and asymptotically normal. To demonstrate the methods proposed in this article, we apply them to the National Supported Work (NSW) data, originally analyzed in Lalonde (1986). We also carry out a small simulation study based on the NSW example. In this simulation study, a simple implementation of the bias-corrected matching estimator performs well compared to both simple matching estimators and to regression estimators in terms of bias, root-mean-squared-error, and coverage rates. Software to compute the estimators proposed in this article is available on the authors' web pages (http://www.economics.harvard.edu/faculty/imbens/software.html) and documented in Abadie et al. (2003).<p>(This abstract was borrowed from another version of this item.)"] ['Two recent papers, Deaton (2009) and Heckman and Urzua (2009), argue against what they see as an excessive and inappropriate use of experimental and quasi-experimental methods in empirical work in economics in the last decade. They specifically question the increased use of instrumental variables and natural experiments in labor economics and of randomized experiments in development economics. In these comments, I will make the case that this move toward shoring up the internal validity of estimates, and toward clarifying the description of the population these estimates are relevant for, has been important and beneficial in increasing the credibility of empirical work in economics. I also address some other concerns raised by the Deaton and Heckman-Urzua papers. (JEL C21, C31)'] ['Many empirical questions in economics and other social sciences depend on causal effects of programs or policies. In the last two decades, much research has been done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of, earlier work in both the statistics and econometrics literatures. It has by now reached a level of maturity that makes it an important tool in many areas of empirical research in economics, including labor economics, public finance, development economics, industrial organization, and other areas of empirical microeconomics. In this review, we discuss some of the recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more technical research.'] [' This paper uses control variables to identify and estimate models with nonseparable, multidimensional disturbances. Triangular simultaneous equations models are considered, with instruments and disturbances that are independent and a reduced form that is strictly monotonic in a scalar disturbance. Here it is shown that the conditional cumulative distribution function of the endogenous variable given the instruments is a control variable. Also, for any control variable, identification results are given for quantile, average, and policy effects. Bounds are given when a common support assumption is not satisfied. Estimators of identified objects and bounds are provided, and a demand analysis empirical example is given. Copyright 2009 The Econometric Society.'] ['Properties of GMM estimators are sensitive to the choice of instrument. Using many instruments leads to high asymptotic asymptotic efficiency but can cause high bias and/or variance in small samples. In this paper we develop and implement asymptotic mean square error (MSE) based criteria for instrument selection in estimation of conditional moment restriction models. The models we consider include various nonlinear simultaneous equations models with unknown heteroskedasticity. We develop moment selection criteria for the familiar two-step optimal GMM estimator (GMM), a bias corrected version, and generalized empirical likelihood estimators (GEL), that include the continuous updating estimator (CUE) as a special case. We also find that the CUE has lower higher-order variance than the bias-corrected GMM estimator, and that the higher-order efficiency of other GEL estimators depends on conditional kurtosis of the moments.'] [' Estimation of average treatment effects under unconfounded or ignorable treatment assignment is often hampered by lack of overlap in the covariate distributions between treatment groups. This lack of overlap can lead to imprecise estimates, and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used ad hoc methods for trimming the sample. We develop a systematic approach to addressing lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely. Under some conditions, the optimal selection rules depend solely on the propensity score. For a wide range of distributions, a good approximation to the optimal rule is provided by the simple rule of thumb to discard all units with estimated propensity scores outside the range [0.1,0.9]. Copyright 2009, Oxford University Press.'] [' In this paper we develop two nonparametric tests of treatment effect heterogeneity. The first test is for the null hypothesis that the treatment has a zero average effect for all subpopulations defined by covariates. The second test is for the null hypothesis that the average effect conditional on the covariates is identical for all subpopulations, that is, that there is no heterogeneity in average treatment effects by covariates. We derive tests that are straightforward to implement and illustrate the use of these tests on data from two sets of experimental evaluations of the effects of welfare-to-work programs. Copyright by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.'] [' Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romano (1994). Copyright 2008 The Econometric Society.'] ['In Regression Discontinuity (RD) designs for evaluating causal effects of interventions, assignment to a treatment is determined at least partly by the value of an observed covariate lying on either side of a fixed threshold. These designs were first introduced in the evaluation literature by Thistlewaite and Campbell (1960). With the exception of a few unpublished theoretical papers, these methods did not attract much attention in the economics literature until recently. Starting in the late 1990s, there has been a large number of studies in economics applying and extending RD methods. In this paper we review some of the practical and theoretical issues involved in the implementation of RD methods.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['In paired randomized experiments units are grouped in pairs, often based on covariate information, with random assignment within the pairs. Average treatment effects are then estimated by averaging the within-pair differences in outcomes. Typically the variance of the average treatment effect estimator is estimated using the sample variance of the within-pair differences. However, conditional on the covariates the variance of the average treatment effect estimator may be substantially smaller. Here we propose a simple way of estimating the conditional variance of the average treatment effect estimator by forming pairs-of-pairs with similar covariate values and estimating the variances within these pairs-of-pairs. Even though these within-pairs-of-pairs variance estimators are not consistent, their average is consistent for the conditional variance of the average treatment effect estimator and leads to asymptotically valid confidence intervals.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' Since the pioneering work by Daniel McFadden, utility-maximization-based multinomial response models have become important tools of empirical researchers. Various generalizations of these models have been developed to allow for unobserved heterogeneity in taste parameters and choice characteristics. Here we investigate how rich a specification of the unobserved components is needed to rationalize arbitrary choice patterns in settings with many individual decision makers, multiple markets, and large choice sets. We find that if one restricts the utility function to be monotone in the unobserved choice characteristics, then up to two unobserved choice characteristics may be needed to rationalize the choices. Copyright 2007 by the Economics Department Of The University Of Pennsylvania And Osaka University Institute Of Social And Economic Research Association.'] ["Using a Cox proportional hazard model that allows for a flexible time dependence that can incorporate both seasonal and business cycle effects, we analyze the determinants of re-employment probabilities of young workers from 1978-1989. We find considerable changes in the chances of young workers finding jobs over the business cycle, however, the characteristics of those starting jobless spells do not vary much over time. Therefore, government programs that target specific demographic groups may change individuals' positions within the queue of job seekers but will probably have a more limited impact on the overall re-employment probability. Living in an area with high local unemployment reduces re-employment chances as does being in a long spell of non-employment. However, when we allow for an interaction between the length of time of a jobless spell and the local unemployment rate we find the interaction term is positive. In other words, while workers appear to be scarred by a long spell of unemployment, the damage seems to be reduced if they are unemployed in an area with high overall unemployment.<p>(This abstract was borrowed from another version of this item.)"] ["We show how data from an evaluation in which subjects are randomly assigned to some treatment versus a control group can be combined with nonexperimental methods to estimate the differential effects of alternative treatments. We propose tests for the validity of these methods. We use these methods and tests to analyze the differential effects of labor force attachment (LFA) versus human capital development (HCD) training components with data from California's Greater Avenues to Independence (GAIN) program. While LFA is more effective than HCD training in the short term, we find that HCD is relatively more effective in the longer term."] [' Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N-super-1&amp;sol;2-consistent in general and describe conditions under which matching estimators do attain N-super-1&amp;sol;2-consistency. Second, we show that even in settings where matching estimators are N-super-1&amp;sol;2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R. Copyright The Econometric Society 2006.'] [' This paper develops a generalization of the widely used difference-in-differences method for evaluating the effects of policy changes. We propose a model that allows the control and treatment groups to have different average benefits from the treatment. The assumptions of the proposed model are invariant to the scaling of the outcome. We provide conditions under which the model is nonparametrically identified and propose an estimator that can be applied using either repeated cross section or panel data. Our approach provides an estimate of the entire counterfactual distribution of outcomes that would have been experienced by the treatment group in the absence of the treatment and likewise for the untreated group in the presence of the treatment. Thus, it enables the evaluation of policy interventions according to criteria such as a mean-variance trade-off. We also propose methods for inference, showing that our estimator for the average treatment effect is root-N consistent and asymptotically normal. We consider extensions to allow for covariates, discrete dependent variables, and multiple groups and time periods. Copyright The Econometric Society 2006.'] [' An instrument or instrumental variable manipulates a treatment and affects the outcome only indirectly through its manipulation of the treatment. For instance, encouragement to exercise might increase cardiovascular fitness, but only indirectly to the extent that it increases exercise. If instrument levels are randomly assigned to individuals, then the instrument may permit consistent estimation of the effects caused by the treatment, even though the treatment assignment itself is far from random. For instance, one can conduct a randomized experiment assigning some subjects to \'encouragement to exercise\' and others to \'no encouragement\' but, for reasons of habit or taste, some subjects will not exercise when encouraged and others will exercise without encouragement; none-the-less, such an instrument aids in estimating the effect of exercise. Instruments that are weak, i.e. instruments that have only a slight effect on the treatment, present inferential problems. We evaluate a recent proposal for permutation inference with an instrumental variable in four ways: using Angrist and Krueger\'s data on the effects of education on earnings using quarter of birth as an instrument, following Bound, Jaeger and Baker in using simulated independent observations in place of the instrument in Angrist and Krueger\'s data, using entirely simulated data in which correct answers are known and finally using statistical theory to show that "only" permutation inferences maintain correct coverage rates. The permutation inferences perform well in both easy and hard cases, with weak instruments, as well as with long-tailed responses. Copyright 2005 Royal Statistical Society.'] ['No abstract is available for this item.'] [' In this paper we propose a new estimator for a model with one endogenous regressor and many instrumental variables. Our motivation comes from the recent literature on the poor properties of standard instrumental variables estimators when the instrumental variables are weakly correlated with the endogenous regressor. Our proposed estimator puts a random coefficients structure on the relation between the endogenous regressor and the instruments. The variance of the random coefficients is modelled as an unknown parameter. In addition to proposing a new estimator, our analysis yields new insights into the properties of the standard two-stage least squares (TSLS) and limited-information maximum likelihood (LIML) estimators in the case with many weak instruments. We show that in some interesting cases, TSLS and LIML can be approximated by maximizing the random effects likelihood subject to particular constraints. We show that statistics based on comparisons of the unconstrained estimates of these parameters to the implicit TSLS and LIML restrictions can be used to identify settings when standard large sample approximations to the distributions of TSLS and LIML are likely to perform poorly. We also show that with many weak instruments, LIML confidence intervals are likely to have under-coverage, even though its finite sample distribution is approximately centered at the true value of the parameter. In an application with real data and simulations around this data set, the proposed estimator performs markedly better than TSLS and LIML, both in terms of coverage rate and in terms of risk. Copyright Econometric Society 2004.'] [' Recently there has been a surge in econometric work focusing on estimating average treatment effects under various sets of assumptions. One strand of this literature has developed methods for estimating average treatment effects for a binary treatment under assumptions variously described as exogeneity, unconfoundedness, or selection on observables. The implication of these assumptions is that systematic (for example, average or distributional) differences in outcomes between treated and control units with the same values for the covariates are attributable to the treatment. Recent analysis has considered estimation and inference for average treatment effects under weaker assumptions than typical of the earlier literature by avoiding distributional and functional-form assumptions. Various methods of semiparametric estimation have been proposed, including estimating the unknown regression functions, matching, methods using the propensity score such as weighting and blocking, and combinations of these approaches. In this paper I review the state of this literature and discuss some of its unanswered questions, focusing in particular on the practical implementation of these methods, the plausibility of this exogeneity assumption in economic applications, the relative performance of the various semiparametric estimators when the key assumptions (unconfoundedness and overlap) are satisfied, alternative estimands such as quantile treatment effects, and alternate methods such as Bayesian inference. \xc2\xa9 2004 President and Fellows of Harvard College and the Massachusetts Institute of Technology.'] ['This paper presents an implementation of matching estimators for average treatment effects in Stata. The nnmatch command allows you to estimate the average effect for all units or only for the treated or control units; to choose the number of matches; to specify the distance metric; to select a bias adjustment; and to use heteroskedastic-robust variance estimators. Copyright 2004 by StataCorp LP.'] [' Recently a growing body of research has studied inference in settings where parameters of interest are partially identified. In many cases the parameter is real-valued and the identification region is an interval whose lower and upper bounds may be estimated from sample data. For this case confidence intervals (CIs) have been proposed that cover the entire identification region with fixed probability. Here, we introduce a conceptually different type of confidence interval. Rather than cover the entire identification region with fixed probability, we propose CIs that asymptotically cover the true value of the parameter with this probability. However, the exact coverage probabilities of the simplest version of our new CIs do not converge to their nominal values uniformly across different values for the width of the identification region. To avoid the problems associated with this, we modify the proposed CI to ensure that its exact coverage probabilities do converge uniformly to their nominal values. We motivate this modified CI through exact results for the Gaussian case. Copyright The Econometric Society 2004.'] ['No abstract is available for this item.'] [' We are interested in estimating the average effect of a binary treatment on a scalar outcome. If assignment to the treatment is exogenous or unconfounded, that is, independent of the potential outcomes given covariates, biases associated with simple treatment-control average comparisons can be removed by adjusting for differences in the covariates. Rosenbaum and Rubin (1983) show that adjusting solely for differences between treated and control units in the propensity score removes all biases associated with differences in covariates. Although adjusting for differences in the propensity score removes all the bias, this can come at the expense of efficiency, as shown by Hahn (1998), Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey (1992). We show that weighting by the inverse of a nonparametric estimate of the propensity score, rather than the true propensity score, leads to an efficient estimate of the average treatment effect. We provide intuition for this result by showing that this estimator can be interpreted as an empirical likelihood estimator that efficiently incorporates the information about the propensity score. Copyright The Econometric Society 2003.'] [' This article evaluates the usefulness of a nonparametric approach to Bayesian inference by presenting two applications. Our first application considers an educational choice problem. We focus on obtaining a predictive distribution for earnings corresponding to various levels of schooling. This predictive distribution incorporates the parameter uncertainty, so that it is relevant for decision making under uncertainty in the expected utility framework of microeconomics. The second application is to quantile regression. Our point here is to examine the potential of the nonparametric framework to provide inferences without relying on asymptotic approximations. Unlike in the first application, the standard asymptotic normal approximation turns out not to be a good guide.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' This paper reports estimates of the effects of JTPA training programs on the distribution of earnings. The estimation uses a new instrumental variable (IV) method that measures program impacts on quantiles. The quantile treatment effects (QTE) estimator reduces to quantile regression when selection for treatment is exogenously determined. QTE can be computed as the solution to a convex linear programming problem, although this requires first-step estimation of a nuisance function. We develop distribution theory for the case where the first step is estimated nonparametrically. For women, the empirical results show that the JTPA program had the largest proportional impact at low quantiles. Perhaps surprisingly, however, JTPA training raised the quantiles of earnings for men only in the upper half of the trainee earnings distribution. Copyright The Econometric Society 2002.'] [" Generalized method of moments (GMM) estimation has become an important unifying framework for inference in econometrics in the last 20 years. It can be thought of as encompassing almost all of the common estimation methods, such as maximum likelihood, ordinary least squares, instrumental variables, and two-stage least squares, and nowadays is an important part of all advanced econometrics textbooks. The GMM approach links nicely to economic theory where orthogonality conditions that can serve as such moment functions often arise from optimizing behavior of agents. Much work has been done on these methods since the seminal article by Hansen, and much remains in progress. This article discusses some of the developments since Hansen's original work. In particular, it focuses on some of the recent work on empirical likelihood-type estimators, which circumvent the need for a first step in which the optimal weight matrix is estimated and have attractive information theoretic interpretations."] ['In many fields researchers wish to consider statistical models that allow for more complex relationships than can be inferred using only cross-sectional data. Panel or longitudinal data where the same units are observed repeatedly at different points in time can often provide the richer data needed for such models. Although such data allows researchers to identify more complex models than cross-sectional data, missing data problems can be more severe in panels. In particular, even units who respond in initial waves of the panel may drop out in subsequent waves, so that the subsample with complete data for all waves of the panel can be less representative of the population than the original sample. Sometimes, in the hope of mitigating the effects of attrition without losing the advantages of panel data over cross-sections, panel data sets are augmented by replacing units who have dropped out with new units randomly sampled from the original population. Following Ridder (1992), who used these replacement units to test some models for attrition, we call such additional samples refreshment samples. We explore the benefits of these samples for estimating models of attrition. We describe the manner in which the presence of refreshment samples allows the researcher to test various models for attrition in panel data, including models based on the assumption that missing data are missing at random (MAR, Rubin, 1976; Little and Rubin, 1987). The main result in the paper makes precise the extent to which refreshment samples are informative about the attrition process; a class of non-ignorable missing data models can be identified without making strong distributional or functional form assumptions if refreshment samples are available.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['This paper provides empirical evidence about the effect of unearned income on earnings, consumption, and savings. Using an original survey of people playing the lottery in Massachusetts in the mid-1980s, we analyze the effects of the magnitude of lottery prizes on economic behavior. The critical assumption is that among lottery winners the magnitude of the prize is randomly assigned. We find that unearned income reduces labor earnings, with a marginal propensity to consume leisure of approximately 11 percent, with larger effects for individuals between 55 and 65 years old. After receiving about half their prize, individuals saved about 16 percent.'] [' We consider the implications of an alternative to the classical measurement-error model, in which the observed, mismeasured data are optimal predictions of the true values, given some information set. In this model, any measurement error is uncorrelated with the reported value and, by necessity, correlated with the true value of interest. In a regression model, such measurement error in the regressor does not lead to bias, whereas measurement error in the dependent variable leads to bias toward 0. In general, the measurement-error model, together with the information set, is critical for determining the bias in econometric estimates.'] [' In markets where prices are determined by the intersection of supply and demand curves, standard identification results require the presence of instruments that shift one curve but not the other. These results are typically presented in the context of linear models with fixed coefficients and additive residuals. The first contribution of this paper is an investigation of the consequences of relaxing both the linearity and the additivity assumption for the interpretation of linear instrumental variables estimators. Without these assumptions, the standard linear instrumental variables estimator identifies a weighted average of the derivative of the behavioural relationship of interest. A second contribution is the formulation of critical identifying assumptions in terms of demand and supply at different prices and instruments, rather than in terms of functional-form specific residuals. Our approach to the simultaneous equations problem and the average-derivative interpretation of instrumental variables estimates is illustrated by estimating the demand for fresh whiting at the Fulton fish market. Strong and credible instruments for identification of this demand function are available in the form of weather conditions at sea. Copyright 2000 by The Review of Economic Studies Limited'] [" In this paper we analyze the estimation of coefficients in regression models under moment restrictions in which the moment restrictions are derived from auxiliary data. The moment restrictions yield weights for each observation that can subsequently be used in weighted regression analysis. We discuss the interpretation of these weights under two assumptions: that the target population (from which the moments are constructed) and the sampled population (from which the sample is drawn) are the same, and that these populations differ. We present an application based on omitted ability bias in estimation of wage regressions. The National Longitudinal Survey Young Men's Cohort (NLS)--in addition to containing information for each observation on wages, education, and experience--records data on two test scores that may be considered proxies for ability. The NLS is a small dataset, however, with a high attrition rate. We investigate how to mitigate these problems in the NLS by forming moments from the joint distribution of education, experience, and log wages in the 1% sample of the 1980 U.S. Census and using these moments to construct weights for weighted regression analysis of the NLS. We analyze the impacts of our weighted regression techniques on the estimated coefficients and standard errors of returns to education and experience in the NLS controlling for ability, with and without the assumption that the NLS and the Census samples are random samples from the same population. \xc2\xa9 1999 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology"] ["In a recent paper in this journal, Heckman discussed the use of instrumental variables methods in evaluation research and our local average treatment effects (LATE) interpretation of instrumental variables estimates. This comment provides additional background for Heckman's paper, and a review of our rationale for focusing on LATE. We also show that a set of assumptions proposed by Heckman as an alternative to the LATE assumptions are not compatible with either latent-index assignment models or the definition we proposed for an instrument."] [" Two-stage-least-squares (2SLS) estimates are biased towards the probability limit of OLS estimates. This bias grows with the degree of over-identification and can generate highly misleading results. In this paper we propose two simple alternatives to 2SLS and limited-information-maximum-likelihood (LIML) estimators for models with more instruments than endogenous regressors. These estimators can be interpreted as instrumental variables procedures using an instrument that is independent of disturbances even in finite samples. Independence is achieved by using a 'leave-one-out' jackknife-type fitted value in place of the usual first stage equation. The new estimators are first order equivalent to 2SLS but with finite-sample properties superior, in terms of bias and coverage rate of confidence intervals, compared to those of 2SLS and similar to those of LIML, when there are many instruments."] ["This paper develops a variant of one-step efficient GMM based on the KLIC rather than empirical likelihood. As in other one-step methods, the authors introduce M (the number of moments) auxiliary 'tilting' parameters which are used to construct a reweighting of the data so that the reweighted sample obeys all the moment conditions at the parameter estimates. Parameter and overidentification tests can be recast in terms of these tilting parameters; such tests are often startlingly more effective than their conventional counterparts. These performance differences cannot be completely explained by the leading terms of the statistics' asymptotic expansions."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' The authors investigate the effect of serving in the military in the Netherlands on future earnings. Estimating the cost or benefit of military service is complicated by the complex selection mechanisms that determine who eventually serves in the military. In this paper, the authors use substantial, policy-induced variation in aggregate military enrollment rates to deal with these selections issues. The estimates show that, approximately ten years after serving in the military, former conscripts have earnings that are on average 5 percent lower than the earnings of members of their birth cohort who did not serve in the military.'] ['No abstract is available for this item.'] [' An alternative form of the proportional hazard model is proposed. It allows one to introduce correlation between exit rates at the same (calendar) time for different individuals. One can, in the context of this model, still allow for, and estimate, duration effects. These should be parametrized. These modifications to the original Cox model are possible by reversing the roles of duration and calendar time. It is argued that flexibility with respect to the effects of these macro processes is of particular relevance in economic models. An example using Dutch data on labor market transitions illustrates the idea that to ignore calendar time effects may have severe consequences for the estimation of duration dependence. Copyright 1994 by MIT Press.'] [' Census reports can be interpreted as providing nearly exact knowledge of moments of the marginal distribution of economic variables. This information can be combined with cross-sectional or panel samples to improve accuracy of estimation. In this paper, the authors show how to do this efficiently. They show that the gains from use of marginal information can be substantial. The authors also discuss how to test the compatibility of sample and marginal information. Copyright 1994 by The Review of Economic Studies Limited.'] ['We investigate conditions sufficient for identification of average treatment effects using instrumental variables. First we show that the existence of valid instruments is not sufficient to identify any meaningful average treatment effect. We then establish that the combination of an instrument and a condition on the relation between the instrument and the participation status is sufficient for identification of a local average treatment effect for those who can be induced to change their participation status by changing the value of the instrument. Finally we derive the probability limit of the standard IV estimator under these conditions. It is seen to be a weighted average of local average treatment effects.<p>(This abstract was borrowed from another version of this item.)'] [' In this paper, a new estimator is proposed for discrete choice models with choice-based sampling. The estimator is efficient and can incorporate information on the marginal choice probabilities in a straightforward manner and for that case leads to a procedure that is computationally and intuitively more appealing than the estimators that have been proposed before. The idea is to start with a flexible parametrization of the distribution of the explanatory variables and then rewrite the estimator to remove dependence on these parametric assumptions. Copyright 1992 by The Econometric Society.']