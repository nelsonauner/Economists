 ['We investigate the sources of skewness in aggregate risk factors and the cross section of stock returns. In an ICAPM setting with conditional volatility, we find theoretical time series predictions on the relationships among volatility, returns, and skewness for priced risk factors. Market returns resemble these predictions; however, size, book-to-market, and momentum factor returns are not always consistent with our predictions. We find evidence that size and book-to-market may be priced post-crisis but not in the decade before. Momentum does not appear priced by our test. We link aggregate risk and skewness to individual stocks and find empirically that the risk aversion effect manifests in individual stock skewness. Additionally, we find several firm characteristics that explain stock skewness. Smaller firms, value firms, highly levered firms, and firms with poor credit ratings have more positive skewness.'] ['We compare the capital shortfall measured by regulatory stress tests, to that of a benchmark methodology \xe2\x80\x94 the \xe2\x80\x9cV-Lab stress test\xe2\x80\x9d \xe2\x80\x94 that employs only publicly available market data. We find that when capital shortfalls are measured relative to risk-weighted assets, the ranking of financial institutions is not well correlated to the ranking of the V-Lab stress test, whereas rank correlations increase when required capitalization is a function of total assets. We show that the risk measures used in risk-weighted assets are cross-sectionally uncorrelated with market measures of risk, as they do not account for the \xe2\x80\x9crisk that risk will change.\xe2\x80\x9d Furthermore, the banks that appeared to be best capitalized relative to risk-weighted assets were no better than the rest when the European economy deteriorated into the sovereign debt crisis in 2011.'] ['In financial time series analysis we encounter several instances of non\xe2\x80\x93negative valued processes (volumes, trades, durations, realized volatility, daily range, and so on) which exhibit clustering and can be modeled as the product of a vector of conditionally autoregressive scale factors and a multivariate iid innovation process (vector Multiplicative Error Model). Two novel points are introduced in this paper relative to previous suggestions: a more general specification which sets this vector MEM apart from an equation by equation specification; and the adoption of a GMM-based approach which bypasses the complicated issue of specifying a general multivariate non\xe2\x80\x93negative valued innovation process. A vMEM for volumes, number of trades and realized volatility reveals empirical support for a dynamically interdependent pattern of relationships among the variables on a number of NYSE stocks.<p>(This abstract was borrowed from another version of this item.)'] [' We revisit the relation between stock market volatility and macroeconomic activity using a new class of component models that distinguish short-run from long-run movements. We formulate models with the long-term component driven by inflation and industrial production growth that are in terms of pseudo out-of-sample prediction for horizons of one quarter at par or outperform more traditional time series volatility models at longer horizons. Hence, imputing economic fundamentals into volatility models pays off in terms of long-horizon forecasting. We also find that macroeconomic fundamentals play a significant role even at short horizons. \xc2\xa9 2013 The President and Fellows of Harvard College and the Massachusetts Institute of Technology.'] [' The financial crisis of 2007-2009 has given way to the sovereign debt crisis of 2010-2012, yet many of the banking issues remain the same. We discuss a method to estimate the capital that a financial firm would need to raise if we have another financial crisis. This measure of capital shortfall is based on publicly available information but is conceptually similar to the stress tests conducted by US and European regulators. We argue that this measure summarizes the major characteristics of systemic risk and provides a reliable interpretation of the past and current financial crises.'] [' We model the interrelations of equity market volatility in eight East Asian countries before, during, and after the Asian currency crisis. Using a new class of asymmetric volatility multiplicative error models based on the daily range, we find that dynamic propagation of volatility shocks occurs through a network of interdependencies, and shocks originating in Hong Kong may be amplified in their transmission throughout the system, posing greater risks to the region than shocks originating elsewhere. Although this partly explains the severity of the currency crisis, we also find evidence that parameters shifted, making the system more unstable during the crisis. \xc2\xa9 2011 The President and Fellows of Harvard College and the Massachusetts Institute of Technology.'] [' Financial risk management has generally focused on short-term risks rather than long-term risks, and arguably this was an important component of the recent financial crisis. Econometric approaches to measuring long-term risk are developed in order to estimate the term structure of value at risk and expected shortfall. Long-term negative skewness increases the downside risk and is a consequence of asymmetric volatility models. A test is developed for long-term skewness. In a Merton style structural default model, bankruptcies are accompanied by substantial drops in equity prices. Thus, skewness in a market factor implies high defaults and default correlations even far in the future corroborating the systemic importance of long-term skewness. Investors concerned about long-term risks may hedge exposure as in the Intertemporal Capital Asset Pricing Model (ICAPM). As a consequence, the aggregate wealth portfolio should have asymmetric volatility and hedge portfolios should have reversed asymmetric volatility. Using estimates from VLAB, reversed asymmetric volatility is found for many possible hedge portfolios such as volatility products, long- and short-term treasuries, some exchange rates, and gold. JEL: G01 Copyright The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com., Oxford University Press.'] [' A new covariance matrix estimator is proposed under the assumption that at every time period all pairwise correlations are equal. This assumption, which is pragmatically applied in various areas of finance, makes it possible to estimate arbitrarily large covariance matrices with ease. The model, called DECO, involves first adjusting for individual volatilities and then estimating correlations. A quasi-maximum likelihood result shows that DECO provides consistent parameter estimates even when the equicorrelation assumption is violated. We demonstrate how to generalize DECO to block equicorrelation structures. DECO estimates for U.S. stock return data show that (block) equicorrelated models can provide a better fit of the data than DCC. Using out-of-sample forecasts, DECO and Block DECO are shown to improve portfolio selection compared to an unrestricted dynamic correlation structure.'] [' We propose a new approach to model high and low frequency components of equity correlations. Our framework combines a factor asset pricing structure with other specifications capturing dynamic properties of volatilities and covariances between a single common factor and idiosyncratic returns. High frequency correlations mean revert to slowly varying functions that characterize long-term correlation patterns. We associate such term behavior with low frequency economic variables, including determinants of market and idiosyncratic volatilities. Flexibility in the time-varying level of mean reversion improves both the empirical fit of equity correlations in the United States and correlation forecasts at long horizons.'] ["We propose a model of dynamic correlations with a short- and long-run component specification, by extending the idea of component models for volatility. We call this class of models DCC-MIDAS. The key ingredients are the Engle (2002) DCC model, the Engle and Lee (1999) component GARCH model replacing the original DCC dynamics with a component specification and the Engle et\xc3\x82 al. (2006) GARCH-MIDAS specification that allows us to extract a long-run correlation component via mixed data sampling. We provide a comprehensive econometric analysis of the new class of models, and provide extensive empirical evidence that supports the model's specification."] ['No abstract is available for this item.'] ["The intertemporal capital asset pricing model of Merton (1973) is examined using the dynamic conditional correlation (DCC) model of Engle (2002). The mean-reverting DCC model is used to estimate a stock's (portfolio's) conditional covariance with the market and test whether the conditional covariance predicts time-variation in the stock's (portfolio's) expected return. The risk-aversion coefficient, restricted to be the same across assets in panel regression, is estimated to be between two and four and highly significant. The risk premium induced by the conditional covariation of assets with the market portfolio remains positive and significant after controlling for risk premia induced by conditional covariation with macroeconomic, financial, and volatility factors."] [" We propose a new method for pricing options based on GARCH models with filtered historical innovations. In an incomplete market framework, we allow for different distributions of historical and pricing return dynamics, which enhances the model's flexibility to fit market option prices. An extensive empirical analysis based on S&amp;P; 500 index options shows that our model outperforms other competing GARCH pricing models and ad hoc Black-Scholes models. We show that the flexible change of measure, the asymmetric GARCH volatility, and the nonparametric innovation distribution induce the accurate pricing performance of our model. Using a nonparametric approach, we obtain decreasing state-price densities per unit probability as suggested by economic theory and corroborating our GARCH pricing model. Implied volatility smiles appear to be explained by asymmetric volatility and negative skewness of filtered historical innovations. The Author 2008. Published by Oxford University Press on behalf of The Society for Financial Studies. All rights reserved. For permissions, please e-mail: journals.permissions@oxfordjournals.org., Oxford University Press."] [' We propose a dynamic econometric microstructure model of trading, and we investigate how the dynamics of trades and trade composition interact with the evolution of market liquidity, market depth, and order flow. We estimate a bivariate generalized autoregressive intensity process for the arrival rates of informed and uninformed trades for 16 actively traded stocks over 15 years of transaction data. Our results show that both informed and uninformed trades are highly persistent, but that the uninformed arrival forecasts respond negatively to past forecasts of the informed intensity. Our estimation generates daily conditional arrival rates of informed and uninformed trades, which we use to construct forecasts of the probability of information-based trade (PIN). These forecasts are used in turn to forecast market liquidity as measured by bid-ask spreads and the price impact of orders. We observe that PINs vary across assets and over time, and most importantly that they are correlated across assets. Our analysis shows that one principal component explains much of the daily variation in PINs and that this systemic liquidity factor may be important for asset pricing. We also find that PINs tend to rise before earnings announcement days and decline afterwards. Copyright The Author 2008., Oxford University Press.'] [' Twenty-five years of volatility research has left the macroeconomic environment playing a minor role. This paper proposes modeling equity volatilities as a combination of macro- economic effects and time series dynamics. High-frequency return volatility is specified to be the product of a slow-moving component, represented by an exponential spline, and a unit GARCH. This slow-moving component is the low-frequency volatility, which in this model coincides with the unconditional volatility. This component is estimated for nearly 50 countries over various sample periods of daily data. Low-frequency volatility is then modeled as a function of macroeconomic and financial variables in an unbalanced panel with a variety of dependence structures. It is found to vary over time and across countries. The low-frequency component of volatility is greater when the macroeconomic factors of GDP, inflation, and short-term interest rates are more volatile or when inflation is high and output growth is low. Volatility is higher not only for emerging markets and markets with small numbers of listed companies and market capitalization relative to GDP, but also for large economies. The model allows long horizon forecasts of volatility to depend on macroeconomic developments, and delivers estimates of the volatility to be anticipated in a newly opened market. The Author 2008. Published by Oxford University Press on behalf of the Society for Financial Studies. All rights reserved. For permissions, please e-mail: journals.permissions@oxfordjournals.org., Oxford University Press.'] ['No abstract is available for this item.'] [' This paper proposes a new generalized autoregressive conditionally heteroskedastic (GARCH) process, the asymmetric generalized dynamic conditional correlation (AG-DCC) model. The AG-DCC process extends previous specifications along two dimensions: it allows for series-specific news impact and smoothing parameters and permits conditional asymmetries in correlation dynamics. The AG-DCC specification is well suited to examine correlation dynamics among different asset classes and investigate the presence of asymmetric responses in conditional variances and correlations to negative returns. We employ the AG-DCC model to analyze the behavior of international equities and government bonds. While equity returns show strong evidence of asymmetries in conditional volatility, little is found for bond returns. However, both equities and bonds exhibit asymmetries in conditional correlations, with equities responding stronger than bonds to joint bad news. The article also finds that, during periods of financial turmoil, equity market volatilities show important linkages, and conditional equity correlations among regional groups increase dramatically. Furthermore, in January 1999 with the introduction of the euro, we document significant evidence of a structural break in correlation although not in volatility. The introduction of a fixed exchange rate regime leads to near-perfect correlation among bond returns within the European Monetary Union (EMU) countries, which is not surprising when considering the harmonization in monetary policy. However, the increase in return correlation is not restricted to bond returns in EMU countries: equity return correlation both within and outside the EMU also increases. Copyright 2006, Oxford University Press.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ["Many ways exist to measure and model financial asset volatility. In principle, as the frequency of the data increases, the quality of forecasts should improve. Yet, there is no consensus about a true' or best' measure of volatility. In this paper we propose to jointly consider absolute daily returns, daily high-low range and daily realized volatility to develop a forecasting model based on their conditional dynamics. As all are non-negative series, we develop a multiplicative error model that is consistent and asymptotically normal under a wide range of specifications for the error density function. The estimation results show significant interactions between the indicators. We also show that one-month-ahead forecasts match well (both in and out of sample) the market-based volatility measure provided by an average of implied volatilities of index options as measured by VIX.<p>(This abstract was borrowed from another version of this item.)"] ['No abstract is available for this item.'] ['In this paper we analyze and interpret the quote price dynamics of 100 NYSE stocks with varying average trade frequencies. We specify an error-correction model for the log difference of the bid and the ask price, with the spread acting as the error-correction term, and include as regressors the characteristics of the trades occurring between quote observations, if any. We find that short duration and medium volume trades have the largest impacts on quote prices for all one hundred stocks, and that buyer initiated trades primarily move the ask price while seller initiated trades primarily move the bid price. Trades have a greater impact on quotes in both the short and the long run for the infrequently traded stocks than for the more actively traded stocks. Finally, we find strong evidence that the spread is mean reverting.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['The advantage of knowing about risks is that we can change our behavior to avoid them. Of course, it is easily observed that to avoid all risks would be impossible; it might entail no flying, no driving, no walking, eating and drinking only healthy foods and never being touched by sunshine. Even a bath could be dangerous. I could not receive this prize if I sought to avoid all risks. There are some risks we choose to take because the benefits from taking them exceed the possible costs. Optimal behavior takes risks that are worthwhile. This is the central paradigm of finance; we must take risks to achieve rewards but not all risks are equally rewarded. Both the risks and the rewards are in the future, so it is the expectation of loss that is balanced against the expectation of reward. Thus we optimize our behavior, and in particular our portfolio, to maximize rewards and minimize risks.<p>(This abstract was borrowed from another version of this item.)'] ['Value at Risk (VaR) has become the standard measure of market risk employed by financial institutions for both internal and regulatory purposes. VaR is defined as the value that a portfolio will lose with a given probability, over a certain time horizon (usually one or ten days). Despite its conceptual simplicity, its measurement is a very challenging statistical problem and none of the methodologies developed so far give satisfactory solutions. Interpreting the VaR as the quantile of future portfolio values conditional on current information, we propose a new approach to quantile estimation which does not require any of the extreme assumptions invoked by existing methodologies (such as normality or i.i.d. returns). The Conditional Autoregressive Value-at-Risk or CAViaR model moves the focus of attention from the distribution of returns directly to the behavior of the quantile. We specify the evolution of the quantile over time using a special type of autoregressive process and use the regression quantile framework introduced by Koenker and Bassett to determine the unknown parameters. Since the objective function is not differentiable, we use a differential evolutionary genetic algorithm for the numerical optimization. Utilizing the criterion that each period the probability of exceeding the VaR must be independent of all the past information, we introduce a new test of model adequacy, the Dynamic Quantile test. Applications to simulated and real data provide empirical support to this methodology and illustrate the ability of these algorithms to adapt to new risk environments.<p>(This abstract was borrowed from another version of this item.)'] [' This article formulates a bivariate point process to jointly analyze trade and quote arrivals. In microstructure models, trades may reveal private information that is then incorporated into new price quotes. This article examines the speed of this information flow and the circumstances that govern it. A joint likelihood function for trade and quote arrivals is specified in a way that recognizes that an intervening trade sometimes censors the time between a trade and the subsequent quote. Models of trades and quotes are estimated for eight stocks using Trade and Quote database (TAQ) data. The essential finding for the arrival of price quotes is that information flow variables, such as high trade arrival rates, large volume per trade, and wide bid--ask spreads, all predict more rapid price revisions. This means prices respond more quickly to trades when information is flowing so that the price impacts of trades and ultimately the volatility of prices are high in such circumstances. , .'] ['This paper investigates the empirical characteristics of investor risk aversion over equity return states by estimating a daily semi-parametric pricing kernel. The two key features of this estimator are: (1) the functional form of the pricing kernel is estimated semi-parametrically, instead of being prespecified and (2) the pricing kernel is re-estimated on a daily basis, allowing measurement of time-variation in risk-aversion over equity return states.<p>(This abstract was borrowed from another version of this item.)'] [' Time varying correlations are often estimated with multivariate generalized autoregressive conditional heteroskedasticity (GARCH) models that are linear in squares and cross products of the data. A new class of multivariate models called dynamic conditional correlation models is proposed. These have the flexibility of univariate GARCH models coupled with parsimonious parametric models for the correlations. They are not linear but can often be estimated very simply with univariate or two-step methods based on the likelihood function. It is shown that they perform well in a variety of situations and provide sensible empirical results.'] [' In the 20 years following the publication of the ARCH model, there has been a vast quantity of research uncovering the properties of competing volatility models. Wide-ranging applications to financial data have discovered important stylized facts and illustrated both the strengths and weaknesses of the models. There are now many surveys of this literature. This paper looks forward to identify promising areas of new research. The paper lists five new frontiers. It briefly discusses three-high-frequency volatility models, large-scale multivariate ARCH models, and derivatives pricing models. Two further frontiers are examined in more detail-application of ARCH models to the broad class of non-negative processes, and use of Least Squares Monte Carlo to examine non-linear properties of any model that can be simulated. Using this methodology, the paper analyses more general types of ARCH models, stochastic volatility models, long-memory models and breaking volatility models. The volatility of volatility is defined, estimated and compared with option-implied volatilities. Copyright \xc2\xa9 2002 John Wiley &amp; Sons, Ltd.'] ['No abstract is available for this item.'] ['ARCH and GARCH models have become important tools in the analysis of time series data, particularly in financial applications. These models are especially useful when the goal of the study is to analyze and forecast volatility. This paper gives the motivation behind the simplest GARCH model and illustrates its usefulness in examining portfolio risk. Extensions are briefly discussed.'] ['No abstract is available for this item.'] [' A volatility model must be able to forecast volatility; this is the central requirement in almost all financial applications. In this paper we outline some stylized facts about volatility that should be incorporated in a model: pronounced persistence and mean-reversion, asymmetry such that the sign of an innovation also affects volatility and the possibility of exogenous or pre-determined variables influencing volatility. We use data on the Dow Jones Industrial Index to illustrate these stylized facts, and the ability of GARCH-type models to capture these features. We conclude with some challenges for future research in this area.'] [" We use Hasbrouck's (1991) vector autoregressive model for prices and trades to empirically test and assess the role played by the waiting time between consecutive transactions in the process of price formation. We find that as the time duration between transactions decreases, the price impact of trades, the speed of price adjustment to trade-related information, and the positive autocorrelation of signed trades all increase. This suggests that times when markets are most active are times when there is an increased presence of informed traders; we interpret such markets as having reduced liquidity. Copyright The American Finance Association 2000."] ['A complete transactions record is defined to be ultra-high frequency data. The transaction arrival times and associated characteristics can be analyzed by marked point processes. The ACD model developed by Engle and Russell (1998) is then applied to IBM transactions data to develop semi-parametric hazard estimates and measures of conditional variances. Both returns and variances are negatively influenced by surprisingly long durations as suggested by asymmetric information models of market micro-structure.'] [' This paper bridges the gap between processes where shocks are permanent and those with transitory shocks by formulating a process in which the long-run impact of each innovation is time-varying and stochastic. In the stochastic permanent breaks (STOPBREAK) process, frequent transitory shocks are supplemented by occasional permanent shifts. Consistency and asymptotic normality of quasi-maximum-likelihood estimates is established, and locally best hypothesis tests of the null of a random walk are developed. The model is applied to relative prices of pairs of stocks and significant test statistics result. \xc2\xa9 2000 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology'] ['This paper proposes a new statistical model for the analysis of data which arrives at irregular intervals. The model treats the time between events as a stochastic process and proposes a new class of point processes with dependent arrival rates. The conditional intensity is developed and compared with other self-exciting processes. The model is applied to the arrival times of financial transactions and therefore is a model of transaction volume, and also to the arrival of other events such as price changes. Models for the volatility of prices are estimated, and examined from a market microstructure point of view.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['This paper presents theoretical results on the formulation and estimation of multivariate generalized ARCH models within simultaneous equations systems. A new parameterization of the multivariate ARCH process is proposed, and equivalence relations are discussed for the various ARCH parameterizations. Constraints sufficient to guarantee the positive definiteness of the conditional covariance matrices are developed, and necessary and sufficient conditions for covariance stationarity are presented. Identification and maximum likelihood estimation of the parameters in the simultaneous equations context are also covered.'] ['No abstract is available for this item.'] [' This article investigates empirically how returns and volatilities of stock indices are correlated between the Tokyo and New York markets. Using intradaily data that define daytime and overnight returns for both markets, we find that Tokyo (New York) daytime returns are correlated with New York (Tokyo) overnight returns. We interpret this result as evidence that information revealed during the trading hours of one market has a global impact on the returns of the other market. In order to extract the global factor from the daytime returns of one market, we propose and estimate a signal-extraction model with GARCH processes. Article published by Oxford University Press on behalf of the Society for Financial Studies in its journal, The Review of Financial Studies.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [" The existence of a serial correlation common feature among the first differences of a set of I(1) variables implies the existence of a common cycle in the Beveridge-Nelson-Stock-Watson decomposition of those variables. A test for the existence of common cycles among cointegrated variables is developed. The test is used to examine the validity of the common trend-common cycle structure implied by Flavin's excess sensitivity hypothesis and Campbell and Mankiw's mixture of rational expectations and rule-of-thumb hypothesis for consumption and income. Linear independence between the cointegration and the cofeature vectors is exploited to decompose consumption and income into their trend and cycle components. Copyright 1993 by John Wiley &amp; Sons, Ltd."] [' A common finding in many of the recent empirical studies with the ARCH class of models applied to high frequency financial data concerns the apparent persistence of shocks for forecast of the future conditional variances. It is likely that several different variables share this same implied long-run component, however. In that situation, the variables are defined to be copersistent in variance. Conditions for copersistence to occur in the linear multivariate GARCH model are presented. These conditions parallel the conditions for linear cointegration in the mean. A simple empirical example with foreign exchange rate data illustrates the ideas. Copyright 1993 by The Econometric Society.'] [' In this paper, the authors consider a framework in which the cross-sectional and time-series behavior of the yield curve can be studied simultaneously. They examine the relationship between the yield curve and the time-varying conditional volatility of the Treasury bill market. The authors demonstrate that different shaped yield curves can result given different combinations of volatility and expectations about future spot rates. Moreover, adjusting the forward rate for the volatility related forward premium can improve its performance as a predictor for the future spot rate. Copyright 1993 by Ohio State University Press.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' This article introduces a class of statistical tests for the hypothesis that some feature that is present in each of several variables is common to them. Features are data properties such as serial correlation, trends, seasonality, heteroscedasticity, autoregressive conditional heteroscedasticity, and excess kurtosis. A feature is detected by a hypothesis test taking no feature as the null, and a common feature is detected by a test that finds linear combinations of variables with no feature. Often, an exact asymptotic critical value can be obtained that is simply a test of overidentifying restrictions in an instrumental variable regression. This article tests for a common international business cycle.'] [' In this article, the authors take advantage of the time-varying structure of stock-returns variances to investigate whether two international stock markets share the same volatility process. They use a test recently developed by R. F. Engle and S. Kozicki (1990). This test is also used to assess the validity of a one-factor autoregress ive conditional heteroskedasticity model. The authors find that some international stock markets have the same time-varying volatility.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [" This paper defines the news impact curve that measures how new information is incorporated into volatility estimates. Various new and existing ARCH models, including a partially nonparametric one, are compared and estimated with daily Japanese stock return data. New diagnostic tests are presented that emphasize the asymmetry of the volatility response to news. The authors' results suggest that the model by L. Glosten, R. Jagannathan, and D. Runkle (1989) is the best parametric model. The EGARCH also can capture most of the asymmetry; however, there is evidence that the variability of the conditional variance implied by the EGARCH is too high. Copyright 1993 by American Finance Association."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['The purpose of this paper is to examine the intra-daily volatility of the yen/dollar exchange rate over three different regimes from 1979 to 1988 which correspond to different degrees of international policy coordination. In each regime we test for heat wave vs. meteor shower effects. The heat wave hypothesis assumes that volatility has only country specific autocorrelations, while the meteor shower hypothesis allows volatility spillovers from one market to the next. Meteor showers can be caused by stochastic policy coordination, by gradual release of private information, or by market failures such as fads, bubbles or bandwagons. The rejection of the heat wave model over the first half of the 1980s discredits the stochastic policy coordination interpretation because there was little policy coordination among industrial countries prior to the Plaza Agreement in 1985.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' A semiparametric ARCH model is introduced with conditional first and second moments given by ARMA and ARCH formulations, and a conditional density that is approximated by a nonparametric density estimator. For several densities, the relative efficiency of the quasi-maximum likelihood estimator is compared with maximum likelihood under correct specification. These potential efficiency gains for a fully adaptive procedure are compared in a Monte Carlo experiment with the observed gains from using the semiparametric procedure, and it is found that the estimator captures a substantial proportion of the potential. The estimator is applied to daily stock returns and to the British pound/dollar exchange rate.'] ['Asset pricing relations are developed for a vector of assets with a time varying covariance structure. Assuming that the eigenvectors are constant but the eigenvalues changing, both the Capital Asset Pricing Model and the Arbitrage Pricing Theory suggest the same testable implication: the time varying part of risk premia are proportional to the time varying eigenvalues. Specifying the eigenvalues as general ARCH processes. the model is a multivariate Factor ARCH model. Univariate portfolios corresponding to the eigenvectors will have (time varying) risk premia proportional to their own (time varying) variance and can be estimated using the GARCH-M model. This structure is applied to monthly treasury bills from two to twelve months maturity and the value weighted NYSE returns index. The bills appear to have a single factor in the variance process and this factor is influenced or "caused in variance" by the stock returns.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' This paper seeks to explain the causes of volatility clustering in exchange rates. Careful examination of intra-daily exchange rates provides a test of two hypotheses--heat waves and meteor showers. The heat wave hypothesis is that the volatility in one market is predicted only by the past of that market. The meteor shower hypothesis is that intra-daily volatility spills over from one market to the next. Using the GARCH model to specify the heteroskedasticity across intra-daily market segments, we find that the empirical evidence is generally against the null hypothesis of the heat wave. Using a volatility type of vector autoregression we examine the impact of news in one market on the time path of per-hour volatility in other markets. Copyright 1990 by The Econometric Society.'] ['No abstract is available for this item.'] [' The capital asset pricing model provides a theoretical structure for the pricing of assets with uncertain returns. The premium to induc e risk-averse investors to bear risk is proportional to the nondivers ifiable risk, which is measured by the covariance of the asset return with the market portfolio return. In this paper, a multivariate, gen eralized-autoregressive, conditional, heteroscedastic process is esti mated for returns to bills, bonds, and stocks where the expected retu rn is proportional to the conditional covariance of each return with that of a fully diversified or market portfolio. It is found that the conditional covariances are quite variable over time and are a signi ficant determinant of the time-varying risk premia. The implied betas are also time varying and forecastable. Copyright 1988 by University of Chicago Press.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' The relationship between cointegration and error correction models, first suggested by Granger, is here extended and used to develop estimation procedures, tests, and empirical examples. A vector of time series is said to be cointegrated with cointegrating vector a if each element is stationary only after differencing while linear combinations a8xt are themselves stationary. A representation theorem connects the moving average , autoregressive, and error correction representations for cointegrated systems. A simple but asymptotically efficient two-step estimator is proposed and applied. Tests for cointegration are suggested and examined by Monte Carlo simulation. A series of examples are presented. Copyright 1987 by The Econometric Society.'] [" The expectati on of the excess holding yield on a long bond is postulated to depend upon its conditional variance. Engle's ARCH model is extended to allow the conditional variance to be a determinant of the mean and is called ARCH-M. Estimation and infer ence procedures are proposed, and the model is applied to three interest rate data sets. In most cases the ARCH process and the time varying risk premium are highly significant. A collection of LM diagnostic tests reveals the robustness of the model to various specification changes such as alternative volatility or ARCH measures, regime changes, and interest rate formulations. The model explains and interprets the recent econometric failures of the expectations hypothesis of the term structure. Copyright 1987 by The Econometric Society."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['Because utilities bill their residential and commercial customers by cycle on each working day of the month, the calculation of weather variables to associate with monthly sales data is complicated. We examined three different methods of calculating weather variables. 1.(1) For a utility that bills monthly, the most appropriate method is to calculate daily weather measures, then take a weighted sum of these daily measures over the current and previous month, with the weights for each day being proportional to the number of customers whose consumption on that day is billed in the current month. When weather variables are calculated in this way, accurate econometric models of electricity sales can be estimated.2.(2) If data on the number of customers in each cycle are unavailable, the first procedure can be applied under an assumption concerning the number of customers consuming on each day. For the three utilities in the study, using these approximate weights reduced the model accuracy noticeably but not substantially, implying: if data on the number of customers in each cycle can be retrieved, the effort expended in doing so will be rewarded with more accurate models; however, if such data are impossible to obtain, fairly accurate models can still be estimated.3.(3) The easiest method for calculating weather variables is to ignore the billing cycle phenomenon and take an unweighted sum of daily weather measures over days in the previous or current, or both, months. Our estimation results indicate that these simple measures decrease the accuracy of the models substantially, implying that the additional effort required to calculate weather variables that reflect the billing cycle phenomenon is clearly worthwhile in terms of increased model accuracy.'] ['No abstract is available for this item.'] ['In spite of the importance of exogeneity in econometric modeling, an unambiguous definition does not seem to have been proposed to date. This lack has not only hindered systematic discussion, it has served to confuse the connections between "casuality" and "exogeneity". Moreover, many existing definitions have been formulated in terms of disturbances from relationships which contain unknown parameters, yet whether or not such disturbances satisfy certain orthogonality conditions with other observables may be a matter of construction or may be a testable hypothesis : a clear distinction between these situations is essential. To achieve such an objective, we formulate definitions in terms of the distributions of the observable variables, distinguishing between exogeneity assumptions and causality assumptions, where causality is used in the sense of Granger (1969). Following in particular Koopman\'s pioneering article (1950), exogeneity will be related to the statistical completeness of a model. In short, a variable will be considered exogenous for a given purpose if a statistical analysis can be conducted conditionally on that variable without loss or relevant sample information<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' This paper proposes a new intraday volatility forecasting model, particularly suitable for modeling a large number of assets. We decompose volatility of high-frequency returns into components that may be easily interpreted and estimated. The conditional variance is a product of daily, diurnal, and stochastic intraday components. This model is applied to a comprehensive sample consisting of 10-minute returns on more than 2500 US equities. Apart from building a new model, we obtain several interesting forecasting results. We apply a number of different specifications. We estimate models for separate companies, pool data into industries, and consider other criteria for grouping returns. In general, forecasts from pooled cross-section of companies outperform the corresponding forecasts from company-by-company estimation. For less liquid stocks, however, we obtain better forecasts when we group less frequently traded companies together. Copyright The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com., Oxford University Press.']