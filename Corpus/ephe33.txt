 ['We investigate alternative robust approaches to forecasting, using a new class of robust devices, contrasted with equilibrium-correction models. Their forecasting properties are derived facing a range of likely empirical problems at the forecast origin, including measurement errors, impulses, omitted variables, unanticipated location shifts and incorrectly included variables that experience a shift. We derive the resulting forecast biases and error variances, and indicate when the methods are likely to perform well. The robust methods are applied to forecasting US GDP using autoregressive models, and also to autoregressive models with factors extracted from a large dataset of macroeconomic variables. We consider forecasting performance over the Great Recession, and over an earlier more quiescent period.'] [' Many economic models (such as the new-Keynesian Phillips curve, NKPC) include expected future values, often estimated after replacing the expected value by the actual future outcome, using Instrumental Variables (IV) or Generalized Method of Moments (GMM). Although crises, breaks, and regime shifts are relatively common, the underlying theory does not allow for their occurrence. We show the consequences for such models of breaks in data processes, and propose an impulse-indicator saturation test of such specifications, applied to USA and Euro-area NKPCs.'] ['Unpredictability arises from intrinsic stochastic variation, unexpected instances of outliers, and unanticipated extrinsic shifts of distributions. We analyze their properties, relationships, and different effects on the three arenas in the title, which suggests considering three associated information sets. The implications of unanticipated shifts for forecasting, economic analyses of efficient markets, conditional expectations, and inter-temporal derivations are described. The potential success of general-to-specific model selection in tackling location shifts by impulse-indicator saturation is contrasted with the major difficulties confronting forecasting.'] ['When a model under-specifies the data generation process, model selection can improve over estimating a prior specification, especially if location shifts occur. Impulse-indicator saturation (IIS) can \xe2\x80\x98correct\xe2\x80\x99 non-constant intercepts induced by location shifts in omitted variables, which leave slope parameters unaltered even when correlated with included variables. Location shifts in included variables induce changes in estimated slopes when there are correlated omitted variables. IIS helps mitigate the adverse impacts of induced location shifts on non-constant intercepts and estimated standard errors, and can provide an automatic intercept correction to improve forecasts following location shifts.'] ['No abstract is available for this item.'] ['We consider forecasting with factors, variables and both, modeling in-sample using Autometrics so all principal components and variables can be included jointly, while tackling multiple breaks by impulse-indicator saturation. A forecast-error taxonomy for factor models highlights the impacts of location shifts on forecast-error biases. Forecasting US GDP over 1-, 4- and 8-step horizons using the dataset from Stock and Watson (2009) updated to 2011:2 shows factor models are more useful for nowcasting or short-term forecasting, but their relative performance declines as the forecast horizon increases. Forecasts for GDP levels highlight the need for robust strategies, such as intercept corrections or differencing, when location shifts occur as in the recent financial crisis.'] ["General unrestricted models (GUMs) may include important individual determinants, many small relevant effects, and irrelevant variables.\xef\xbf\xbd Automatic model selection procedures can handle perfect collinearity and more candidate variables than observations, allowing substantial dimension reduction from GUMs with salient regressors, lags, non-linear transformations, and multiple location shifts, together with all the principal components representing 'factor' structures, which can also capture small influences that selection may not retain individually.\xef\xbf\xbd High dimensional GUMs and even the final model can implicitly include more variables than observations entering via 'factors'.\xef\xbf\xbd We simulate selection in several special cases to illustrate.<p>(This abstract was borrowed from another version of this item.)"] ['We consider model selection facing uncertainty over the choice of variables and the occurrence and timing of multiple location shifts. General-to-simple selection is extended by adding an impulse indicator for every observation to the set of candidate regressors: see Johansen and Nielsen (2009). We apply that approach to a fat-tailed distribution, and to processes with breaks: Monte Carlo experiments show its capability of detecting up to 20 shifts in 100 observations, while jointly selecting variables. An illustration to US real interest rates compares impulse-indicator saturation with the procedure in Bai and Perron (1998).'] ['Reducing the number of over-identifying instruments, or adding them to a structural equation, increases estimation dispersion. Added instruments should be insignificant under correct specification, with parameter estimates nearly unaffected, confirmed by Monte Carlo. Selecting instruments does not affect these results.'] ['Economies are so high dimensional and non-constant that many features of models cannot be derived by prior reasoning, intrinsically involving empirical discovery and requiring theory evaluation. Despite important differences, discovery and evaluation in economics are similar to those of science. Fitting a pre-specified equation limits discovery, but automatic methods can formulate much more general initial models with many possible variables, long lag lengths and non-linearities, allowing for outliers, data contamination, and parameter shifts; then select congruent parsimonious-encompassing models even with more candidate variables than observations, while embedding the theory; finally rigorously evaluate selected models to ascertain their viability.'] [" We revisit equilibrium-correction modelling of aggregate real consumers' expenditure in the UK, using Autometrics applied to the data in Davidson, Hendry, Srba and Yeo (DHSY; 1978). The many selection decisions involved in developing viable empirical models are discussed in a setting where there are more candidate explanatory variables than observations, here due to Impulse-Indicator Saturation (IIS) for detecting breaks, outliers and data contamination. Additional tests of the selected model include whether it encompasses the original specification, evidence of nonlinearity and if the conditioning variables are super exogenous. We consider how IIS affects economic interpretations of models, and conversely, and the implications of robust forecasting devices."] ['We outline a range of criteria for evaluating model selection approaches that have been used in the literature. Focusing on three key criteria, we evaluate automatically selecting the relevant variables in an econometric model from a large candidate set. General-to-specific selection is outlined for a regression model in orthogonal variables, where only one decision is required to select, irrespective of the number of regressors. Comparisons with an automated model selection algorithm, Autometrics (Doornik, 2009), show similar properties, but not restricted to orthogonal cases. Monte Carlo experiments examine the roles of post-selection bias corrections and diagnostic testing as well as evaluate selection in dynamic models by costs of search versus costs of inference.'] ['Economies are buffeted by natural shocks, wars, policy changes, and other unanticipated events. Observed data can be subject to substantial revisions. Consequently, a \xc2\x93correct\xc2\x94 theory can manifest serious mis-specification if just fitted to data ignoring its time-series characteristics. Modelling U.S. expenditure on food, the simplest theory implementation fails to describe the evidence. Embedding that theory in a general framework with dynamics, outliers and structural breaks and using impulse-indicator saturation, the selected model performs well, despite commencing with more variables than observations (see Doornik, 2009b), producing useful robust forecasts. Although this illustration involves a simple theory, the implications are generic and apply to sophisticated theories.'] ['To forecast an aggregate, we propose adding disaggregate variables, instead of combining forecasts of those disaggregates or forecasting by a univariate aggregate model. New analytical results show the effects of changing coefficients, mis-specification, estimation uncertainty and mis-measurement error. Forecastorigin shifts in parameters affect absolute, but not relative, forecast accuracies; mis-specification and estimation uncertainty induce forecast-error differences, which variable-selection procedures or dimension reductions can mitigate. In Monte Carlo simulations, different stochastic structures and interdependencies between disaggregates imply that including disaggregate information in the aggregate model improves forecast accuracy. Our theoretical predictions and simulations are corroborated when forecasting aggregate US inflation pre- and post 1984 using disaggregate sectoral data. JEL Classification: C51, C53, E31<p>(This abstract was borrowed from another version of this item.)'] ['A new test for non-linearity in the conditional mean is proposed using functions of the principal components of regressors. The test extends the non-linearity tests based on Kolmogorov-Gabor polynomials ([Thursby and Schmidt, 1977], [Tsay, 1986] and [Ter\xc3\xa4svirta et\xc3\x82 al., 1993]), but circumvents problems of high dimensionality, is equivariant to collinearity, and includes exponential functions, so is a portmanteau test with power against a wide range of possible alternatives. A Monte Carlo analysis compares the performance of the test to the optimal infeasible test and to alternative tests. The relative performance of the test is encouraging: the test has the appropriate size and has high power in many situations.'] ['No abstract is available for this item.'] [' Given a need for nowcasting, we consider how nowcasts can best be achieved, the use and timing of information, including disaggregation over variables and common features, and the role of automatic model selection for nowcasting missing disaggregates. We focus on the impact of location shifts on nowcast failure and nowcasting during breaks, using impulse saturation, its relation to intercept correction, and to robust methods to avoid systematic nowcast failure. We propose a nowcasting strategy, building models of all N disaggregate series by automatic methods, forecasting every variable each period, then testing for shifts in available measures, switching to robust forecasts of missing series when breaks are detected. Copyright \xc2\xa9 2009 John Wiley &amp; Sons, Ltd.'] ["When location shifts occur, cointegration-based equilibrium-correction models (EqCMs) face forecasting problems. We consider alleviating such forecast failure by updating, intercept corrections, differencing, and estimating the future progress of an 'internal' break. Updating leads to a loss of cointegration when an EqCM suffers an equilibrium-mean shift, but helps when collinearities are changed by an 'external' break with the EqCM staying constant. Both mechanistic corrections help compared to retaining a pre-break estimated model, but an estimated model of the break process could outperform. We apply the approaches to EqCMs for UK M1, compared with updating a learning function as the break evolves."] ["As it is almost 50 years since the Phillips curve, we analyze an historical series on UK wages and their determinants [see Phillips, A.W.H., 1958. The relation between unemployment and the rate of change of money wage rates in the United Kingdom, 1861-1957. Economica, 25, 283-299]. Huge changes have occurred over this long-run, so congruence is hard to establish: real wages have risen more than 6 fold, and nominal 500 times; laws, technology, wealth distribution, and social structure are unrecognizably different from 1860. We investigate: wage rates and weekly earnings; real versus nominal wages; breaks over 1860-2004; non-linearities, including Phillips' non-linear response to unemployment; 'trade union power' and unemployment benefits; and measures of excess demand, where workers react more to inflation when it rises."] ['No abstract is available for this item.'] ['We consider the reasons for nowcasting, the timing of information and sources thereof, especially contemporaneous data, which introduce different aspects compared to forecasting. We allow for the impact of location shifts inducing nowcast failure and nowcasting during breaks, probably with measurement errors. We also apply a variant of the nowcasting strategy proposed in Castle and Hendry (2009) to nowcast Euro Area GDP growth. Models of disaggregate monthly indicators are built by automatic methods, forecasting all variables that are released with a publication lag each period, then testing for shifts in available measures including survey data, switching to robust forecasts of missing series when breaks are detected.'] ['No abstract is available for this item.'] ['In \xc2\x93Excessive Ambitions," Jon Elster criticizes a wide range of social science aspirations to understand a complicated and evolving reality. Some of his analysis is to the point, but some is flawed, as explained in my comments. Crucially, however, his conclusions on empirical modeling are diametrically opposite to what is required\xc2\x96the problem has been a serious lack of ambition. And this is precisely the area where Elster is most guilty of \xc2\x91criticizing others on the basis of third-party authorities.\' Notwithstanding \xc2\x91pitfalls and fallacies in statistical data analysis,\' heterogeneous, high-dimensional objects like economies, which are subject to large, intermittent, and usually unanticipated, shifts, require ambitious approaches to characterize their behavior. I will try and explain how more ambitious empirical objectives can be achieved by automatic modeling methods which enhance human capabilities in tackling complicated data problems. En route, I re-emphasize the closely linked explanation for forecast failure recently discussed by myself, Michael Clements and Neil Ericsson in this journal. That leaves open the key issue as to why unanticipated shifts occur, and I speculate on that lacuna in existing economic theories, most of which omit any discussion of the mean levels of variables, and almost none address why such means might shift.'] [' The objective of this paper is to apply the mis-specification (M-S) encompassing perspective to the problem of choosing between "linear" and "log-linear" unit-root models. A simple M-S encompassing test, based on an auxiliary regression stemming from the conditional second moment, is proposed and its empirical size and power are investigated using Monte Carlo simulations. It is shown that by focusing on the conditional process the sampling distributions of the relevant statistics are well behaved under both the null and alternative hypotheses. The proposed M-S encompassing test is illustrated using US total disposable income quarterly data. Copyright (c) Blackwell Publishing Ltd and the Department of Economics, University of Oxford, 2008.'] ['This article explains the basis for a theory of economic forecasting developed over the past decade by the authors. The research has resulted in numerous articles in academic journals, two monographs, Forecasting Economic Time Series, 1998, Cambridge University Press, and Forecasting Nonstationary Economic Time Series, 1999, MIT Press, and three edited volumes, Understanding Economic Forecasts, 2001, MIT Press, A Companion to Economic Forecasting, 2002, Blackwells, and the Oxford Bulletin of Economics and Statistics, 2005. The aim here is to provide an accessible, non-technical, account of the main ideas. The interested reader is referred to the monographs for derivations, simulation evidence, and further empirical illustrations, which in turn reference the original articles and related material, and provide bibliographic perspective.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' An open question in empirical economics is whether models should be estimated by using the actual, or linear, values of economic variables or their logarithms. This paper applies the principle of encompassing to suggest specification and mis-specification tests of log vs. linear individual equations fitted to "I"(1) data, and illustrates the analysis for US quarterly disposable income. The finite-sample properties of the encompassing tests are examined in a Monte Carlo experiment customized to the parameter values found in the empirical analysis. Copyright (c) Blackwell Publishing Ltd and the Department of Economics, University of Oxford, 2008.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['In this paper, we extend the impulse saturation algorithm to a class of dynamic models. We show that the procedure is still correctly sized for stationary AR(1) processes, independently of the number of splits used for sample partitions. We derive theoretical power when there is an additive outlier in the data, and present simulation evidence showing good empirical rejection frequencies against such an alternative. Extensive Monte Carlo evidence is presented to document that the procedure has good power against a level shift in the last rT% of the sample observations. This result does not depend on the level of serial correlation of the data and does not require the use of a (mis-specified) location-scale model, thus opening the door to an automatic class of break tests that could outperform those of the Bai-Perron type.'] ['No abstract is available for this item.'] [' After reviewing the simulation performance of general-to-specific automatic regression-model selection, as embodied in "PcGets", we show how model selection can be non-distortionary: approximately unbiased \'selection estimates\' are derived, with reported standard errors close to the sampling standard deviations of the estimated DGP parameters, and a near-unbiased goodness-of-fit measure. The handling of theory-based restrictions, non-stationarity and problems posed by collinear data are considered. Finally, we consider how "PcGets" can handle three \'intractable\' problems: more variables than observations in regression analysis; perfectly collinear regressors; and modelling simultaneous equations without "a priori" restrictions. Copyright 2005 Royal Economic Society.'] [' Although out-of-sample forecast performance is often deemed to be the \'gold standard\' of evaluation, it is not in fact a good yardstick for evaluating models in general. The arguments are illustrated with reference to a recent paper by Carruth, Hooker and Oswald ["Review of Economics and Statistics" (1998), Vol. 80, pp. 621-628], who suggest that the good dynamic forecasts of their model support the efficiency-wage theory on which it is based. Copyright 2005 Blackwell Publishing Ltd.'] [' Ordinary least squares estimation of an impulse-indicator coefficient is inconsistent, but its variance can be consistently estimated. Although the ratio of the inconsistent estimator to its standard error has a "t"-distribution, that test is inconsistent: one solution is to form an index of indicators. We provide Monte Carlo evidence that including a plethora of indicators need not distort model selection, permitting the use of many dummies in a general-to-specific framework. Although White\'s (1980) heteroskedasticity test is incorrectly sized in that context, we suggest an easy alteration. Finally, a possible modification to impulse \'intercept corrections\' is considered. Copyright 2005 Blackwell Publishing Ltd.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['We evaluate the asymptotic and finite-sample properties of direct multi-step estimation (DMS) for forecasting at several horizons. For forecast accuracy gains from DMS in finite samples, mis-specification and non-stationarity of the DGP are necessary, but when a model is well-specified, iterating the one-step ahead froecasts may not be asymptotically preferable. If a model is mis-specified for a non-stationary DGP, in particular omitting either negative residual serial correlation or regime shifts, DMS can forecast more accurately. Monte Carlo simulations clarify the non-linear dependence of the estimation and forecast biases on the parameters of the DGP, and explain existing results.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] [' The controversy over the selection of \'growth regressions\' was precipitated by some remarkably numerous \'estimation\' strategies, including two million regressions by Sala-i-Martin ["American Economic Review" (1997b) Vol. 87, pp. 178-183]. Only one regression is really needed, namely the general unrestricted model, appropriately reduced to a parsimonious encompassing, congruent representation. We corroborate the findings of Hoover and Perez ["Oxford Bulletin of Economics and Statistics" (2004) Vol. 66], who also adopt an automatic general-to-simple approach, despite the complications of data imputation. Such an outcome was also achieved in just one run of "PcGets", within a few minutes of receiving the data set in Fern\xc3\xa1ndez, Ley and Steel ["Journal of Applied Econometrics" (2001) Vol. 16, pp. 563-576] from Professor Ley. Copyright 2004 Blackwell Publishing Ltd.'] ['We consider forecasting using a combination, when no model coincides with a non-constant data generation process (DGP). Practical experience suggests that combining forecasts adds value, and can even dominate the best individual device. We show why this can occur when forecasting models are differentially mis-specified, and is likely to occur when the DGP is subject to location shifts. Moreover, averaging may then dominate over estimated weights in the combination. Finally, it cannot be proved that only non-encompassed devices should be retained in the combination. Empirical and Monte Carlo illustrations confirm the analysis. Copyright Royal Economic Socciety 2004'] [' We establish the consistency of the selection procedures embodied in "PcGets", and compare their performance with other model selection criteria in linear regressions. The significance levels embedded in the "PcGets" Liberal and Conservative algorithms coincide in very large samples with those implicit in the Hannan-Quinn (HQ) and Schwarz information criteria (SIC), respectively. Thus, both "PcGets" rules are consistent under the same conditions as HQ and SIC. However, "PcGets" has a rather different finite-sample behaviour. Pre-selecting to remove many of the candidate variables is confirmed as enhancing the performance of SIC. Copyright 2003 Blackwell Publishing Ltd.'] [' This paper describes some recent advances and contributions to our understanding of economic forecasting. The framework we develop helps explain the findings of forecasting competitions and the prevalence of forecast failure. It constitutes a general theoretical background against which recent results can be judged. We compare this framework to a previous formulation, which was silent on the very issues of most concern to the forecaster. We describe a number of aspects which it illuminates, and draw out the implications for model selection. Finally, we discuss the areas where research remains needed to clarify empirical findings which lack theoretical explanations.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' We analyse by simulation the impact of model-selection strategies (sometimes called pre-testing) on forecast performance in both constant-and non-constant-parameter processes. Restricted, unrestricted and selected models are compared when either of the first two might generate the data. We find little evidence that strategies such as general-to-specific induce significant over-fitting, or thereby cause forecast-failure rejection rates to greatly exceed nominal sizes. Parameter non-constancies put a premium on correct specification, but in general, model-selection effects appear to be relatively small, and progressive research is able to detect the mis-specifications. Copyright Royal Economic Society, 2002'] ["Since forecast failure is due to unanticipated location shifts, 'sensible' agents should adopt 'robust forecasting rules'. In such a non-stationary world, causal variables can dominate non-causal in forecasting, so 'rational expectations' do not have a sound basis: agents cannot know how all relevant information enters the data density at every point in time. Although econometric models 'break down' intermittently, that is not due to the Lucas critique and need not preclude policy analyses."] [" UK inflation has varied greatly in response to many economic policy and exchange-rate regime shifts, two world wars and two oil crises, as well as legislative and technological changes. Inflation is modelled as responding to excess demands from all sectors of the economy: goods and services, factors of production, money, financial assets, foreign exchange, and government deficits. Equilibrium-correction terms are developed for each of these over the sample. Indicator variables and commodity prices capture turbulent years. Variables representative of most theories of inflation matter empirically, yielding an eclectic model inconsistent with any 'single-cause' explanation. Copyright \xc2\xa9 2001 John Wiley &amp; Sons, Ltd."] [" Existing methods of reconstructing historical Euro-zone data by aggregation of the individual countries' aggregate data raises numerous difficulties, especially due to past exchange rate changes. The approach proposed here is designed to avoid such distortions, and aggregate exactly when exchange rates are fixed. We first compute growth rates within states, aggregate these, then cumulate this Euro-zone growth rate to obtain the aggregated levels variables. The aggregate of the implicit-deflator price index coincides with the implicit deflator of our aggregate nominal and real data. We apply the method to Eurozone M3, GDP and prices over the previous two decades."] ['No abstract is available for this item.'] ['That econometric methodology remains in dispute partly reflects the lack of clear evidence on alternative approaches. This paper reconsiders econometric model selection from a computer-automation perspective, focusing on general-to-specific reduction approaches, as embodied in the program PcGets (general-to-specific). Starting from a general linear, dynamic statistical model, which captures the essential data characteristics, standard testing procedures are applied to eliminate statistically-insignificant variables, using diagnostic tests to check the validity of the reductions, ensuring a congruent final model. As the joint issue of variable selection and diagnostic testing eludes most attempts at theoretical analysis, a simulation-based analysis of modelling strategies is presented. The results of the Monte Carlo experiments cohere with the established theory: PcGets recovers the DGP specification with remarkable accuracy. Empirical size and power of PcGets are close to what one would expect if the DGP were known.<p>(This abstract was borrowed from another version of this item.)'] ['Using annual observations on industrial production over the last three centuries, and on GDP over a 100-year period, we seek an historical perspective on the forecastability of these UK output measures. The series are dominated by strong upward trends, so we consider various specifications of this, including the local linear trend structural time-series model, which allows the level and slope of the trend to vary. Our results are not unduly sensitive to how the trend in the series is modelled: the average sizes of the forecast errors of all models, and the wide span of prediction intervals, attests to a great deal of uncertainty in the economic environment. It appears that, from an historical perspective, the postwar period has been relatively more forecastable.'] [' While there has been a great deal of interest in the modelling of non-linearities in economic time series, there is no clear consensus regarding the forecasting abilities of non-linear time-series models. We evaluate the performance of two leading non-linear models in forecasting post-war US GNP, the self-exciting threshold autoregressive model and the Markov-switching autoregressive model. Two methods of analysis are employed: an empirical forecast accuracy comparison of the two models, and a Monte Carlo study. The latter allows us to control for factors that may otherwise undermine the performance of the non-linear models.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ["'Classical' econometric theory assumes that observed data come from a stationary process, where means and variances are constant over time. Graphs of economic time series, and the historical record of economic forecasting, reveal the invalidity of such an assumption. Consequently, we discuss the importance of stationarity for empirical modeling and inference; describe the effects of incorrectly assuming stationarity; explain the basic concepts of non-stationarity; note some sources of non-stationarity; formulate a class of non-stationary processes (autoregressions with unit roots) that seem empirically relevant for analyzing economic time series; and show when an analysis can be transformed by means of differencing and cointegrating combinations so stationarity becomes a reasonable assumption. We then describe how to test for unit roots and cointegration. Monte Carlo simulations and empirical examples illustrate the analysis."] [' The policy implications of estimated macro-econometric systems depend on the formulations of their equations, the methodology of empirical model selection and evaluation, the techniques of policy analysis, and their forecast performance. Drawing on recent results in the theory of forecasting, we question the role of "rational expectations"; criticize a common approach to testing economic theories; show that impulse-response methods of evaluating policy are seriously flawed; and question the mechanistic derivation of forecasts from econometric systems. In their place, we propose that expectations should be treated as instrumental to agents\' decisions; discuss a powerful new approach to the empirical modelling of econometric relationships; offer viable alternatives to studying policy implications; and note modifications to forecasting devices that can enhance their robustness to unanticipated structural breaks. Copyright 2000 by Oxford University Press.'] ["To explain which methods might win forecasting competitions on economic time series, we consider forecasting in an evolving economy subject to structural breaks, using mis-specified, data-based models. `Causal' models need not win when facing deterministic shifts, a primary factor underlying systematic forecast failure. We derive conditional forecast biases and unconditional (asymptotic) variances to show that when the forecast evaluation sample includes sub-periods following breaks, non-causal models will outperform at short horizons. This suggests using techniques which avoid systematic forecasting errors, including improved intercept corrections. An application to a small monetary model of the UK illustrates the theory."] ['Even though pieces of empirical evidence individually may corroborate an economic theory, their joint existence may refute that same theory. Testing of rational expectations models provides a concrete illustration of this principle. Surprisingly, empirical refutation of a rational expectations model may occur without having to estimate that model, and the refutation may be for a large class of expectations-based models and not just for a particular model specification. Narrow money demand in the United Kingdom illustrates such refutation. The general proposition concerning corroboration and refutation strongly favors the building of empirical models that are consistent with all available evidence.'] [' Kevin Hoover and Stephen Perez take important steps towards resolving some key issues in econometric methodology. They simulate general-to-specific selection for linear, dynamic regression models, and find that their algorithm performs well in re-mining the ?Lovell database?. We discuss developments that improve on their results, automated in PcGets. Monte Carlo experiments and re-analyses of empirical studies show that pre-selection F-tests, encompassing tests, and sub-sample reliability checks all help eliminate ?spuriously-significant? regressors, without impugning recovery of the correct specification.'] ['Several studies have developed empirical models of U.K. money demand using the century of annual and phase-average data in Friedman and Schwartz (1982). The current paper evaluates key models from those studies, employing tests of constancy and encompassing. The evidence strongly favors an annual model from Ericsson, Hendry, and Prestwich (1998a), whereas models based on the phase-average data fare poorly.'] [" Using annual data from M. Friedman and A. Schwartz (1982), D. F. Hendry and N. R. Ericsson (1991) developed an empirical model of the demand for broad money in the United Kingdom over 1878-75. The authors update that model over 1976-93, accounting for changed data definitions and clarifying the concept of constancy. With appropriate measures of opportunity cost and credit deregulation, the model's parameters are empirically constant over the extended sample, which was economically turbulent. Policy implications follow for parameter nonconstancy and predictive failure, causation between money and prices, monetary targeting, deregulation and financial innovation, and the effect of policy on economic agents' behavior. Copyright 1998 by The editors of the Scandinavian Journal of Economics."] [' The paper addresses the practical determination of cointegration rank. This is difficult for many reasons: deterministic terms play a crucial role in limiting distributions, and systems may not be formulated to ensure similarity to nuisance parameters; finite-sample critical values may differ from asymptotic equivalents; dummy variables alter critical values, often greatly; multiple cointegration vectors must be identified to allow inference; the data may be 1(2) rather than 1(1), altering distributions; and conditioning must be done with care. These issues are illustrated by an empirical application of multivariate cointegration analysis to a small model of narrow money, prices, output and interest rates in the UK. Copyright 1998 by Blackwell Publishers Ltd'] ['No abstract is available for this item.'] ['Since the objective of economic policy is to change target variables in the DGP, when economic policy analysis uses an econometric model, it is important that the model delivers reliable inferences about policy responses in the DGP. This requires that the model be congruent and encompassing, and hence exogeneity, causality, cointegration, co-breaking, and invariance all play major roles. We discuss these roles in linear cointegrated VARs, prior to illustrating their importance in a bivariate model of money and interest rates in the UK over the last century.'] ['No abstract is available for this item.'] [" This overview examines conditions for reliable economic policy analysis based on econometric models, focusing on the econometric concepts of exogeneity, cointegration, causality, and invariance. Weak, strong, and super exogeneity are discussed in general and these concepts are then applied to the use of econometric models in policy analysis when the variables are cointegrated. Implications follow for model constancy, the Lucas critique, equation inversion, and impulse response analysis. A small money-demand model for the United Kingdom illustrates the main analytical points. This article then summarizes the other articles in this issue's special section on exogeneity, cointegration, and economic policy analysis."] [' When an econometric model coincides with the mechanism generating the data in an unchanging world, the theory of economic forecasting is reasonably well developed. However, less is known about forecasting when model and mechanism differ in a nonstationary and changing world. This paper addresses the basic concepts; the invariance of forecast accuracy measures to isopmorphic model representations; the roles of causal information, parsimony, and collinearity; a reformulated taxonomy of forecast errors; differencing and intercept corrections to robustify forecasts against biases due to shifts in deterministic factors; the removal of structural breaks by cobreaking; and forecasting using leading indicators. Copyright 1997 by Royal Economic Society.'] [' To reconcile forecast failure with building congruent empirical models, the authors analyze the sources of misprediction. This reveals that an ex ante forecast failure is purely a function of forecast-period events, not determinable from in-sample information. The primary causes are unmodeled shifts in deterministic factors, rather than model misspecification, collinearity, or a lack of parsimony. The authors examine the effects of deterministic breaks on equilibrium-correction mechanisms and consider the role of causal variables. Throughout, Monte Carlo simulation and empirical models illustrate the analysis and support a progressive research strategy based on learning from past failures. Copyright 1997 by Scottish Economic Society.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [" Analyses of forecasting that assume a constant, time-invariant data generating process (DGP), and so implicitly rule out structural change or regime shifts in the economy, ignore an aspect of the real world responsible for some of the more dramatic historical episodes of predictive failure. Some models may offer greater protection against unforeseen structural breaks than others, and various tricks may be employed to robustify forecasts to change. We show that in certain states of nature, vector autoregressions in the differences of the variables (in the spirit of Box-Jenkins time-series-modelling), can outperform vector 'equilibrium-correction' mechanisms. However, appropriate intercept corrections can enhance the performance of the latter, albeit that reductions in forecast bias may only be achieved at the cost of inflated forecast error variances. Copyright 1996 by John Wiley &amp; Sons, Ltd."] [' The application of econometric analysis to the process of economic policy formulation is considered. A framework is provided by the theory of reduction, specifically reductions where key information losses would invalidate policy. Consequently, model evaluation, the role of econometric models, forecasting, exogeneity, causality, constancy and invariance, unobservables, seasonality, and data integrability are considered, together with specific policy issues where econometrics can clarify the problems. Copyright 1996 by Blackwell Publishing Ltd'] ['A model M is said to encompass another model N if the former can explain the results obtained by the latter. In this paper, we propose a general notion of encompassing that covers both classical and Bayesian viewpoints and essentially represents a concept of sufficiency among models. We introduce the parent notion of specificity that aims at measuring lack of encompassing. Tests for encompassing are discussed and the test statistics are compared to Bayesian posterior odds. Operational approximations are offered to cover situations where exact solutions cannot be obtained.'] [' The authors delineate conditions which favor multistep, or dynamic, estimation for multistep forecasting. An analytical example shows how dynamic estimation may accommodate incorrectly specified models as the forecast lead alters, improving forecast performance for some misspecifications. However, in correctly specified models, reducing finite-sample biases does not justify dynamic estimation. In a Monte Carlo forecasting study for integrated processes, estimating a unit root in the presence of a neglected negative moving-average error may favor dynamic estimation, though other solutions exist to that scenario. A second Monte Carlo study obtains the estimator biases and explains those using asymptotic approximations. Copyright 1996 by Blackwell Publishing Ltd'] ["Structural breaks in stationary time series can induce apparent unit roots in those series. Thus, using recently developed recursive Monte Carlo techniques, this paper investigates the properties of several cointegration tests when the marginal process of one of the variables in the cointegrating relationship is stationary with a structural break. The break has little effect on the tests' size. However, tests based on estimated error correction models generally are more powerful than Engle and Granger's two-step procedure employing the Dickey-Fuller unit root test. Discrepancies in power arise when the data generation process does not have a common factor.<p>(This abstract was borrowed from another version of this item.)"] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' We consider the implications for forecast accuracy of imposing unit roots and cointegrating restrictions in linear systems of I(1) variables in levels, differences, and cointegrated combinations. Asymptotic formulae are obtained for multi-step forecast error variances for each representation. Alternative measures of forecast accuracy are discussed. Finite sample behavior in a bivariate model is studied by Monte Carlo using control variables. We also analyse the interaction between unit roots and cointegrating restrictions and intercepts in the DGP. Some of the issues are illustrated with an empirical example of forecasting the demand for M1 in the U.K. Copyright 1995 by John Wiley &amp; Sons, Ltd.'] ['After reviewing the history of analyses of economic forecasting, the role of econometrics in improving economic forecasting is considered, building on CLEMENTS and HENDRY (1994a). The basis of the analysis is a world where model selection is difficult, no model coincides with the economic mechanism, and that mechanism is both non-stationary and evolves over time. On the constructive side, econometric analysis suggests ways of reducing each of the resulting five sources of forecast uncertainty (parameter non-constancy; estimation uncertainty; variable uncertainty; innovation uncertainty; and model mis-specification). On the critical side, the lack of invariance of forecast evaluation procedures to the representation of the model may camouflage inadequate models. We show that forecasts generated from vector autoregressions in differences may be more robust to certain forms of structural change over the forecast period, and that a similar result can be achieved by suitable forms of intercept corrections in vector error-correction mechanisms.'] [' We reconsider the model of Hendry and von Ungern-Sternberg (1981) on a recent data vintage. The data have been extensively revised since their study, such that their model no longer holds over the sample period they used, so the effects of data revisions are studied. Issues of functional form, seasonality, liquidity, uncertainty, dynamics, cointegration, structural change, financial deregulation, inflation and relative prices are considered. A revised specification is proposed which seems congruent with the data evidence and is constant over the sample, and its interpretation is discussed. Copyright 1994 by Oxford University Press.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' Econometric modeling of linear dynamic systems is considered in the light of new reasons for general to simple modeling of the joint data density. To offset the resulting modeling burden due to large numbers of variables, equations, and parameters, the authors consider PcFiml 8 as a modeling tool. Graphics allow vast amounts of information to be appraised at a glance. The demand for M1 in the United Kingdom is modeled as a system using the approach described and establishes that inflation, the interest rate, and output interact closely but are weakly exogenous in the money demand equation. Copyright 1994 by Scottish Economic Society.'] [" Using general to simple methods, J. M. Boughton (1993) develops an econometric model that fits almost as well as Y. Baba, D. F. Hendry, and R. M. Starr (BHS) (1992) but differs in economic implications and dynamic adjustments. He claims the new model is constant, is not encompassed by BHS, but does not encompass BHS. He concludes that the new variables in BHS do not matter for fit or constancy. The authors replicate Boughton's findings but their simplification encompassing test confirms the importance of the novel variables in BHS and shows that BHS encompasses his model. An explanation is offered for its constancy when previous studies suffered predictive failure. Copyright 1993 by Royal Economic Society."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' The paper exposits Wiener distribution theory for I(1) time series as an overview to a special issue on testing integration and cointegration. The behavior of an I(1) series is related to a Wiener process to derive the limiting distribution of its sample mean. Other Wiener processes are related to functions of the normal distribution. The analysis is applied to an autoregressive process, a bivariate regression, and the similarity and power properties of two single-equation tests for cointegration. Systems analyses of cointegration based on the Johansen approach are derived by successive concentration of the likelihood function. An empirical model for Norwegian consumption expenditure is examined. Copyright 1992 by Blackwell Publishing Ltd'] [' Estimated U.S. M1 demand functions appear unstable, regularly "breaking down," over 1960-88 (e.g., missing money, great velocity decline, M1-explosion). The authors propose a money demand function whose arguments include inflation, real income, long-term bond yield and risk, T-bill interest rates, and learning curve weighted yields on newly introduced instruments in M1 and nontransactions M2. The model is estimated in dynamic error-correction form; it is constant and, with an equation standard error of 0.4 percent, variance-dominates most previous models. Estimating alternative specifications explains earlier "breakdowns," showing the model\'s distinctive features to be important in accounting for the data. Copyright 1992 by The Review of Economic Studies Limited.'] [" This paper evaluates an empirical model of U.K. money demand developed by Milton Friedman and Anna J. Schwartz in Monetary Trends in the United States and the United Kingdom. Testing reveals misspecification and, hence, the potential for an improved model. Using recursive procedures on their annual data, the authors obtain a better-fitting, constant, dynamic error-correction (cointegration) model. Results on exogeneity and encompassing imply that the authors' money-demand model is interpretable as a model of money, but not of prices, since its constancy holds only conditionally on contemporaneous prices. Copyright 1991 by American Economic Association."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['Despite the importance of well-specified empirical money-demand functions for inference, forecasting, and policy, problems in modeling have arisen concerning the economic theories of money demand, the data, institutional frameworks, financial innovation, and econometric implementation. By developing constant, data-coherent M1 demand equations for the UK and the US, we investigate these issues and explain such puzzles as "missing money", the great velocity decline, and the recent explosion in M1. The endogeneity of money, the Lucas critique, and the non-invertibility of our M1 models are also discussed.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ["This paper considers the statistical and econometric effect that fixed n-period phase-averaging has on time series generated by some simple dynamic processes. We focus on the variance and autocorrelation of the data series and of the disturbance term for levels and difference equations involving the phase-average data. Further, we examine the effect of phase-averaging on the erogeneity of variables in those equations and the implications phase-averaging has for conducting statistical inference. ; To illustrate our analytical results, we investigate claims by Friedman and Schwartz in their 1982 book Monetary Trends in the United States and the United Kingdom about what the properties of phase-average data and the relationships between those data ought to be. We present certain features of the observed series on velocity, examine how well our analytical model captures them, and contrast them with Friedman and Schwartz's predictions. While our model is an extremely simplified characterization of the phase-averaging adopted by Friedman and Schwartz, it does offer several insights into the likely consequences of their approach.<p>(This abstract was borrowed from another version of this item.)"] ['No abstract is available for this item.'] [' If expectations are an important ingredient of economic decisions, when expectations alter feedback, models should manifest parameter change. This aspect of the "Lucas critique" therefore has implications for feedforward models when expectations processes change but conditional models do not. Both invariance and encompassing attributes of each type of model are investigated to demonstrate that the Lucas critique is refutable, as well as confirmable, even with incomplete information about how agents form expectations. The approach is applied to the transactions demand for money in the United Kingdom and corroborates earlier work by the author by refuting a claimed expectations interpretation. Copyright 1988 by Royal Economic Society.'] ['No abstract is available for this item.'] [' C. M. Kelly (1985) claims that long-run solutions from econometric models may be seriously misleading when expectations variables are erroneou sly replaced by observed outcomes. It is shown that his results deriv e uniquely from an invalid exogeneity assumption. All inferences are therefore potentially invalid, illustrated by a case where the long-r un is correct while the short-run is biased. Using an encompassing fr amework, error-variance rankings and related tests distinguishing exp ectational from conditional models are derived for stationary cases. For nonstationary integrated series, the long-run will be correctly e stimated when the data are cointegrated, whereas the short-run remain s biased. Copyright 1988 by Royal Economic Society.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['In spite of the importance of exogeneity in econometric modeling, an unambiguous definition does not seem to have been proposed to date. This lack has not only hindered systematic discussion, it has served to confuse the connections between "casuality" and "exogeneity". Moreover, many existing definitions have been formulated in terms of disturbances from relationships which contain unknown parameters, yet whether or not such disturbances satisfy certain orthogonality conditions with other observables may be a matter of construction or may be a testable hypothesis : a clear distinction between these situations is essential. To achieve such an objective, we formulate definitions in terms of the distributions of the observable variables, distinguishing between exogeneity assumptions and causality assumptions, where causality is used in the sense of Granger (1969). Following in particular Koopman\'s pioneering article (1950), exogeneity will be related to the statistical completeness of a model. In short, a variable will be considered exogenous for a given purpose if a statistical analysis can be conducted conditionally on that variable without loss or relevant sample information<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.']