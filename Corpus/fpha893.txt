 [' We analyze telecommunications prices in Mexico by using a panel data of countries similar to Mexico to estimate demand models for mobile and fixed-line telecommunications. We find that Mexico\xe2\x80\x99s actual mobile and fixed-line prices are below the predicted prices based on similar countries\xe2\x80\x99 prices. Mexican consumers are paying lower prices than what one would expect based on comparisons of comparable countries. We calculate that in 2011 Mexican consumers received at least $4\xe2\x80\x93$5 billion (USD) in consumer surplus from these lower mobile prices and in 2010 they received over $1 billion (USD) in consumer surplus from these lower fixed-line prices. These findings are in contrast to the general perception that concentrated telecommunications markets in Mexico are resulting in high prices and harming consumers. Copyright Springer Science+Business Media New York 2013'] ['We refute the OECD\xe2\x80\x99s conclusion that Mexico\xe2\x80\x99s telecommunications sector has experienced a lack of competition. The OECD\xe2\x80\x99s conclusion is based on its calculation that high pricing of Mexico\xe2\x80\x99s telecommunications services have caused consumers to lose 129.2 billion (USD) in consumer surplus. The OECD is incorrect. There has been no loss in consumer surplus. In fact, consumers in Mexico have obtained billions of dollars of benefits from lower prices and increased purchases of telecommunications services. The OECD\xe2\x80\x99s contrary conclusions were achieved because of mistakes, the incorrect use of facts and data and the application of incorrect economic analysis. Correct econometric analysis finds no evidence of market failure in Mexico. Mobile prices in Mexico are far below the average prices in other comparable countries (including nine OECD countries). The fixed-line sector performs better than a comparable sample of its peers. Mexican consumers are receiving billions of dollars of benefits from these lower prices.// Refutamos la conclusi\xc3\xb3n de la OCDE que sostiene que el sector de las telecomunicaciones en M\xc3\xa9xico ha experimentado una falta de competencia. La conclusi\xc3\xb3n de la OCDE se basa en sus c\xc3\xa1lculos que indican que los precios altos en los servicios de telecomunicaciones en M\xc3\xa9xico han ocasionado una p\xc3\xa9rdida del excedente del consumidor de 129 mil millones de d\xc3\xb3lares. En realidad, los consumidores mexicanos han obtenido beneficios de miles de millones de d\xc3\xb3lares debido a precios m\xc3\xa1s bajos y mayores compras de servicios de telecomunicaciones. Las conclusiones opuestas de la OCDE provienen de errores, del uso incorrecto de hechos y datos y de la aplicaci\xc3\xb3n del an\xc3\xa1lisis econ\xc3\xb3mico err\xc3\xb3neo. Un an\xc3\xa1lisis econom\xc3\xa9trico correcto revela que no existen pruebas de una falla del mercado en M\xc3\xa9xico. Los precios de la telefon\xc3\xada celular en M\xc3\xa9xico est\xc3\xa1n muy por debajo de los precios promedio de otros pa\xc3\xadses comparables (incluyendo nueve pa\xc3\xadses de la OCDE). El sector de la telefon\xc3\xada fija tiene un mejor desempe\xc3\xb1o que una muestra comprable de sus pares. Los consumidores mexicanos est\xc3\xa1n obteniendo miles de millones de d\xc3\xb3lares en beneficios gracias a estos precios m\xc3\xa1s bajos.'] [' Alfred E. Kahn was an observer and practitioner of telecommunications regulation as technology changed the industry from a natural monopoly to a platform-based oligopoly among telephone, cable, satellite, and wireless carriers. Regulation and legislation were slow to recognize these changes, and large welfare losses occurred, some of which could have been avoided if regulators, legislators and economists had followed Fred\xe2\x80\x99s economic advice: Prices must be informed by costs; the relevant costs are actual incremental costs; costs and prices are an outcome of a Schumpeterian competitive process, not the starting point; excluding firms from markets is fundamentally anticompetitive; a reliance on imperfect markets subject to antitrust law is preferable to necessarily imperfect regulation; and a regulatory transition to deregulation entails propensities to micromanage the process to generate preferred outcomes, visible competitors, and expedient price reductions. Copyright Springer Science+Business Media New York 2013'] ['Since the advent of heteroskedasticity-robust standard errors, several papers have proposed adjustments to the original White formulation. We replicate earlier findings that each of these adjusted estimators performs quite poorly in finite samples. We propose a class of alternative heteroskedasticity-robust tests of linear hypotheses based on an Edgeworth expansion of the test statistic distribution. Our preferred test outperforms existing methods in both size and power for low, moderate, and severe levels of heteroskedasticity.'] ['In this paper, we introduce a new Poisson mixture model for count panel data where the underlying Poisson process intensity is determined endogenously by consumer latent utility maximization over a set of choice alternatives. This formulation accommodates the choice and count in a single random utility framework with desirable theoretical properties. Individual heterogeneity is introduced through a random coefficient scheme with a flexible semiparametric distribution. We deal with the analytical intractability of the resulting mixture by recasting the model as an embedding of infinite sequences of scaled moments of the mixing distribution, and newly derive their cumulant representations along with bounds on their rate of numerical convergence. We further develop an efficient recursive algorithm for fast evaluation of the model likelihood within a Bayesian Gibbs sampling scheme. We apply our model to a recent household panel of supermarket visit counts. We estimate the nonparametric density of three key variables of interest\xe2\x80\x93price, driving distance, and their interaction\xe2\x80\x93while controlling for a range of consumer demographic characteristics. We use this econometric framework to assess the opportunity cost of time and analyze the interaction between store choice, trip frequency, search intensity, and household and store characteristics. We also conduct a counterfactual welfare experiment and compute the compensating variation for a 10%\xe2\x80\x9330% increase in Walmart prices.'] [" From Fred Kahn's writings and experiences as a telecommunications regulator and commenter, we draw the following conclusions: prices must be informed by costs; costs are actual incremental costs; costs and prices are an outcome of a Schumpeterian competitive process, not the starting point; excluding incumbents from markets is fundamentally anticompetitive; and a regulatory transition to deregulation entails propensities to micromanage the process to generate preferred outcomes, visible competitors and expedient price reductions. And most important, where effective competition takes place among platforms characterized by sunk investment--land-line telephony, cable and wireless--traditional regulation is unnecessary and likely to be anticompetitive."] ["Approximately 20 years ago, Peter Diamond and I wrote an article for this journal analyzing contingent valuation methods. At that time Peter's view was that contingent valuation was hopeless, while I was dubious but somewhat more optimistic. But 20 years later, after millions of dollars of largely government-funded research, I have concluded that Peter's earlier position was correct and that contingent valuation is hopeless. In this paper, I selectively review the contingent valuation literature, focusing on empirical results. I find that three long-standing problems continue to exist: 1) hypothetical response bias that leads contingent valuation to overstatements of value; 2) large differences between willingness to pay and willingness to accept; and 3) the embedding problem which encompasses scope problems. The problems of embedding and scope are likely to be the most intractable. Indeed, I believe that respondents to contingent valuation surveys are often not responding out of stable or well-defined preferences, but are essentially inventing their answers on the fly, in a way which makes the resulting data useless for serious analysis. Finally, I offer a case study of a prominent contingent valuation study done by recognized experts in this approach, a study that should be only minimally affected by these concerns but in which the answers of respondents to the survey are implausible and inconsistent."] ['This paper derives the limiting distributions of alternative jackknife instrumental variables (JIV) estimators and gives formulas for accompanying consistent standard errors in the presence of heteroskedasticity and many instruments. The asymptotic framework includes the many instrument sequence of Bekker (1994, Econometrica 62, 657\xe2\x80\x93681) and the many weak instrument sequence of Chao and Swanson (2005, Econometrica 73, 1673\xe2\x80\x931691). We show that JIV estimators are asymptotically normal and that standard errors are consistent provided that null as n \xe2\x86\x92\xe2\x88\x9e, where K and r denote, respectively, the number of instruments and the concentration parameter. This is in contrast to the asymptotic behavior of such classical instrumental variables estimators as limited information maximum likelihood, bias-corrected two-stage least squares, and two-stage least squares, all of which are inconsistent in the presence of heteroskedasticity, unless K n/ rn \xe2\x86\x920. We also show that the rate of convergence and the form of the asymptotic covariance matrix of the JIV estimators will in general depend on the strength of the instruments as measured by the relative orders of magnitude of r and K .'] ["This paper gives a relatively simple, well behaved solution to the problem of many instruments in heteroskedastic data. Such settings are common in microeconometric applications where many instruments are used to improve efficiency and allowance for heteroskedasticity is generally important. The solution is a Fuller (1977) like estimator and standard errors that are robust to heteroskedasticity and many instruments. We show that the estimator has finite moments and high asymptotic efficiency in a range of cases. The standard errors are easy to compute, being likeWhite's (1982), with additional terms that account for many instruments. They are consistent under standard, many instrument, and many weak instrument asymptotics. Based on a series of Monte Carlo experiments, we find that the estimators perform as well as LIML or Fuller (1977) under homoskedasticity, and have much lower bias and dispersion under heteroskedasticity, in nearly all cases considered.<p>(This abstract was borrowed from another version of this item.)"] ["Product variety is an important strategic tool that firms can use to attract customers and respond to competition. This study focuses on the retail industry and investigates how stores manage their product variety, contingent on the presence of competition and their actual distance from rivals. Using a unique data set that contains all Best Buy and Circuit City stores in the United States, the authors find that a store's product variety (i.e., number of stock-keeping units) increases if a rival store exists in its market but, in the presence of such competition, decreases when the rival store is collocated (within one mile of the focal store). Moreover, collocated rival stores tend to differentiate themselves by overlapping less in product range than do noncollocated rivals. This smaller and more differentiated product variety may be because of coordinated interactions between collocated stores. In summary, this paper presents evidence of both coordination and competition in retailers' use of product variety. This paper was accepted by Bruno Cassiman, business strategy."] ['No abstract is available for this item.'] ['We derive the formula for the unilateral price effects of mergers of two products with linear demand in the general asymmetric situation. The formula uses the same information required to calculate upward pricing pressure in the 2010 Horizontal Merger Guidelines.'] ['In this paper, we analyze properties of the Continuous Updating Estimator (CUE) proposed by Hansen et al. (1996), which has been suggested as a solution to the finite sample bias problems of the two-step GMM estimator. We show that the estimator should be expected to perform poorly in finite samples under weak identification, in particular, the estimator is not guaranteed to have finite moments of any order. We propose the Regularized CUE (RCUE) as a solution to this problem. The RCUE solves a modification of the first-order conditions for the CUE estimator and is shown to be asymptotically equivalent to CUE under many weak moment asymptotics. Our theoretical findings are confirmed by extensive Monte Carlo studies.'] ['No abstract is available for this item.'] ['Using many valid instrumental variables has the potential to improve efficiency but makes the usual inference procedures inaccurate. We give corrected standard errors, an extension of Bekker to nonnormal disturbances, that adjust for many instruments. We find that this adjustment is useful in empirical work, simulations, and in the asymptotic theory. Use of the corrected standard errors in t-ratios leads to an asymptotic approximation order that is the same when the number of instrumental variables grows as when the number of instruments is fixed. We also give a version of the Kleibergen weak instrument statistic that is robust to many instruments.'] ['In this paper, we introduce a new flexible mixed model for multinomial discrete choice where the key individual- and alternative-specific parameters of interest are allowed to follow an assumption-free nonparametric density specification, while other alternative-specific coefficients are assumed to be drawn from a multivariate Normal distribution, which eliminates the independence of irrelevant alternatives assumption at the individual level. A hierarchical specification of our model allows us to break down a complex data structure into a set of submodels with the desired features that are naturally assembled in the original system. We estimate the model, using a Bayesian Markov Chain Monte Carlo technique with a multivariate Dirichlet Process (DP) prior on the coefficients with nonparametrically estimated density. We employ a "latent class" sampling algorithm, which is applicable to a general class of models, including non-conjugate DP base priors. The model is applied to supermarket choices of a panel of Houston households whose shopping behavior was observed over a 24-month period in years 2004-2005. We estimate the nonparametric density of two key variables of interest: the price of a basket of goods based on scanner data, and driving distance to the supermarket based on their respective locations. Our semi-parametric approach allows us to identify a complex multi-modal preference distribution, which distinguishes between inframarginal consumers and consumers who strongly value either lower prices or shopping convenience.'] ['We investigate the estimation and inference in difference in difference econometric models used in the analysis of treatment effects. When the innovations in such models display serial correlation, commonly used ordinary least squares (OLS) procedures are inefficient and may lead to tests with incorrect size. Implementation of feasible generalized least squares (FGLS) procedures is often hindered by too few observations in the cross-section to allow for unrestricted estimation of the weight matrix without leading to tests with similar size distortions as conventional OLS based procedures. We analyze the small sample properties of FGLS based tests with a formal higher order Edgeworth expansion that allows us to construct a size corrected version of the test. We also address the question of optimal temporal aggregation as a method to reduce the dimension of the weight matrix. We apply our procedure to data on regulation of mobile telephone service prices. We find that a size corrected FGLS based test outperforms tests based on OLS.'] ['No abstract is available for this item.'] [' Current methods of estimating the random coefficients logit model employ simulations of the distribution of the taste parameters through pseudo-random sequences. These methods suffer from difficulties in estimating correlations between parameters and computational limitations such as the curse of dimensionality. This article provides a solution to these problems by approximating the integral expression of the expected choice probability using a multivariate extension of the Laplace approximation. Simulation results reveal that our method performs very well, in terms of both accuracy and computational time. Copyright 2007 by the Economics Department Of The University Of Pennsylvania And Osaka University Institute Of Social And Economic Research Association.'] [' Non-traditional retail outlets, including supercenters, warehouse club stores, and mass merchandisers, have nearly doubled their share of consumer food-at-home expenditures in the U.S. from 1998 to 2003. Wal-Mart supercenters have had the biggest impact on food retailing as they compete most closely with traditional supermarkets and offer many identical food items at an average price about 15%-25% lower than traditional supermarkets. We consider consumer benefits from this market share growth and estimate the effect on consumer welfare of entry and expansion into new geographic markets. We calculate the compensating variation that arises from both the direct variety effect of the entry of supercenters and the indirect price effect that arises from the increased competition that supercenters create and find the average effect of the total compensating variation to be 25% of food expenditures. Since we find that lower income households tend to shop more at these lower priced outlets, a significant decrease in consumer surplus arises from restricting entry and expansion of supercenters into new geographic markets. Copyright \xc2\xa9 2007 John Wiley &amp; Sons, Ltd.'] ['No abstract is available for this item.'] [" Patent litigation has become an increasingly important consideration in business strategy. Damage awards in patent litigation are supposed to compensate the patent owner for economic harm created by infringement and are therefore important for protecting returns to innovation. We analyze the effects that a recent court decision in the United States, called Grain Processing , has had on the incentives of potential infringers to infringe and innovators to innovate. We find that Grain Processing has decreased the expected value of damages awards in patent cases by conferring a 'free option' on infringers. Grain Processing also concluded that the patent owner in the case did not suffer lost profits due to the infringement because the infringer would have adopted an (inferior) non-infringing technology had it not infringed. We demonstrate that this conclusion is inconsistent with standard economic models. Copyright 2006 The Authors Journal compilation \xef\xbf\xbd 2006 Blackwell Publishing Ltd."] ['No abstract is available for this item.'] ['We demonstrate analytically that for the widely used simultaneous equation model with one jointly endogenous variable and valid instruments, 2SLS has smaller MSE error, up to second order, than OLS unless the R\xc2\xb2, or the F statistic of the reduced form equation is extremely low. We also consider the relative bias of estimators when the instruments are invalid, i.e. the instruments are correlated with the stochastic disturbance. Here, both 2SLS and OLS are biased in finite samples and inconsistent. We investigate conditions under which the approximate finite sample bias or the MSE of 2SLS is smaller than the corresponding statistics for the OLS estimator. We again find that 2SLS does better than OLS under a wide range of conditions, which we characterize as functions of observable statistics and one unobservable statistic.'] ['No abstract is available for this item.'] ['The benefits of competition among the long-distance interexchange carriers (IXCs) are not realized equally by all their customers. Despite the declines in rates under the discount plans, we document that basic message toll service (MTS) rates have been rising for several years. We show that poorer and less educated customers pay more than better educated and more affluent customers. We suspect that the reason for this correlation is that they are more apt to pay the MTS rates or other high rates, and we present some preliminary evidence that this tendency explains the correlation that we find. We also present evidence that the payment differences exist even after controlling for usage. These findings are significant because it seems likely to us that these two patterns (rising MTS rates and higher payments by the poor and the less educated) will each be ameliorated by the entry of the regional Bell operating companies (RBOCs) into long-distance markets\xe2\x80\x94a state-by-state regulatory process that was nearly complete as of the beginning of 2004.'] [' This paper considers estimation of a transformation model in which the transformed dependent variable is subject to classical measurement error. We consider cases in which the transformation function is known and unspecified. In special cases (e.g. log and square-root transformations), least-squares or non-linear least-squares estimators are applicable. A flexible approximation approach (based on Taylor expansion) is proposed for a parametrized transformation function (like the Box--Cox model), and a semi-parametric approach (combining a semi-parametric linear-index estimator and non-parametric regression) is proposed for the case of an unspecified transformation function. The methods are applied to the estimation of earnings equations, using wage data from the Current Population Survey (CPS). Copyright Royal Economic Socciety 2004'] ["In this paper, we consider parameter estimation in a linear simultaneous equations model. It is well known that two-stage least squares (2SLS) estimators may perform poorly when the instruments are weak. In this case 2SLS tends to suffer from the substantial small sample biases. It is also known that LIML and Nagar-type estimators are less biased than 2SLS but suffer from large small sample variability. We construct a bias-corrected version of 2SLS based on the Jackknife principle. Using higher-order expansions we show that the MSE of our Jackknife 2SLS estimator is approximately the same as the MSE of the Nagar-type estimator. We also compare the Jackknife 2SLS with an estimator suggested by Fuller (Econometrica 45, 933--54) that significantly decreases the small sample variability of LIML. Monte Carlo simulations show that even in relatively large samples the MSE of LIML and Nagar can be substantially larger than for Jackknife 2SLS. The Jackknife 2SLS estimator and Fuller's estimator give the best overall performance. Based on our Monte Carlo experiments we conduct informal statistical tests of the accuracy of approximate bias and MSE formulas. We find that higher-order expansions traditionally used to rank LIML, 2SLS and other IV estimators are unreliable when identification of the model is weak. Overall, our results show that only estimators with well-defined finite sample moments should be used when identification of the model is weak. Copyright Royal Economic Socciety 2004"] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' We evaluate the competitive and governance effects of "duality." Duality refers to the joint membership (e.g., by banks) in competing associations or joint ventures (e.g., Visa and MasterCard). We first show that the not-for-profit nature of the associations along with the usage-based fees they charge yield productive efficiency. We then analyze the impact of (i) membership exclusivity, when the associations remain not-for-profit, and (ii) the conversion into for-profit systems. We illustrate the results in the case of a double-differentiation model that is of independent interest. Finally, we discuss extensions to (i) endogenous system differentiation, and (ii) agency considerations. Copyright 2003 by the RAND Corporation.'] ['Four sources of bias in the Consumer Prices Index (CPI) have been identified. The most discussed is substitution bias, which creates a second order bias in the CPI. Three other changes besides prices changes create first order effects on a correctly measured cost of living index (COLI). I explain in this paper that a "pure price" based approach of surveying prices to estimate a COLI cannot succeed in solving the 3 problems of first order bias. I discuss economic and econometric approaches to measuring the first order bias effects as well as the availability of scanner data that would permit implementation of the techniques.'] [' This paper analyzes the competitive effect of a new product introduction. We break the overall competitive effect into two parts: the effect on the prices of existing products due to increased competition, and the effect of having additional product variety. Using data from both before and after the introduction, we directly estimate the price effects and the additional variety effect. Then, using only the estimated post-introduction demand structure, along with an assumed model of competition, we estimate the price effects indirectly. By comparing the "indirect" and "direct" estimates, we assess the validity of alternative models of competition for the industry. Copyright 2002 by Blackwell Publishing Ltd'] [' We develop a new specification test for IV estimators adopting a particular second order approximation of Bekker. The new specification test compares the difference of the forward (conventional) 2SLS estimator of the coefficient of the right-hand side endogenous variable with the reverse 2SLS estimator of the same unknown parameter when the normalization is changed. Under the null hypothesis that conventional first order asymptotics provide a reliable guide to inference, the two estimates should be very similar. Our test sees whether the resulting difference in the two estimates satisfies the results of second order asymptotic theory. Essentially the same idea is applied to develop another new specification test using second-order unbiased estimators of the type first proposed by Nagar. If the forward and reverse Nagar-type estimators are not significantly different we recommend estimation by LIML, which we demonstrate is the optimal linear combination of the Nagar-type estimators (to second order). We also demonstrate the high degree of similarity for "k"-class estimators between the approach of Bekker and the Edgeworth expansion approach of Rothenberg. An empirical example and Monte Carlo evidence demonstrate the operation of the new specification test. Copyright The Econometric Society 2002.'] ['No abstract is available for this item.'] [' The Surface Transportation Board (STB) applies the theory of contestable markets to regulate dominant railroad freight movements. The STB bases its determination whether railroad revenues are excessive if they would be more than sufficient to support investment in a hypothetical stand-alone railroad designed to handle the at-issue traffic efficiently. The STB regulatory approach does not take correct account of the importance of sunk costs and irreversible investments in the railroad industry. We estimate how large the mistakes can be by applying a real options approach that takes into account the effect of sunk costs, irreversible investment, and asymmetric returns. Copyright 2002 by Kluwer Academic Publishers'] ['The effect of mismeasured variables in the most straightforward regression analysis with a single regressor variable leads to a least squares estimate that is downward biased in magnitude toward zero. I begin by reviewing classical issues involving mismeasured variables. I then consider three recent developments for mismeasurement econometric models. The first issue involves difficulties in using instrumental variables. A second involves the consistent estimators that have recently been developed for mismeasured nonlinear regression models. Finally, I return to mismeasured left hand side variables, where I will focus on issues in binary choice models and duration models.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [" Since their introduction in 1983, cellular telephones' adoption has grown at 25 percent-35 percent per year. At year end 1997, about 55 million cellular telephones were in use in the United States. The Bureau of Labor Statistics (BLS) did not know that cellular telephones existed, at least in terms of calculating the Consumer Price Index (CPI), until 1998 when they were finally included in the CPI. Omitting cellular telephones from the CPI created a significant bias. The author estimates a bias in the BLS estimate of the telecommunications-services index of between .8 percent-1.9 percent per year because of the omission of the cellular telephone. Rather than telecommunications-service prices increasing at about 1.1 percent per year, the correct calculation has them decreasing at about .8 percent per year. Omission of new goods from the CPI, for significant periods of time, leads to important bias in the calculation of the CPI."] ['This paper considers mismeasurement of the dependent variable in a general linear index model, which includes qualitative choice models, proportional and additive hazard models, and censored models as special cases. The monotone rank estimator of Cavanagh and Sherman [1998] is shown to be consistent in the presence of any mismeasurement process that obeys a simple stochastic-dominance condition. The emphasis is on measurement error which is independent of the covariates, but extensions to covariate-dependent measurement error are also discussed. We consider the proportional hazard duration model in detail and apply the estimator to mismeasured unemployment duration data from the Survey of Income and Program Participation (SIPP).'] ['No abstract is available for this item.'] [' We model demand for four cephalosporins and compute own- and cross-price elasticities between branded and generic versions of the four drugs. We model demand as a multistage budgeting problem, and we argue that such a model is appropriate to the multistage nature of the purchase of pharmaceutical products, in particular the prescribing and dispensing stages. We find quite high elasticities between generic substitutes and also significant elasticities between some therapeutic substitutes.'] [" An econometric analysis demonstrates that television ratings for NBA games are substantially higher when certain players ('superstars') are involved. Thus, these superstars are quite important for generating revenue, not only for their own teams but for other teams as well. Using the econometric analysis and additional information on attendance and paraphernalia sales, the authors estimate the value of Michael Jordan to the other NBA teams to be approximately $53 million. The positive externality superstars have on other teams can lead to an inefficient distribution of player talent. The authors examine several league policies that might be used to address the externality. Copyright 1997 by University of Chicago Press."] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' The authors apply nonparametric regression models to estimation of demand curves of the type most often used in applied research. From the demand curve estimators they derive estimates of exact consumers surplus and deadweight loss. The authors also develop tests of the symmetry and downward sloping properties of compensated demand. They work out asymptotic normal sampling theory for kernel and series nonparametric estimators as well as for the parametric case. The paper includes an application to gasoline demand that estimates the shape of the demand curve and the average magnitude of welfare loss from a tax on gasoline. Copyright 1995 by The Econometric Society.'] ['No abstract is available for this item.'] ["Without market outcomes for comparison, internal consistency tests, particularly adding-up tests, are needed for credibility. When tested, contingent valuation has failed. Proponents find surveys tested poorly done. To the authors' knowledge, no survey has passed these tests. The 'embedding effect' is the similarity of willingness-to-pay responses that theory suggests (and sometimes requires) be different. This problem has long been recognized but not solved. The authors conclude that current methods are not suitable for damage assessment or benefit-cost analysis. They believe the problems come from an absence of preferences, not a flaw in survey methodology, making improvement unlikely."] ['Differentiated products are the central economic focus of competition in consumer goods products such as cereal, soda, and beer. We first estimate demand models which do not restrict unduly the pattern of consumer preferences as does much previous research in the area of differenciated products. Using recently available transactions data we estimate own and cross price elasticities in a relatively unrestricted manner. We next turn to competitive analysis using our estimated demand system. We consider two applications in this paper. The main economic factor that we consider is that the firms which produce the differentiated products almost always tend to be multi-product firms in the given industry. Our first application is competitive analysis when two firms are allowed to merge. The other application that we consider is inference on the competitive structure in an industry. In both applications we consider the effect of a multi-product firm where its competitive decisions for one brand affects it sales and prices for other brands that it produces.'] ['No abstract is available for this item.'] ['We estimate the conditional distribution of trade-to-trade price changes using ordered probit, a statistical model for discrete random variables. Such an approach takes into account the fact that transaction price changes occur in discrete increments, typically eighths of a dollar, and occur at irregularly spaced time intervals. Unlike existing continuous-time/discrete-state models of discrete transaction prices, ordered probit can capture the effects of other economic variables on price changes, such as volume, past price changes, and the time between trades. Using 1988 transactions data for over 100 randomly chosen U.S. stocks, we estimate the ordered probit model via maximum likelihood and use the parameter estimates to measure several transaction-related quantities, such as the price impact of trades of a given size, the tendency towards price reversals from one transaction to the next, and the empirical significance of price discreteness.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' In this paper we specify and estimate a flexible parametric proportional hazards model. The model specification is flexibly parametric in the sense that the baseline hazard is nonparametric while the effect of the covariates takes a particular functional form. We also add parametric heterogeneity to the underlying hazard model specification. We specify a flexible parametric proportional competing risks model which permits unrestricted correlation among the risks. Unemployment duration data are then analyzed using the flexible parametric duration and competing risks specifications. We find an important effect arising from the exhaustion of unemployment insurance and significantly different hazards for the two types of risks, new jobs and recalls. Copyright 1990 by John Wiley &amp; Sons, Ltd.'] ["This paper evaluates the effects of the 1986 Tax Reform Act on household labor supply and savings. It describes the tax bill's effects on incentives to work and to save, and uses recent econometric estimates of labor supply and savings elasticities to describe the reform's impact on household behavior. Two factors lead us to conclude that the new law will have small aggregate effects. First, most households experience only small changes in their marginal tax rates. Forty-one percent of the taxpaying population will face marginal tax rates as high, or higher, under the new law as under the previous tax code. Only eleven percent of taxpayers receive marginal tax rate reductions of ten percentage points or more. Second, plausible estimates of both the labor supply and savings elasticities suggest that even for those households that receive rate reductions, behavioral changes will be small. Our analysis suggests that the tax reform will increase labor supply by about one percent, and slightly reduce private savings."] ['No abstract is available for this item.'] [' Patent and antitrust policy are often presumed to be in conflict. As an important example, there is ongoing controversy about whether price discrimination by a patent holder is an illegal or socially undesirable exploitation of monopoly power. In this article we show that no conflict exists in many price discrimination cases. Even ignoring the (dynamic) effects on incentives for innovation, third-degree price discrimination by patent holders can raise (static) social welfare. In fact, Pareto improvements may well occur. Welfare gains occur because price discrimination allows patent holders to open new markets and to achieve economies of scale or learning. Further, even in cases where discrimination incurs static welfare losses, it may be efficient relative to other mechanisms, such as length of patent life, for rewarding innovators with profits.'] ["This paper evaluates the effects of the 1986 Tax Reform Act on household labor supply and savings. It describes the tax bill's effects on incentives to work and to save, and uses recent econometric estimates of labor supply and savings elasticities to describe the reform's impact on household behavior. Two factors lead us to conclude that the new law will have small aggregate effects. First, most households experience only small changes in their marginal tax rates. Forty-one percent of the taxpaying population will face marginal tax rates as high, or higher, under the new law as under the previous tax code. Only eleven percent of taxpayers receive marginal tax rate reductions of ten percentage points or more. Second, plausible estimates of both the labor supply and savings elasticities suggest that even for those households that receive rate reductions, behavioral changes will be small. Our analysis suggests that the tax reform will increase labor supply by about one percent, and slightly reduce private savings.<p>(This abstract was borrowed from another version of this item.)"] [" The authors consider estimation of simultaneous equations models with covariance restrictions. They consider FIML estimation and extend J. A. Hausman's instrumental variables interpretation of the FIML estim ator to the covariance restrictions case. A slight variation on the i nstrumental variables theme yields a simple, efficient alternative to FIML. The authors augment the original equation system by equations implied by the covariance restrictions, linearized around an initial consistent estimator, and perform three-stage least squares to obtain an asymptotically efficient estimator. They also present a simple me thod of obtaining an initial consistent estimator when the covariance restrictions are needed for identification. Finally, they consider i dentification from the standpoint of the moment restrictions implied by instrument-residual orthogonality and covariance restrictions. Copyright 1987 by The Econometric Society."] ['No abstract is available for this item.'] ['Not all people with health problems are disabled. Some individuals with severe physical or mental impairments, such as blindness or limb amputation, continue to hold jobs and generally function satisfactorily.They constitute, however, a group of potentially disabled individuals who might apply and qualify for Disability Insurance or other disability-related benefits if they were to lose their jobs or to decide that employment offered an inadequate financial or non-pecuniary reward. Thus, disability, or a health-related inability to work, is more than a medical problem but involves motivational and attitudinal factors. We specify a model of the application process, which we model as choice under uncertainty about approval of an application for Disability Insurance. We specify the possible outcomes to the choice process of an individual in which the probability of acceptance for Disability Insurance is a key consideration. We then estimate a joint model of labor supply and application to the Disability Insurance program based on the 1972 survey. We then compare our results to the observed time series applications process since 1976. Lastly, we estimate the sensitivity of the application process to the probability of acceptance and the level of benefits.<p>(This abstract was borrowed from another version of this item.)'] ['Panel data based on various longitudinal surveys have become ubiquitous in economics in recent years. Estimation using the analysis of covariance approach allows for control of various "individual effects" by estimation of the relevant relationships from the "within" dimension of the data. Quite often, however, the "within" results are unsatisfactory, "too low" and insignificant. Errors of measurement in the independent variables whose relative importance gets magnified in the within dimension are often blamed for this outcome. However, the standard errors-in-variables model has not been applied widely, partly because in the usual micro data context it requires extraneous information to identify the parameters of interest. In the panel data context a variety of errors-in-variables models may be identifiable and estimable without the use of external instruments. We develop this idea and illustrate its application in a relatively simple but not uninteresting case: the estimation of "labor demand" relationships, also known as the "short run increasing returns to scale" puzzle.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['This paper focuses on developing and adapting statistical models of counts (non-negative integers) in the context of panel data and using them to analyze the relationship between patents and R&amp;D; expenditures. The model used is an application and generalization of the Poisson distribution to allow for independent variables; persistent individual (fixed or random) effects, and "noise" or randomness in the Poisson probability function. We apply our models to a data set previously analyzed by Pakes and Griliches using observations on 128 firms for seven years, 1968-74. Our statistical results indicate clearly that to rationalize the data, we need both a disturbance in the conditional within dimension and a different one, with a different variance, in the marginal (between) dimension. Adding firm specific variables, log book value and a scientific industry dummy, removes most of the positive correlation between the individual firm propensity to patent and its R&amp;D; intensity. The other new finding is that there is an interactive negative trend in the patents - R&amp;D; relationship, that is, firms are getting less patents from their more recent R&amp;D; investments, implying a decline in the "effectiveness" or productivity of R&amp;D.;<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['Over the period 1960 - 1983 the proportion of federal tax revenue raised by taxation of labor supply has risen from 57-77 percent. In this paper, we specify and estimate a model of family labor supply which treats both federal and state taxation. Husbands and wives labor supply are treated jointly rather than in aseparate manner as in previous research. A method to calculate the virtual wage for nonworking spouses is used within a utility maximizing framework to treat correctly the joint family labor supply decision. Joint family efforts are found to be important. The efficiency cost (deadweight loss) of labor taxation is estimated to be 29.6% of tax revenue raised. The effect of the new 10% deduction to ease the marriage tax for working spouses leads to a prediction of 3.8% increase in wives labor supply and a .9% decrease in husbands labor supply.Overall taxes paid are predicted to decrease by 3.4%.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['We consider the retirement behavior of civilian employees of the United States government. Unlike previous studies, this investigation is based upon a data set containing fairly complete and accurate information about the Social Security and employer-provided pensions for which employees are (or ultimately will be) eligible. These data permit us to specify the financial aspects of individual retirement decisions with a reasonable degree of precision. A large fraction of civil service pensioners is eligible to receive Social Security benefits because a part of their working careers was spent in Social-Security-covered employment. The prevalence of double pension coverage among government employees has raised serious equity questions about the treatment of civil servants by Social Security, and these questions have led to various suggestions for pension reform. Partly, the reform proposals have been put forward due to the perceived unfairness of "double dipping" which arises from the double pension coverage of government employees. Our analysis finds: (1) Both the amount of a Federal pension entitlement and the expected wait until the pension commences affect the timing of retirement from the Federal service. (2) The rate of anticipated wage growth significantly affects individual decisions to remain in Federal employment. (3) Workers who are eligible to ultimately receive Social Security in some cases show a different pattern of retirement than do workers not vested in Social Security. However, our analysis does not reveal any massive shift of Federal workers into Social-Security-covered employment in order to benefit from the "tilt" in the Social Security formula.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' This paper applies methods for analyzing data from samples that have been chosen by restricting the target population in discernible ways to date from a 1976 experiment in time-of-day pricing of electricity for residential customers in Arizona. We find that whereas conventional estimation methods lead to the conclusion that the peak price elasticity of demand is larger (in absolute value) than either the corresponding midpeak or offpeak elasticity, once truncation bias is accounted for, the peak elasticity is smaller than the other two. This finding accords with a preliminary analysis of data from Wisconsin, where no such sample truncation was present.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' This article presents a model of individual behavior in the purchase and utilization of energy-using durables. The tradeoff between capital costs for more energy efficient appliances and operating costs for the appliances is emphasized. Using data on both the purchase and utilization of room air conditioners, the model is applied to a sample of households. The utilization equation indicates a relatively low price elasticity. The purchase equation, based on a discrete choice model, demonstrates that individuals do trade off capital costs and expected operating costs. The results also show that individuals use a discount rate of about 20 percent in making the tradeoff decision and that the discount rate varies inversely with income.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [" The key question which the Federal Energy Administration's Project Independence Report attempts to answer is the level of U.S. oil imports under different world oil prices. The three essential components of the model which produces the forecasts -- oil and natural gas supply, energy demand, and the integrating model -- are discussed and their potential biases identified. The paper concludes that national gas supply is likely to be higher than the model forecasts, while serious faults exist in the demand model which finds natural gas and oil products to be complements. In a deregulated BTU equilibrium, these biases lead to a more pessimistic forecast of U.S. imports by 1985 than may actually occur."] ['No abstract is available for this item.']