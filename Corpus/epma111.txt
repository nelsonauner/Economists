 ['Attempting to shed light on the optimal size of government, economists have analyzed planning problems that specify a set of feasible taxation-spending policies and a social welfare function. The analysis characterizes the optimal policy choice of a planner who knows the welfare achieved by each policy. This paper examines choice of size of government by a planner who has partial knowledge of population preferences and the productivity of spending. This is a problem of decision making under ambiguity. Focusing on income-tax financed public spending for infrastructure that aims to enhance productivity, I examine scenarios where the planner observes the outcome of a status quo policy and uses various decision criteria (expected welfare, maximin, Hurwicz, minimax-regret) to choose policy. The analysis shows that the planner can reasonably choose a wide range of spending levels--thus, a society can rationalize having a small or large government. I conclude that to achieve credible conclusions about the desirable size of government, we need to vastly improve current knowledge of population preferences and the productivity of public spending.<p>(This abstract was borrowed from another version of this item.)'] [" The merits of alternative income tax policies depend on the population distribution of preferences for income and leisure. Standard theory, which supposes that persons want more income and more leisure, does not predict how they resolve the tension between these desires. Empirical studies of labor supply have imposed strong preference assumptions that lack foundation. This paper examines anew the problem of inference on income\xe2\x80\x93leisure preferences and considers the implications for evaluation of tax policy. I first perform a basic revealed\xe2\x80\x90preference analysis assuming only that persons prefer more income and leisure. This shows that observation of a person's time allocation under a status quo tax policy may bound his allocation under a proposed policy or may have no implications, depending on the tax schedules and the person's status quo time allocation. I next explore the identifying power of two classes of assumptions that restrict the distribution of income\xe2\x80\x93leisure preferences. One assumes that groups of persons who face different choice sets have the same preference distribution. The second restricts the shape of this distribution. The generic finding is partial identification of preferences. This implies partial prediction of tax revenue under proposed policies and partial knowledge of the welfare function for utilitarian policy evaluation."] ['No abstract is available for this item.'] ['Institutions for collective decision making often defer to the status quo, granting it a privileged position relative to proposed policy innovations. The possible benefits of status quo deference must be weighed against a cost: status quo deference can prevent a society from learning the merits of innovations. This paper explores the potential for learning through adaptive diversification of treatment choice in decision systems that feature status quo deference. I first review the basic elements of my earlier analysis of adaptive diversification by a planner and then extend the analysis to two collective decision processes, voting and bilateral negotiation.'] ["This paper develops a formal language for study of treatment response with social interactions, and uses it to obtain new findings on identification of potential outcome distributions. Defining a person's treatment response to be a function of the entire vector of treatments received by the population, I study identification when shape restrictions and distributional assumptions are placed on response functions. An early key result is that the traditional assumption of individualistic treatment response (ITR) is a polar case within the broad class of constant treatment response (CTR) assumptions, the other pole being unrestricted interactions. Important non-polar cases are interactions within reference groups and distributional interactions. I show that established findings on identification under assumption ITR extend to assumption CTR. These include identification with assumption CTR alone and when this shape restriction is strengthened to semi-monotone response. I next study distributional assumptions using instrumental variables. Findings obtained previously under assumption ITR extend when assumptions of statistical independence (SI) are posed in settings with social interactions. However, I find that random assignment of realized treatments generically has no identifying power when some persons are leaders who may affect outcomes throughout the population. Finally, I consider use of models of endogenous social interactions to derive restrictions on response functions. I emphasize that identification of potential outcome distributions differs from the longstanding econometric concern with identification of structural functions.   This paper is a revised version of CWP01/10<p>(This abstract was borrowed from another version of this item.)"] ['We study first- and second-order subjective expectations (beliefs) in strategic decision-making. We elicit probabilistically both first- and second-order beliefs and apply the method to a Hide-and-Seek experiment. We study the relationship between choice and beliefs in terms of whether observed choice coincides with the optimal action given elicited beliefs. We study the relationship between first- and second-order beliefs under a coherence criterion. Weak coherence requires that if an event is assigned, according to first-order beliefs, a probability higher/lower/equal to the one assigned to another event, then the same holds according to second-order beliefs. Strong coherence requires the probability assigned according to first- and second-order beliefs to coincide. Evidence of heterogeneity across participants is reported. Verbal comments collected at the end of the experiment shed light on how subjects think and decide in a complex environment that is strategic, dynamic and populated by potentially heterogeneous individuals.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['We analyze probabilistic expectations of equity returns elicited in the Survey of Economic Expectations in 1999 %uF8182001 and in the Michigan Survey of Consumers in 2002 %uF8182004. Our empirical findings suggest that individuals use interpersonally variable but intrapersonally stable processes to form their expectations. We therefore propose to think of the population as a mixture of expectations types, each forming expectations in a stable but different way. We use our expectations data to learn about the prevalence of several specific types suggested by research in conventional and behavioral finance, but conclude that these types do not adequately explain the diverse expectations held by the population.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ["Survey measures of consumer inflation expectations have an important shortcoming in that, while providing useful summary measures of the distribution of point forecasts across individuals, they contain no direct information about an individual's uncertainty about future inflation. The latter is important not only for forecasting inflation and other macroeconomic outcomes, but also for assessing a central bank's credibility and effectiveness of communication. This paper explores the feasibility of eliciting individual consumers' subjective probability distributions of future inflation outcomes. ; In November 2007, we began administering web-based surveys to participants in RAND's American Life Panel. In addition to their point predictions, respondents were asked for their subjective assessments of the percentage chance that inflation will fall in each of several predetermined intervals. We find that our measures of individual forecast densities and uncertainty are internally consistent and reliable. Those who are more uncertain about year-ahead price inflation are also generally more uncertain about longer term price inflation and future wage changes. We find also that participants expressing higher uncertainty in their density forecasts make larger revisions to their point forecasts over time. Measures of central tendency derived from individual density forecasts are highly correlated with point forecasts, but they usually differ, often substantially, at the individual level. ; Finally, we relate our direct measure of aggregate consumer uncertainty to a more conventional approach that uses disagreement among individual forecasters, as seen in the dispersion of their point forecasts, as a proxy for forecast uncertainty. Although the two measures are positively correlated, our results suggest that disagreement and uncertainty are distinct concepts, both relevant to the analysis of inflation expectations.<p>(This abstract was borrowed from another version of this item.)"] ['Someone reading empirical research relating human genetics to personal outcomes must be careful to distinguish two types of work: An old literature on heritability attempts to decompose cross-sectional variation in observed outcomes into unobservable genetic and environmental components. A new literature measures specific genes and uses them as observed covariates when predicting outcomes. I will discuss these two types of work in terms of how they may inform social policy. I will argue that research on heritability is fundamentally uninformative for policy analysis, but make a cautious argument that research using genes as covariates is potentially informative.'] ['Economists studying choice with partial knowledge typically assume that the decision maker places a subjective distribution on unknown quantities and maximizes expected utility. Someone lacking a subjective distribution faces a problem of choice under ambiguity. This article reviews recent research on policy choice under ambiguity, when the task is to choose treatments for a population. Ambiguity arises when a planner has partial knowledge of treatment response and does not feel able to place a subjective distribution on the unknowns. I first discuss dominance and alternative criteria for choice among undominated policies. I then illustrate with the choice of a vaccination policy by a planner who has partial knowledge of the effect of vaccination on illness. I next study a class of problems in which a planner may want to cope with ambiguity by diversification, assigning observationally identical persons to different treatments. Lastly, I consider a setting in which a planner should not diversify treatment.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['Analyses of public policy regularly express certitude about the consequences of alternative policy choices. Yet policy predictions often are fragile, with conclusions resting on critical unsupported assumptions. Then the certitude of policy analysis is not credible. This paper develops a typology of incredible analytical practices and gives illustrative cases. I call these practices conventional certitudes, dueling certitudes, conflating science and advocacy, and wishful extrapolation. I contrast these practices with my vision for credible policy analysis.<p>(This abstract was borrowed from another version of this item.)'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ["Research on collective provision of private goods has focused on distributional considerations. This paper studies a class of problems of decision under uncertainty in which the argument for collective choice emerges from the mathematics of aggregating individual payoffs. Consider decision making when each member of a population has the same objective function, which depends on an unknown state of nature. If agents knew the state of nature, they would make the same decision. However, they may have different beliefs or may use different decision criteria. Hence, they may choose different actions even though they share the same objective. Let the set of feasible actions be convex and the objective function be concave in actions, for all states of nature. Then Jensen's inequality implies that consensus choice of the mean privately-chosen action yields a larger aggregate payoff than does individualistic decision making, in all states of nature. If payoffs are transferable, the aggregate payoff from consensus choice may be allocated to Pareto dominate individualistic decision making, in all states of nature. I develop these ideas. I also use Jensen's inequality to show that a planner with the power to assign actions to the members of the population should not diversify. Finally, I give a version of the collective choice result that holds with consensus choice of the median rather than mean action.<p>(This abstract was borrowed from another version of this item.)"] [' When choice data are not available, researchers studying preferences sometimes ask respondents to state the actions they would choose in choice scenarios. Data on stated choices are then used to estimate random utility models, as if they are data on actual choices. Stated and actual choices may differ because researchers typically provide respondents less information than they would have in actuality. Elicitation of choice probabilities overcomes this problem by permitting respondents to express uncertainty about behavior. This article shows how to use elicited choice probabilities to estimate random utility models and reports estimates of preferences for electricity reliability. Copyright (2010) by the Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association.'] ["Economists studying collective decision problems often consider how a social planner would behave. The standard exercise presumes complete knowledge of the welfare achieved by each feasible policy. However, we often have only partial knowledge of policy impacts. This paper extends my program of research on planning under ambiguity from settings with individualistic treatment to ones where treatments interact, each person's outcome depending on his treatment and on the population treatment allocation. I consider the problem in abstraction and use medical treatment of an infectious disease to illustrate."] ['We use data from the Survey of Professional Forecasters (SPF) to compare point predictions of gross domestic product (GDP) growth and inflation with the subjective probability distributions held by forecasters. We find that most SPF point predictions are quite close to the central tendencies of forecasters subjective distributions tend to be asymmetric, with SPF forecasters tending to report point predictions that give a more favorable view of the economy than do their subjective means/medians/modes.'] [' Econometric analyses of treatment response often use\xe2\x80\x82instrumental variable\xe2\x80\x82(IV) assumptions to identify treatment effects. The traditional IV assumption holds that mean response is constant across the sub-populations of persons with different values of an observed covariate. Manski and Pepper (2000) introduced\xe2\x80\x82monotone instrumental variable\xe2\x80\x82assumptions, which replace equalities with weak inequalities. This paper presents further analysis of the monotone instrumental variable (MIV) idea. We use an explicit response model to enhance the understanding of the content of MIV and traditional IV assumptions. We study the identifying power of MIV assumptions when combined with the\xe2\x80\x82homogeneous linear response\xe2\x80\x82assumption maintained in many studies of treatment response. We also consider estimation of MIV bounds, with particular attention to finite-sample bias. Copyright (C) The Author(s). Journal compilation (C) Royal Economic Society 2009'] [' This article develops a broad theme about treatment under ambiguity through study of a particular decision criterion. The broad theme is that a planner may want to cope with ambiguity by diversification, assigning observationally identical persons to different treatments. Study of the minimax-regret (MR) criterion substantiates the theme. The article significantly extends my earlier analysis of one-period planning with an individualistic treatment and a linear welfare function. I show that MR treatment allocations are fractional in a large class of planning problems with nonlinear welfare functions, interacting treatments, learning, and noncooperative aspects. Copyright \xef\xbf\xbd (2009) by the Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association.'] ['Charles Manski of Northwestern argues that the drug approval process should be more continuous, so that patients can have access to beneficial drugs earlier and at the same time there are incentives for longer term studies than the current system produces, which could limit problems like those caused by Vioxx.'] ['An important practical objective of empirical studies of treatment response is to provide decision makers with information useful in choosing treatments. Often the decision maker is a planner who must choose treatments for the members of a heterogeneous population; for example, a physician may choose medical treatments for a population of patients. Studies of treatment response cannot provide all the information that planners would like to have, but researchers can be of service by addressing several questions: How should studies be designed in order to be most informative? How should studies report their findings so as to be most useful in decision making? How should planners utilize the information that studies provide? This paper addresses aspects of these broad questions, focusing on pervasive problems of identification that arise when studying treatment response and making treatment choices'] ['No abstract is available for this item.'] [' To provide an empirical basis for the study of expectations, we have undertaken survey research measuring in probabilistic terms the beliefs that Americans hold about equity returns in the year ahead. This paper presents new findings on the expected returns reported in the 2004 Health and Retirement Study. We find substantial heterogeneity of reported beliefs, but, strikingly, nearly two-thirds of respondents report no better than a 50-50 chance of a positive nominal return. As in our earlier work, expected returns decline with age and are higher for men than for women. We find here that the probability of holding stocks increases substantially as the perceived chance of a positive return increases. These findings are potentially of considerable importance for portfolio choice. (JEL: G1, D1, D8) (c) 2007 by the European Economic Association.'] [' This article shows how to predict counterfactual discrete choice behavior when the presumed behavioral model partially identifies choice probabilities. The simple, general approach uses observable choice probabilities to partially infer the distribution of types in the population and then applies the results to predict behavior in unrealized choice settings. Two illustrative applications are given. One assumes only that persons have strict preferences. The other assumes strict preferences and utility functions that are linear in attribute bundles, with no restrictions on the shape of the distribution of preference parameters. Copyright 2007 by the Economics Department Of The University Of Pennsylvania And Osaka University Institute Of Social And Economic Research Association.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' Consider the choice of a profiling policy where decisions to search for evidence of crime may vary with observable covariates of the persons at risk of search. I pose a planning problem whose objective is to minimise the social cost of crime and search. The consequences of a search rule depend on the extent to which search deters crime. I study the planning problem when the planner has partial knowledge of deterrence. I show how the planner can eliminate dominated search rules and how he can use the minimax or minimax-regret criterion to choose an undominated rule. Copyright 2006 The Author(s). Journal compilation Royal Economic Society 2006.'] ["Participants in prediction markets such as the Iowa Electronic Markets trade all-or-nothing contracts that pay a dollar if and only if specified future events occur. Researchers engaged in empirical study of prediction markets have argued broadly that equilibrium prices of the contracts traded are market probabilities' that the specified events will occur. This paper shows that if traders are risk-neutral price takers with heterogenous beliefs, the price of a contract in a prediction market reveals nothing about the dispersion of traders' beliefs and partially identifies the central tendency of beliefs. Most persons have beliefs higher than price when price is above 0.5, and most have beliefs lower than price when price is below 0.5. The mean belief of traders lies in an interval whose midpoint is the equilibrium price. These findings persist even if traders use price data to revise their beliefs in plausible ways.<p>(This abstract was borrowed from another version of this item.)"] [' Research aiming to understand the impact of public and private pension plans on individual decision making has been hampered by a dearth of empirical evidence on benefit expectations. We review the occasional national surveys that have sought to measure pension-benefit expectations and describe our recent efforts to improve the measurement of US Social Security benefit expectations by elicitation of probabilistic expectations about well-defined outcomes. We document striking uncertainty and heterogeneity of beliefs about the long-term existence of the Social Security system and about the level of benefits should the system survive. In so doing, we highlight the additional information that is extracted using our probabilistic elicitation method as opposed to more traditional methods that only seek point forecasts. Copyright 2006 The Authors; Journal compilation 2006 CEIS, Fondazione Giacomo Brodolini and Blackwell Publishing Ltd..'] ['No abstract is available for this item.'] [" We examine the behaviour of pedestrians wishing to cross a stream of traffic at signalized intersections. We model each pedestrian as making a discrete crossing choice by comparing the gaps between vehicles in traffic to an individual-specific 'critical gap' that characterizes the individual's minimal acceptable gap. We propose both parametric and nonparametric approaches to estimate the distribution of critical gaps in the population of pedestrians. To estimate the model, we gather field data on crossing decisions and vehicle flows at three intersections in New Delhi. The estimates provide information about heterogeneity in critical gaps across pedestrians and intersections, and permit simulation of the effect of changes in traffic light sequences on pedestrian crossing behaviour and waiting times. Copyright \xc2\xa9 2005 John Wiley &amp; Sons, Ltd."] [" To predict choice behavior, the standard practice of economists has been to infer decision processes from data on observed choices. When decision makers act with partial information, economists typically assume that persons form probabilistic expectations for unknown quantities and maximize expected utility. Observed choices may be consistent with many alternative specifications of preferences and expectations, so researchers commonly assume particular sorts of expectations. It would be better to measure expectations in the form called for by modern economic theory; that is, subjective probabilities. Data on expectations can be used to relax or validate assumptions about expectations. Since the early 1990's, economists have increasingly undertaken to elicit from survey respondents probabilistic expectations of significant personal events. This article discusses the history underlying the new literature, describes some of what has been learned thus far, and looks ahead towards making further progress. Copyright The Econometric Society 2004."] [' I analyse social interactions that stem from the successive endeavours of new cohorts of heterogeneous decision makers to learn from the experiences of past cohorts. A dynamic process of information accumulation and decision making occurs as the members of each cohort observe the experiences of earlier ones, and then make choices that yield experiences observable by future cohorts. Decision makers face the "selection problem" as they seek to learn from observation of past actions and outcomes, while not observing the counterfactual outcomes that would have occurred had other actions been chosen. Assuming that all cohorts face the same outcome distributions, I show that social learning is a process of sequential reduction in ambiguity. The specific nature of this process, and its terminal state, depend critically on how decision makers make choices under ambiguity. I use the problem of learning about innovations to illustrate. Copyright The Review of Economic Studies Limited, 2004.'] [' Recently a growing body of research has studied inference in settings where parameters of interest are partially identified. In many cases the parameter is real-valued and the identification region is an interval whose lower and upper bounds may be estimated from sample data. For this case confidence intervals (CIs) have been proposed that cover the entire identification region with fixed probability. Here, we introduce a conceptually different type of confidence interval. Rather than cover the entire identification region with fixed probability, we propose CIs that asymptotically cover the true value of the parameter with this probability. However, the exact coverage probabilities of the simplest version of our new CIs do not converge to their nominal values uniformly across different values for the width of the identification region. To avoid the problems associated with this, we modify the proposed CI to ensure that its exact coverage probabilities do converge uniformly to their nominal values. We motivate this modified CI through exact results for the Gaussian case. Copyright The Econometric Society 2004.'] ['Research on consumer confidence has mainly sought to evaluate the power of available data to predict economic outcomes. In contrast, this article considers how best to measure consumer confidence. We analyze the responses to eight questions that have appeared recently on the Michigan Survey of Consumers; four elicit expectations in the traditional qualitative manner and four use a newer "percent chance" format. Examination of the responses suggests three implications. It makes more sense to ask for expectations of events directly relevant to individual economic decisions than for predictions of general business conditions. Surveys should shift away from qualitative questions in favor of ones eliciting subjective probability judgments. While aggregating responses into an index of consumer confidence may provide simple summary statistics, results should also be presented on a question-by-question basis for different subgroups of the population.'] [' An important objective of empirical research on treatment response is to provide decision makers with information useful in choosing treatments. This paper studies minimax-regret treatment choice using the sample data generated by a classical randomized experiment. Consider a utilitarian social planner who must choose among the feasible statistical treatment rules, these being functions that map the sample data and observed covariates of population members into a treatment allocation. If the planner knew the population distribution of treatment response, the optimal treatment rule would maximize mean welfare conditional on all observed covariates. The appropriate use of covariate information is a more subtle matter when only sample data on treatment response are available. I consider the class of conditional empirical success rules; that is, rules assigning persons to treatments that yield the best experimental outcomes conditional on alternative subsets of the observed covariates. I derive a closed-form bound on the maximum regret of any such rule. Comparison of the bounds for rules that condition on smaller and larger subsets of the covariates yields sufficient sample sizes for productive use of covariate information. When the available sample size exceeds the sufficiency boundary, a planner can be certain that conditioning treatment choice on more covariates is preferable (in terms of minimax regret) to conditioning on fewer covariates. Copyright The Econometric Society 2004.'] ['No abstract is available for this item.'] [' This paper explores how private and social incentives for fertility may have combined to produce the complex fertility pattern observed in Israel in the past half-century. Fertility has declined within some ethnic-religious groups, moderately increased in others, and parts of the ultra-Orthodox Jewish population have experienced a reverse fertility transition, in which childbearing has increased rapidly and substantially. We present a theoretical analysis of the social dynamics of fertility that shows how private preferences, preferences for conformity to social norms in childbearing, and piecewise linear child allowances could have combined to yield such a complex fertility pattern. We then explain the identification problem that makes it so difficult to infer the actual Israeli fertility process from data on completed fertility. (JEL:J13, Z13, H53) Copyright (c) 2003 The European Economic Association.'] ['Econometricians have found it useful to separate the problem of empirical inference into statistical and identification components. Studies of identification determine the conclusions that could be drawn if a researcher were able to observe a data sample of unlimited size. Statistical inference seeks to characterize how sampling variability affects the conclusions that can be drawn from samples of limited size. This Association Lecture to the Southern Economic Association describes the broad themes of a research program on identification that I began in the late 1980s and continue today. I show how these themes have played out in my analysis of the selection problem, a fundamental and pervasive identification problem. I examine how the selection problem manifests itself in the econometric analysis of market demand.'] ['We study the problem of identification of the long regression E(y|x,z) when the short conditional distributions P(y|x) and P(z|x) are known but the long conditional distribution P(y|x,z) is not known. This problem often arises when a researcher utilizes data from two separate data sets. (A leading example is the ecological inference problem of political science, where voting behavior across electoral districts is observed from administrative records, the demographic composition of voters within a district is observed from census data, and the researcher wants to infer voting behavior conditional on district and demographic attributes.) We isolate an identification region containing feasible values of the long regression, and show that this region forms a sharp bound on the long regression. The identification region can be calculated precisely when y has finite support. When y has infinite support we characterize two sets, one that contains the identification region, and one that is contained by it. Following this completely nonparametric analysis, we examine the identifying power yielded by exclusion restrictions across distinct covariate values. Such restrictions cause the identification region to shrink, in many cases to a single point. To illustrate the theory, we pose and address this hypothetical question: What would be the outcome if the 1996 U.S. presidential election were re-enacted in a population of different demographic composition, ceteris paribus?<p>(This abstract was borrowed from another version of this item.)'] ['The idea of program evaluation is both simple and appealing. Program outcomes are measured and compared to some minimum performance standard or threshold. In practice, however, evaluation is difficult. Two fundamental problems of outcome measurement must be addressed. The first, which we call the problem of auxiliary outcomes, is that we do not observe outcome of interest. The second, which we call the problem of counterfactual outcomes, is that we do not observe the threshold standard. This article examines how performance standards should be set and applied in the face of these problems in measuring outcomes. The central message is that the proper way to implement standards varies with the prior information an evaluator can credibly bring to bear to compensate for incomplete outcome data. By combining available data with credible assumptions on treatments and outcomes, the performance of a program may be deemed acceptable, unacceptable, or indeterminate.'] [' This paper examines inference on regressions when interval data are available on one variable, the other variables being measured precisely. Let a population be characterized by a distribution "P"("y", "x", "v", "v"-sub-0, "v"-sub-1), where "y" is an element of "R"-super-1, "x" is an element of "R-super-k", and the real variables ("v", "v"-sub-0, "v"-sub-1) satisfy "v"-sub-0\xe2\x89\xa4"v"\xe2\x89\xa4"v"-sub-1. Let a random sample be drawn from "P" and the realizations of ("y", "x", "v"-sub-0, "v"-sub-1) be observed, but not those of "v". The problem of interest may be to infer "E"("y"|"x", "v") or "E"("v"|"x"). This analysis maintains Interval (I), Monotonicity (M), and Mean Independence (MI) assumptions: (I) "P"("v"-sub-0\xe2\x89\xa4"v"\xe2\x89\xa4"v"-sub-1)&amp;equals;1; (M) "E"("y"|"x", "v") is monotone in "v"; (MI) "E"("y"|"x", "v", "v"-sub-0, "v"-sub-1)&amp;equals;"E"("y"|"x", "v"). No restrictions are imposed on the distribution of the unobserved values of "v" within the observed intervals ["v"-sub-0, "v"-sub-1]. It is found that the IMMI Assumptions alone imply simple nonparametric bounds on "E"("y"|"x", "v") and "E"("v"|"x"). These assumptions invoked when "y" is binary and combined with a semiparametric binary regression model yield an identification region for the parameters that may be estimated consistently by a "modified maximum score (MMS)" method. The IMMI assumptions combined with a parametric model for "E"("y"|"x", "v") or "E"("v"|"x") yield an identification region that may be estimated consistently by a "modified minimum-distance (MMD)" method. Monte Carlo methods are used to characterize the finite-sample performance of these estimators. Empirical case studies are performed using interval wealth data in the Health and Retirement Study and interval income data in the Current Population Survey. Copyright The Econometric Society 2002.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ["This paper analyzes the probabilistic measures of job insecurity that have recently become available through the nationwide Survey of Economic Expectations (SEE). Since 1994, employed SEE respondents have been asked questions eliciting their subjective probabilities of job loss in the coming year and their expectations of a good outcome should they lose their current job and have to engage in job search. The responses of 3,561 persons interviewed from 1994 through early 1998 are analyzed here. It is found that workers vary considerably in their perceptions of job insecurity, with most workers perceiving little or no risk but some perceiving moderate to high risk. Expectations of job loss tend to decrease markedly with age, but so do expectations of a good outcome should job search become necessary. The net result is that job insecurity tends not to vary at all with age. Subjective probabilities of job loss tend to decrease with schooling and subjective probabilities of good search outcomes tend to increase with schooling; hence composite job insecurity tends to decrease with schooling. Perceptions of job insecurity vary little by sex. Perceptions of job insecurity vary substantially by race, the main differences being that subjective probabilities of job loss among blacks tend to be nearly double those of whites. Self-employed workers see themselves as facing less job insecurity than do those who work for others. Worker perceptions of job insecurity peaked in 1995. Expectations within groups are heterogeneous, the covariates (age, schooling, sex, race, employer, year) collectively explaining only a small part of the sample variation in worker expectations. Moving beyond descriptive analysis, the paper connects the empirical findings to modern theories of the labor market. A competing-risks formalization of job separations by the two routes of job loss and voluntary quits is used to draw conclusions about workers' expectations of exogenous job destruction in the year ahead. The theory of job search is used to interpret the empirical finding that the distribution of search-outcome expectations is symmetric and quite dispersed."] ['Economics is broadening its scope from analysis of markets to study of general social interactions. Developments in game theory, the economics of the family, and endogenous growth theory have led the way. Economists have also performed new empirical research using observational data on social interactions, but with much less to show. The fundamental problem is that observable outcomes may be generated by many different interaction processes, so empirical findings are open to a wide variety of interpretations. To make sustained progress, empirical research will need richer data, including experiments in controlled environments and subjective data on preferences and expectations.'] ['Econometric analyses of treatment response commonly use instrumental variable (IV) assumptions to identify treatment effects. Yet the credibility of IV assumptions is often a matter of considerable disagreement. There is therefore good reason to consider weaker but more credible assumptions. To this end, we introduce monotone instrumental variable (MIV) assumptions and the important special case of monotone treatment selection (MTS). We study the identifying power of MIV assumptions alone and combined with the assumption of monotone treatment response (MTR). We present an empirical application using the MTS and MTR assumptions to place upper bounds on the returns to schooling.<p>(This abstract was borrowed from another version of this item.)'] [' This paper studies the use of probabilistic expectations data to predict behavior in incomplete scenarios posed by the researcher. The information that respondents have when replying to questions posing incomplete scenarios is a subset of the information that they would have in actual choice settings. Hence such questions do not elicit pure statements of preference; they elicit preferences mixed with expectations of future events that may affect choice behavior. The analysis developed here assumes respondents recognize that their behavior may depend on information they do not have when expectations are elicited, and that they answer coherently and honestly given the information provided. The objective in imagining such ideal respondents is to place a logical upper bound on the predictive content of elicited choice expectations. Copyright 1999 by Kluwer Academic Publishers'] ['Survey nonresponse makes identification of population statistics problematic. Except in special cases, identification is possible only if one makes untestable assumptions about the distribution of the missing data. However, non-response does not preclude identification of bounds on population statistics. This paper shows how identified bounds on unidentified population statistics can be obtained under several forms of nonresponse. Organizations conducting major surveys commonly release public-use data files that provide nonresponse weights or imputations to be used for estimating population statistics. The paper shows how to bound the asymptotic bias of estimates using weights and imputations. The results are illustrated with empirical examples based on the National Longitudinal Survey of Youth.<p>(This abstract was borrowed from another version of this item.)'] [' A common concern of evaluation studies is to learn the distribution of outcomes when a specified treatment policy, or assignment rule, determines the treatment received by each member of a specified population. Recent studies have emphasized evaluation of policies providing the same treatment to all members of the population. In particular, experiments with randomized treatments have this objective. Social programmes mandating homogeneous treatment of the population are of interest, but so are ones in which treatment varies across the population. This paper examines the use of empirical evidence on programmes with homogeneous treatments to infer the outcomes that would occur if treatment were to vary across the population. Experimental evidence from the Perry Pre-school Project is used to illustrate the inferential problem and the main findings of the analysis. Copyright 1997 by The Review of Economic Studies Limited.'] ['This paper investigates what may be learned about treatment response when it is assumed that response functions are monotone, semimonotone, or concave-monotone. Nothing is assumed about the process of treatment selection and cross-individual restrictions on response are not imposed. The idea is to determine, for every member of the population, the response functions that pass through the realized (treatment, outcome) pair and that are consistent with the assumption imposed. These findings are then aggregated to determine what can be learned about the population distribution of response. The analysis is applied to the econometrics of demand and production.'] ['The importance of social programs to a diverse population creates a legitimate concern that the findings of evaluations be widely credible. The weaker the assumptions imposed, the more widely credible are the findings. The classical argument for random assignment of treatments is viewed by many as enabling evaluation under weak assumptions, and it has generated much interest in the conduct of experiments. But the classical argument does impose assumptions, and there often is good reason to doubt their realism. The methodological research described in this article explores the inferences that may be drawn from experimental data under assumptions weak enough to yield widely credible findings. This literature has two branches. One seeks out notions of treatment effect that are identified when the experimental data are combined with weak assumptions. The canonical finding is that the average treatment effect within some context-specific subpopulation is identified. The other branch specifies a population of a priori interest and seeks to learn about treatment effects in this population. Here the canonical finding is a bound on average treatment effects. The various approaches to the analysis of experiments are complementary from a mathematical perspective, but in tension as guides to evaluation practice. The reader of an evaluation reporting that some social program "works" or has a "positive impact" should be careful to ascertain what treatment effect has been estimated and under what assumptions.'] ["We report here on the design and first application of an interactive computer-assisted self-administered interview (CASI) survey eliciting from high school students and college undergraduates their expectations of the income they would earn if they were to complete different levels of schooling. We also elicit respondents' beliefs about current earnings distributions. Whereas a scattering of earlier studies have elicited point expectations of earnings unconditional on future schooling, we elicit subjective earnings distributions under alternative scenarios for future schooling. In this exploratory study, we find that respondents are willing and able to respond meaningfully to questions eliciting their earnings expectations in probabilistic form. The 110 respondents vary considerably in their earnings expectations but there is a common belief that the returns to a college education are positive and that earnings rise between ages 30 and 40. There is a common belief that one's own future earnings are rather uncertain. Moreover, respondents tend to overestimate the current degree of earnings inequality in American society."] ['No abstract is available for this item.'] [' Robust estimation aims at developing point estimators that are not highly sensitive to errors in data. However, the population parameters of interest are not identified under the assumptions of robust estimation, so the rationale for point estimation is not apparent. This paper shows that, under error models used in robust estimation, unidentified population parameters can often be bounded. The bounds provide information that is not available in robust estimation. For example, it is possible to bound the population mean under contaminated sampling. It is argued that estimating the bounds is more natural than attempting point estimation of unidentified parameters. Copyright 1995 by The Econometric Society.'] ['No abstract is available for this item.'] [' This paper examines the reflection problem that arises when a researcher observing the distribution of behavior in a population tries to infer whether the average behavior in some group influences the behavior of the individuals that comprise the group. It is found that inference is not possible unless the researcher has prior information specifying the composition of reference groups. If this information is available, the prospects for inference depend critically on the population relationship between the variables defining reference groups and those directly affecting outcomes. Inference is difficult to impossible if these variables are functionally dependent or are statistically independent. Copyright 1993 by The Review of Economic Studies Limited.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['It is well-known that, under the logit model for binary response, the random sampling and response-based sampling maximum likelihood estimators coincide for all parameters except the intercept. Citing this coincidence, many researchers have assumed the logit model and analyzed data from response-based samples as if those data were obtained by random sampling. We argue that this practice should be avoided unless the researcher really believes the logit specification. One preferable alternative is the weighted maximum likelihood estimator of Manski and Lerman (1977). Random sampling maximum likelihood analysis does not have a natural interpretation when the true response function is not logit. Weighted maximum likelihood analysis estimates a constrained best predictor of the binary response and so remains interpretable.'] ['No abstract is available for this item.'] ['This article considers anew the problem of estimating a regression E(y|x) when realizations of (y, x) are sampled randomly but y is observed selectively. The central issue is the failure of the sampling process to identify E(y|x). The problem faced by the researcher is to find correct prior restrictions which, when combined with the data, identify the regression. Two kinds of restrictions are examined here. One, which has not been studied before, is a bound on the support of y. Such a bound implies a simple, useful bound on E(y|x). The other, which has received much attention, is a separability restriction derived from a latent variable model.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] [' Andersen_(1970) considered the problem of inference on random effects linear models from binary response panel data, and showed that inference is possible if the disturbances for each panel member are known to be white noise with the logistic distribution. The present paper shows that inference remains possible if the disturbances are known only to be time-stationary. A conditional version of the maximum score estimator consistently estimates the parameters up to scale. Copyright 1987 by The Econometric Society.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['No abstract is available for this item.'] ['Models of household vehicle ownership decisions do not suffice as a basis for forecasting the size and composition of aggregate vehicle holdings. Forecasting applications require that such models be imbedded in systems describing the operation of the automobile market. This paper presents a new model of short run equilibrium in the automobile market. The short run is a period within which new car designs and prices are fixed but used car prices adjust competitively to market forces. The magnitude and mix of new car sales, the extent of used car scrappage and the composition of used car holdings are determined in equilibrium with used car prices. An econometric version of the market model has been estimated on Israeli data and applied to analyze the impact of vehicle tax policy on automobile holdings in Israel. The paper describes this application.'] ['No abstract is available for this item.'] ['The effects of tuition costs, financial aid, and individual attributes on college choice are analyzed using a conditional logit model. The results confirm that financial aid can be an important determinant of postsecondary school attendance and that individual academic ability relative to the academic standards of a college is an important determinant of which of available college alternatives is chosen.'] ['A unified approach to the modelling of household motor vehicle ownership level, consumption of vehicle holdings, and vehicle use is described. The model system is founded on a utility theory of household behavior, and is implemented through probabilistic discrete choice models estimated from cross-sectional samples of households. Following the introduction of the model, an application involving forecasts of US motor vehicle sales through the mid-1980s is presented.'] [' Government agencies often offer services or subsidies for which the demand is unknown. This paper treats the problem faced by such an agency when it must select a price-subsidy level so as to meet a budget constraint. It investigates the properties of the "zero-elasticity" pricing rule in which the agency sets an initial price, observes the resulting usage of the service, assumes that demand is totally price-inelastic and replaces the initial-price with one calculated to solve the budgetary problem, and then observes the usage that actually occurs and reapplies the zero-elasticity assumption. The paper presents analytical results on the dynamics of iterated use of the rule, particularly its convergence to a price solving the budgetary problem, and describes a case study of local transit pricing.'] [" The position of the new residential construction sector as a large but very volatile component of total investment has provoked much study of the causes of that volatility and of its relation to the stability of the economy as a whole. There has never been, however, serious analysis of the assertion that this same volatility of housing starts, by creating an unstable environment for firms operating in the industry, serves to increase the cost of producing and marketing housing.This paper will focus on the phenomenon of demand instability as it affects firm behavior. It approaches the demand instability problem from two distinct but complementary perspectives. The first part sets forth a theoretical conception of the manner in which demand instability should influence any firm's organization and operations. The second part presents an analysis of interviews conducted with representatives of a number of firms which supply materials for residential construction. This analysis provides a partial test of the theory and offers insights into areas on which theory alone sheds little light. Copyright American Real Estate and Urban Economics Association."] ['No abstract is available for this item.'] ['No abstract is available for this item.']